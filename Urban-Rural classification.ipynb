{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26668f12-8164-4a20-9204-16e41f54c648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_7456\\2845060036.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196f62ae-efc8-423e-bb16-8c46b7c2883b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>State FIPS Code</th>\n",
       "      <th>County FIPS Code</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Median Household Income</th>\n",
       "      <th>Urban-Rural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Autauga County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>59285</td>\n",
       "      <td>69841</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>239945</td>\n",
       "      <td>75019</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barbour County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>24757</td>\n",
       "      <td>44290</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bibb County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1007</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>22152</td>\n",
       "      <td>51215</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blount County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1009</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>59292</td>\n",
       "      <td>61096</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>Vega Baja Municipio</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>72145</td>\n",
       "      <td>72</td>\n",
       "      <td>145</td>\n",
       "      <td>54058</td>\n",
       "      <td>23877</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>Vieques Municipio</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>72147</td>\n",
       "      <td>72</td>\n",
       "      <td>147</td>\n",
       "      <td>8147</td>\n",
       "      <td>17531</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>Villalba Municipio</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>72149</td>\n",
       "      <td>72</td>\n",
       "      <td>149</td>\n",
       "      <td>21778</td>\n",
       "      <td>24882</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>Yabucoa Municipio</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>72151</td>\n",
       "      <td>72</td>\n",
       "      <td>151</td>\n",
       "      <td>29868</td>\n",
       "      <td>21279</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>Yauco Municipio</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>72153</td>\n",
       "      <td>72</td>\n",
       "      <td>153</td>\n",
       "      <td>33509</td>\n",
       "      <td>21918</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3222 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   County        State   FIPS  State FIPS Code  \\\n",
       "0          Autauga County      Alabama   1001                1   \n",
       "1          Baldwin County      Alabama   1003                1   \n",
       "2          Barbour County      Alabama   1005                1   \n",
       "3             Bibb County      Alabama   1007                1   \n",
       "4           Blount County      Alabama   1009                1   \n",
       "...                   ...          ...    ...              ...   \n",
       "3217  Vega Baja Municipio  Puerto Rico  72145               72   \n",
       "3218    Vieques Municipio  Puerto Rico  72147               72   \n",
       "3219   Villalba Municipio  Puerto Rico  72149               72   \n",
       "3220    Yabucoa Municipio  Puerto Rico  72151               72   \n",
       "3221      Yauco Municipio  Puerto Rico  72153               72   \n",
       "\n",
       "      County FIPS Code  Total Population  Median Household Income Urban-Rural  \n",
       "0                    1             59285                    69841       Urban  \n",
       "1                    3            239945                    75019       Urban  \n",
       "2                    5             24757                    44290       Rural  \n",
       "3                    7             22152                    51215       Rural  \n",
       "4                    9             59292                    61096       Urban  \n",
       "...                ...               ...                      ...         ...  \n",
       "3217               145             54058                    23877       Urban  \n",
       "3218               147              8147                    17531       Rural  \n",
       "3219               149             21778                    24882       Rural  \n",
       "3220               151             29868                    21279       Urban  \n",
       "3221               153             33509                    21918       Urban  \n",
       "\n",
       "[3222 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\MyDocs\\Docs\\MK\\Income_Urban_VS_Rural.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53dea3df-fc79-4caf-b45a-af157761f2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b659919-2c14-46ab-a623-c6ff477dc2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "County\n",
       "Washington County     30\n",
       "Jefferson County      25\n",
       "Franklin County       24\n",
       "Jackson County        23\n",
       "Lincoln County        23\n",
       "                      ..\n",
       "Ponce Municipio        1\n",
       "Peñuelas Municipio     1\n",
       "Patillas Municipio     1\n",
       "Orocovis Municipio     1\n",
       "Chilton County         1\n",
       "Name: count, Length: 1960, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.County.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d06a0c53-5aae-4716-86f3-15f0764a0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = []\n",
    "for i in df.columns:\n",
    "  if df[i].dtype == object and i !=\"Urban-Rural\":\n",
    "    category_columns.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51108807-85f7-4c21-8cf0-eead11a6b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns= []\n",
    "for j in df.columns:\n",
    "  if j not in category_columns and j!=\"Urban-Rural\":\n",
    "    numerical_columns.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c16ada2-e3f4-4b56-86c0-dad9885076eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "arranged_columns = category_columns + numerical_columns + [\"Urban-Rural\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f22d7cd0-bc92-4405-8960-c8f53baa357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[arranged_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2ec3f77-5164-4288-b3f1-9c8d09d83b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf05b98-fcfb-4761-a2a3-a42ad896fee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State\n",
       "Texas                   254\n",
       "Georgia                 159\n",
       "Virginia                133\n",
       "Kentucky                120\n",
       "Missouri                115\n",
       "Kansas                  105\n",
       "Illinois                102\n",
       "North Carolina          100\n",
       "Iowa                     99\n",
       "Tennessee                95\n",
       "Nebraska                 93\n",
       "Indiana                  92\n",
       "Ohio                     88\n",
       "Minnesota                87\n",
       "Michigan                 83\n",
       "Mississippi              82\n",
       "Puerto Rico              78\n",
       "Oklahoma                 77\n",
       "Arkansas                 75\n",
       "Wisconsin                72\n",
       "Alabama                  67\n",
       "Pennsylvania             67\n",
       "Florida                  67\n",
       "South Dakota             66\n",
       "Colorado                 64\n",
       "Louisiana                64\n",
       "New York                 62\n",
       "California               58\n",
       "Montana                  56\n",
       "West Virginia            55\n",
       "North Dakota             53\n",
       "South Carolina           46\n",
       "Idaho                    44\n",
       "Washington               39\n",
       "Oregon                   36\n",
       "New Mexico               33\n",
       "Alaska                   30\n",
       "Utah                     29\n",
       "Maryland                 24\n",
       "Wyoming                  23\n",
       "New Jersey               21\n",
       "Nevada                   17\n",
       "Maine                    16\n",
       "Arizona                  15\n",
       "Vermont                  14\n",
       "Massachusetts            14\n",
       "New Hampshire            10\n",
       "Connecticut               9\n",
       "Hawaii                    5\n",
       "Rhode Island              5\n",
       "Delaware                  3\n",
       "District of Columbia      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.State.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30872b71-e150-4009-9f80-9d2dd399db22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "County                     0\n",
       "State                      0\n",
       "FIPS                       0\n",
       "State FIPS Code            0\n",
       "County FIPS Code           0\n",
       "Total Population           0\n",
       "Median Household Income    0\n",
       "Urban-Rural                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6b19ac5-dc4d-47ae-8ea3-25aa9a658470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_7456\\3027607212.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-1.86395384 -1.86383112 -1.86370839 ...  2.50175265  2.50187537\n",
      "  2.50199809]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.iloc[: , 2:7]= sc.fit_transform(df.iloc[: , 2:7])\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_7456\\3027607212.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-1.85933395 -1.85933395 -1.85933395 ...  2.50116585  2.50116585\n",
      "  2.50116585]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.iloc[: , 2:7]= sc.fit_transform(df.iloc[: , 2:7])\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_7456\\3027607212.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.95937887 -0.94060733 -0.9218358  ...  0.42971502  0.44848656\n",
      "  0.4672581 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.iloc[: , 2:7]= sc.fit_transform(df.iloc[: , 2:7])\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_7456\\3027607212.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.13617918  0.41191025 -0.24093083 ... -0.24996857 -0.22542499\n",
      " -0.21437886]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.iloc[: , 2:7]= sc.fit_transform(df.iloc[: , 2:7])\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_7456\\3027607212.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.02521093 0.02552275 0.02367229 ... 0.02250356 0.02228659 0.02232507]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.iloc[: , 2:7]= sc.fit_transform(df.iloc[: , 2:7])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "df.iloc[: , 2:7]= sc.fit_transform(df.iloc[: , 2:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "675b7e75-e823-4689-9350-a644fadf5df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>State FIPS Code</th>\n",
       "      <th>County FIPS Code</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Median Household Income</th>\n",
       "      <th>Urban-Rural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Autauga County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>-1.863954</td>\n",
       "      <td>-1.859334</td>\n",
       "      <td>-0.959379</td>\n",
       "      <td>-0.136179</td>\n",
       "      <td>0.025211</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>-1.863831</td>\n",
       "      <td>-1.859334</td>\n",
       "      <td>-0.940607</td>\n",
       "      <td>0.411910</td>\n",
       "      <td>0.025523</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barbour County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>-1.863708</td>\n",
       "      <td>-1.859334</td>\n",
       "      <td>-0.921836</td>\n",
       "      <td>-0.240931</td>\n",
       "      <td>0.023672</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bibb County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>-1.863586</td>\n",
       "      <td>-1.859334</td>\n",
       "      <td>-0.903064</td>\n",
       "      <td>-0.248834</td>\n",
       "      <td>0.024089</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blount County</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>-1.863463</td>\n",
       "      <td>-1.859334</td>\n",
       "      <td>-0.884293</td>\n",
       "      <td>-0.136158</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>Vega Baja Municipio</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>2.501507</td>\n",
       "      <td>2.501166</td>\n",
       "      <td>0.392172</td>\n",
       "      <td>-0.152037</td>\n",
       "      <td>0.022443</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>Vieques Municipio</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>2.501630</td>\n",
       "      <td>2.501166</td>\n",
       "      <td>0.410943</td>\n",
       "      <td>-0.291323</td>\n",
       "      <td>0.022061</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>Villalba Municipio</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>2.501753</td>\n",
       "      <td>2.501166</td>\n",
       "      <td>0.429715</td>\n",
       "      <td>-0.249969</td>\n",
       "      <td>0.022504</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>Yabucoa Municipio</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>2.501875</td>\n",
       "      <td>2.501166</td>\n",
       "      <td>0.448487</td>\n",
       "      <td>-0.225425</td>\n",
       "      <td>0.022287</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>Yauco Municipio</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>2.501998</td>\n",
       "      <td>2.501166</td>\n",
       "      <td>0.467258</td>\n",
       "      <td>-0.214379</td>\n",
       "      <td>0.022325</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3222 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   County        State      FIPS  State FIPS Code  \\\n",
       "0          Autauga County      Alabama -1.863954        -1.859334   \n",
       "1          Baldwin County      Alabama -1.863831        -1.859334   \n",
       "2          Barbour County      Alabama -1.863708        -1.859334   \n",
       "3             Bibb County      Alabama -1.863586        -1.859334   \n",
       "4           Blount County      Alabama -1.863463        -1.859334   \n",
       "...                   ...          ...       ...              ...   \n",
       "3217  Vega Baja Municipio  Puerto Rico  2.501507         2.501166   \n",
       "3218    Vieques Municipio  Puerto Rico  2.501630         2.501166   \n",
       "3219   Villalba Municipio  Puerto Rico  2.501753         2.501166   \n",
       "3220    Yabucoa Municipio  Puerto Rico  2.501875         2.501166   \n",
       "3221      Yauco Municipio  Puerto Rico  2.501998         2.501166   \n",
       "\n",
       "      County FIPS Code  Total Population  Median Household Income Urban-Rural  \n",
       "0            -0.959379         -0.136179                 0.025211       Urban  \n",
       "1            -0.940607          0.411910                 0.025523       Urban  \n",
       "2            -0.921836         -0.240931                 0.023672       Rural  \n",
       "3            -0.903064         -0.248834                 0.024089       Rural  \n",
       "4            -0.884293         -0.136158                 0.024684       Urban  \n",
       "...                ...               ...                      ...         ...  \n",
       "3217          0.392172         -0.152037                 0.022443       Urban  \n",
       "3218          0.410943         -0.291323                 0.022061       Rural  \n",
       "3219          0.429715         -0.249969                 0.022504       Rural  \n",
       "3220          0.448487         -0.225425                 0.022287       Urban  \n",
       "3221          0.467258         -0.214379                 0.022325       Urban  \n",
       "\n",
       "[3222 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a03263f9-8336-4849-a186-f5fb66ddb76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1e2cee7-04a1-40bb-9783-c71a6830a775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0598812-97a3-438b-a77e-54a4d645a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0,1])], remainder='passthrough')\n",
    "X = ct.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56bfd89a-ce4c-4224-a3c0-20b96e59dd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ..., -0.95937887,\n",
       "       -0.13617918,  0.02521093])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0074faf9-6fcb-457f-b2bb-50decb5f714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37c326b3-2b6a-497b-bce2-d2d5e73fabcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98df7eee-7fae-429d-b742-051c039a0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassificationModel(nn.Module):\n",
    "  def __init__(self, in_features, out_features):\n",
    "    super().__init__()\n",
    "    self.layer = nn.Sequential(\n",
    "        nn.Linear(in_features = in_features, out_features = 120),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features = 120, out_features =240),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features =240 ,out_features =1)\n",
    "    )\n",
    "  def forward(self , x):\n",
    "    return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c40026ed-7133-4f26-842c-aad1ad1fc594",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = X.shape[1]\n",
    "out_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "138a1b28-aa76-4145-85c7-a4b65ee3c380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ca1b44f-5253-43b9-98d5-92a80dbd7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SimpleClassificationModel(in_features , out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d278b78f-717b-43bc-9457-f53c32862917",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(params = model1.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b84106d-385f-4391-8213-aa5953eb9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.2 , random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19127e8c-ba4a-47fa-bc0f-56b081490d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train , dtype = torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train , dtype = torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test , dtype = torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test , dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ccf2272-fd71-4bdc-9461-fb5cd5d471ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch :0----> train loss : 0.6930719614028931 || test loss : 0.6929395198822021\n",
      " epoch :10----> train loss : 0.693014919757843 || test loss : 0.6928939819335938\n",
      " epoch :20----> train loss : 0.6929582953453064 || test loss : 0.6928486824035645\n",
      " epoch :30----> train loss : 0.6929022669792175 || test loss : 0.6928033828735352\n",
      " epoch :40----> train loss : 0.6928468346595764 || test loss : 0.692758321762085\n",
      " epoch :50----> train loss : 0.6927915811538696 || test loss : 0.6927131414413452\n",
      " epoch :60----> train loss : 0.6927366852760315 || test loss : 0.692668080329895\n",
      " epoch :70----> train loss : 0.6926820874214172 || test loss : 0.6926227807998657\n",
      " epoch :80----> train loss : 0.6926277279853821 || test loss : 0.6925774812698364\n",
      " epoch :90----> train loss : 0.6925733089447021 || test loss : 0.6925317049026489\n",
      " epoch :100----> train loss : 0.6925190687179565 || test loss : 0.6924857497215271\n",
      " epoch :110----> train loss : 0.6924648284912109 || test loss : 0.692439615726471\n",
      " epoch :120----> train loss : 0.6924106478691101 || test loss : 0.6923932433128357\n",
      " epoch :130----> train loss : 0.6923564672470093 || test loss : 0.6923463940620422\n",
      " epoch :140----> train loss : 0.6923023462295532 || test loss : 0.6922993659973145\n",
      " epoch :150----> train loss : 0.6922484636306763 || test loss : 0.6922519207000732\n",
      " epoch :160----> train loss : 0.6921945810317993 || test loss : 0.6922040581703186\n",
      " epoch :170----> train loss : 0.6921404600143433 || test loss : 0.6921557188034058\n",
      " epoch :180----> train loss : 0.6920861601829529 || test loss : 0.6921069622039795\n",
      " epoch :190----> train loss : 0.6920315623283386 || test loss : 0.6920577883720398\n",
      " epoch :200----> train loss : 0.6919768452644348 || test loss : 0.6920079588890076\n",
      " epoch :210----> train loss : 0.6919218301773071 || test loss : 0.6919577717781067\n",
      " epoch :220----> train loss : 0.6918665170669556 || test loss : 0.6919069290161133\n",
      " epoch :230----> train loss : 0.6918109059333801 || test loss : 0.6918556094169617\n",
      " epoch :240----> train loss : 0.6917550563812256 || test loss : 0.6918035745620728\n",
      " epoch :250----> train loss : 0.6916986703872681 || test loss : 0.6917511820793152\n",
      " epoch :260----> train loss : 0.6916419863700867 || test loss : 0.6916982531547546\n",
      " epoch :270----> train loss : 0.6915849447250366 || test loss : 0.691644549369812\n",
      " epoch :280----> train loss : 0.6915273666381836 || test loss : 0.6915901899337769\n",
      " epoch :290----> train loss : 0.6914692521095276 || test loss : 0.6915351748466492\n",
      " epoch :300----> train loss : 0.6914107203483582 || test loss : 0.6914793252944946\n",
      " epoch :310----> train loss : 0.6913514733314514 || test loss : 0.6914226412773132\n",
      " epoch :320----> train loss : 0.6912916898727417 || test loss : 0.6913650631904602\n",
      " epoch :330----> train loss : 0.6912313103675842 || test loss : 0.6913064122200012\n",
      " epoch :340----> train loss : 0.6911700963973999 || test loss : 0.6912468671798706\n",
      " epoch :350----> train loss : 0.691108226776123 || test loss : 0.6911866664886475\n",
      " epoch :360----> train loss : 0.6910457015037537 || test loss : 0.6911258101463318\n",
      " epoch :370----> train loss : 0.690982460975647 || test loss : 0.6910640597343445\n",
      " epoch :380----> train loss : 0.690918505191803 || test loss : 0.6910013556480408\n",
      " epoch :390----> train loss : 0.6908538341522217 || test loss : 0.6909375786781311\n",
      " epoch :400----> train loss : 0.6907885670661926 || test loss : 0.6908730268478394\n",
      " epoch :410----> train loss : 0.6907224059104919 || test loss : 0.6908072829246521\n",
      " epoch :420----> train loss : 0.6906556487083435 || test loss : 0.690740704536438\n",
      " epoch :430----> train loss : 0.6905881762504578 || test loss : 0.6906731724739075\n",
      " epoch :440----> train loss : 0.6905201077461243 || test loss : 0.6906046867370605\n",
      " epoch :450----> train loss : 0.6904513835906982 || test loss : 0.6905351281166077\n",
      " epoch :460----> train loss : 0.6903817057609558 || test loss : 0.6904641389846802\n",
      " epoch :470----> train loss : 0.6903107762336731 || test loss : 0.6903918385505676\n",
      " epoch :480----> train loss : 0.690238893032074 || test loss : 0.69031822681427\n",
      " epoch :490----> train loss : 0.6901658773422241 || test loss : 0.6902433037757874\n",
      " epoch :500----> train loss : 0.6900917291641235 || test loss : 0.6901671886444092\n",
      " epoch :510----> train loss : 0.6900166273117065 || test loss : 0.6900898814201355\n",
      " epoch :520----> train loss : 0.6899404525756836 || test loss : 0.6900115609169006\n",
      " epoch :530----> train loss : 0.6898633241653442 || test loss : 0.6899318099021912\n",
      " epoch :540----> train loss : 0.6897850632667542 || test loss : 0.6898508071899414\n",
      " epoch :550----> train loss : 0.6897056698799133 || test loss : 0.6897682547569275\n",
      " epoch :560----> train loss : 0.6896252632141113 || test loss : 0.6896845102310181\n",
      " epoch :570----> train loss : 0.6895434856414795 || test loss : 0.6895993947982788\n",
      " epoch :580----> train loss : 0.6894605755805969 || test loss : 0.6895130276679993\n",
      " epoch :590----> train loss : 0.6893762946128845 || test loss : 0.6894250512123108\n",
      " epoch :600----> train loss : 0.6892905831336975 || test loss : 0.6893353462219238\n",
      " epoch :610----> train loss : 0.6892035603523254 || test loss : 0.6892441511154175\n",
      " epoch :620----> train loss : 0.6891153454780579 || test loss : 0.6891515254974365\n",
      " epoch :630----> train loss : 0.6890255212783813 || test loss : 0.6890572905540466\n",
      " epoch :640----> train loss : 0.6889342665672302 || test loss : 0.6889613270759583\n",
      " epoch :650----> train loss : 0.6888417601585388 || test loss : 0.6888639330863953\n",
      " epoch :660----> train loss : 0.6887478828430176 || test loss : 0.6887648701667786\n",
      " epoch :670----> train loss : 0.688652515411377 || test loss : 0.6886643171310425\n",
      " epoch :680----> train loss : 0.6885553002357483 || test loss : 0.688561737537384\n",
      " epoch :690----> train loss : 0.6884563565254211 || test loss : 0.6884572505950928\n",
      " epoch :700----> train loss : 0.6883558034896851 || test loss : 0.6883509159088135\n",
      " epoch :710----> train loss : 0.6882533431053162 || test loss : 0.688242495059967\n",
      " epoch :720----> train loss : 0.6881492137908936 || test loss : 0.6881319284439087\n",
      " epoch :730----> train loss : 0.6880430579185486 || test loss : 0.6880194544792175\n",
      " epoch :740----> train loss : 0.6879350543022156 || test loss : 0.6879048943519592\n",
      " epoch :750----> train loss : 0.6878253221511841 || test loss : 0.6877887845039368\n",
      " epoch :760----> train loss : 0.6877135038375854 || test loss : 0.6876705288887024\n",
      " epoch :770----> train loss : 0.6875995397567749 || test loss : 0.6875500679016113\n",
      " epoch :780----> train loss : 0.6874834299087524 || test loss : 0.6874275803565979\n",
      " epoch :790----> train loss : 0.6873651146888733 || test loss : 0.6873030066490173\n",
      " epoch :800----> train loss : 0.6872444748878479 || test loss : 0.687175989151001\n",
      " epoch :810----> train loss : 0.6871218085289001 || test loss : 0.6870469450950623\n",
      " epoch :820----> train loss : 0.6869970560073853 || test loss : 0.6869155764579773\n",
      " epoch :830----> train loss : 0.6868701577186584 || test loss : 0.6867819428443909\n",
      " epoch :840----> train loss : 0.6867411732673645 || test loss : 0.6866464018821716\n",
      " epoch :850----> train loss : 0.6866099834442139 || test loss : 0.6865085959434509\n",
      " epoch :860----> train loss : 0.6864766478538513 || test loss : 0.6863685250282288\n",
      " epoch :870----> train loss : 0.6863410472869873 || test loss : 0.6862258315086365\n",
      " epoch :880----> train loss : 0.6862030029296875 || test loss : 0.6860803961753845\n",
      " epoch :890----> train loss : 0.6860625147819519 || test loss : 0.6859325766563416\n",
      " epoch :900----> train loss : 0.6859197616577148 || test loss : 0.6857819557189941\n",
      " epoch :910----> train loss : 0.6857742667198181 || test loss : 0.6856286525726318\n",
      " epoch :920----> train loss : 0.6856265664100647 || test loss : 0.6854725480079651\n",
      " epoch :930----> train loss : 0.6854763031005859 || test loss : 0.6853135228157043\n",
      " epoch :940----> train loss : 0.6853234171867371 || test loss : 0.6851516366004944\n",
      " epoch :950----> train loss : 0.6851678490638733 || test loss : 0.6849868893623352\n",
      " epoch :960----> train loss : 0.6850095391273499 || test loss : 0.6848191618919373\n",
      " epoch :970----> train loss : 0.6848484873771667 || test loss : 0.6846482753753662\n",
      " epoch :980----> train loss : 0.6846847534179688 || test loss : 0.6844744682312012\n",
      " epoch :990----> train loss : 0.684518039226532 || test loss : 0.6842976212501526\n",
      " epoch :1000----> train loss : 0.684348464012146 || test loss : 0.6841175556182861\n",
      " epoch :1010----> train loss : 0.6841759085655212 || test loss : 0.6839345097541809\n",
      " epoch :1020----> train loss : 0.6840001940727234 || test loss : 0.6837479472160339\n",
      " epoch :1030----> train loss : 0.6838212013244629 || test loss : 0.6835580468177795\n",
      " epoch :1040----> train loss : 0.6836391091346741 || test loss : 0.6833646893501282\n",
      " epoch :1050----> train loss : 0.6834537982940674 || test loss : 0.6831678748130798\n",
      " epoch :1060----> train loss : 0.6832650899887085 || test loss : 0.6829673051834106\n",
      " epoch :1070----> train loss : 0.6830731630325317 || test loss : 0.6827629804611206\n",
      " epoch :1080----> train loss : 0.6828778982162476 || test loss : 0.6825549006462097\n",
      " epoch :1090----> train loss : 0.6826790571212769 || test loss : 0.6823426485061646\n",
      " epoch :1100----> train loss : 0.6824764013290405 || test loss : 0.6821262240409851\n",
      " epoch :1110----> train loss : 0.6822700500488281 || test loss : 0.6819055676460266\n",
      " epoch :1120----> train loss : 0.6820597648620605 || test loss : 0.6816809177398682\n",
      " epoch :1130----> train loss : 0.6818457245826721 || test loss : 0.6814520955085754\n",
      " epoch :1140----> train loss : 0.6816279292106628 || test loss : 0.681219220161438\n",
      " epoch :1150----> train loss : 0.6814063191413879 || test loss : 0.6809821128845215\n",
      " epoch :1160----> train loss : 0.681181013584137 || test loss : 0.6807411313056946\n",
      " epoch :1170----> train loss : 0.6809517741203308 || test loss : 0.6804959774017334\n",
      " epoch :1180----> train loss : 0.6807183623313904 || test loss : 0.6802462935447693\n",
      " epoch :1190----> train loss : 0.6804808378219604 || test loss : 0.679992139339447\n",
      " epoch :1200----> train loss : 0.6802393198013306 || test loss : 0.679733157157898\n",
      " epoch :1210----> train loss : 0.6799935698509216 || test loss : 0.6794694066047668\n",
      " epoch :1220----> train loss : 0.679743230342865 || test loss : 0.6792007088661194\n",
      " epoch :1230----> train loss : 0.6794881224632263 || test loss : 0.6789271831512451\n",
      " epoch :1240----> train loss : 0.679228663444519 || test loss : 0.6786486506462097\n",
      " epoch :1250----> train loss : 0.6789646744728088 || test loss : 0.678365170955658\n",
      " epoch :1260----> train loss : 0.678695797920227 || test loss : 0.6780761480331421\n",
      " epoch :1270----> train loss : 0.6784219145774841 || test loss : 0.6777815818786621\n",
      " epoch :1280----> train loss : 0.6781431436538696 || test loss : 0.6774815320968628\n",
      " epoch :1290----> train loss : 0.6778594851493835 || test loss : 0.6771760582923889\n",
      " epoch :1300----> train loss : 0.6775708198547363 || test loss : 0.6768653988838196\n",
      " epoch :1310----> train loss : 0.677277147769928 || test loss : 0.6765491366386414\n",
      " epoch :1320----> train loss : 0.6769781112670898 || test loss : 0.6762269735336304\n",
      " epoch :1330----> train loss : 0.6766737103462219 || test loss : 0.6758988499641418\n",
      " epoch :1340----> train loss : 0.6763637065887451 || test loss : 0.6755651235580444\n",
      " epoch :1350----> train loss : 0.6760478615760803 || test loss : 0.6752252578735352\n",
      " epoch :1360----> train loss : 0.6757262945175171 || test loss : 0.6748791337013245\n",
      " epoch :1370----> train loss : 0.675399124622345 || test loss : 0.6745269298553467\n",
      " epoch :1380----> train loss : 0.6750661730766296 || test loss : 0.6741682887077332\n",
      " epoch :1390----> train loss : 0.6747271418571472 || test loss : 0.6738031506538391\n",
      " epoch :1400----> train loss : 0.6743822693824768 || test loss : 0.6734317541122437\n",
      " epoch :1410----> train loss : 0.6740310788154602 || test loss : 0.6730535626411438\n",
      " epoch :1420----> train loss : 0.6736738681793213 || test loss : 0.6726688146591187\n",
      " epoch :1430----> train loss : 0.6733101606369019 || test loss : 0.6722767949104309\n",
      " epoch :1440----> train loss : 0.6729401350021362 || test loss : 0.671877920627594\n",
      " epoch :1450----> train loss : 0.6725632548332214 || test loss : 0.6714715361595154\n",
      " epoch :1460----> train loss : 0.6721794605255127 || test loss : 0.6710575819015503\n",
      " epoch :1470----> train loss : 0.6717888712882996 || test loss : 0.6706360578536987\n",
      " epoch :1480----> train loss : 0.671391487121582 || test loss : 0.6702069640159607\n",
      " epoch :1490----> train loss : 0.6709870100021362 || test loss : 0.6697700023651123\n",
      " epoch :1500----> train loss : 0.6705752611160278 || test loss : 0.6693252921104431\n",
      " epoch :1510----> train loss : 0.6701561808586121 || test loss : 0.6688730120658875\n",
      " epoch :1520----> train loss : 0.6697297692298889 || test loss : 0.6684130430221558\n",
      " epoch :1530----> train loss : 0.6692960858345032 || test loss : 0.6679450869560242\n",
      " epoch :1540----> train loss : 0.6688546538352966 || test loss : 0.6674686670303345\n",
      " epoch :1550----> train loss : 0.6684054732322693 || test loss : 0.6669836044311523\n",
      " epoch :1560----> train loss : 0.667948305606842 || test loss : 0.6664897799491882\n",
      " epoch :1570----> train loss : 0.6674829721450806 || test loss : 0.6659872531890869\n",
      " epoch :1580----> train loss : 0.6670095324516296 || test loss : 0.6654754877090454\n",
      " epoch :1590----> train loss : 0.6665278673171997 || test loss : 0.664954662322998\n",
      " epoch :1600----> train loss : 0.6660381555557251 || test loss : 0.6644248962402344\n",
      " epoch :1610----> train loss : 0.665540337562561 || test loss : 0.6638859510421753\n",
      " epoch :1620----> train loss : 0.6650338768959045 || test loss : 0.6633378863334656\n",
      " epoch :1630----> train loss : 0.6645185351371765 || test loss : 0.6627804040908813\n",
      " epoch :1640----> train loss : 0.663994550704956 || test loss : 0.6622137427330017\n",
      " epoch :1650----> train loss : 0.6634616255760193 || test loss : 0.6616368889808655\n",
      " epoch :1660----> train loss : 0.6629194021224976 || test loss : 0.6610498428344727\n",
      " epoch :1670----> train loss : 0.6623672246932983 || test loss : 0.6604523658752441\n",
      " epoch :1680----> train loss : 0.6618054509162903 || test loss : 0.6598445773124695\n",
      " epoch :1690----> train loss : 0.6612338423728943 || test loss : 0.6592260599136353\n",
      " epoch :1700----> train loss : 0.6606521010398865 || test loss : 0.6585966944694519\n",
      " epoch :1710----> train loss : 0.6600599884986877 || test loss : 0.6579556465148926\n",
      " epoch :1720----> train loss : 0.6594578623771667 || test loss : 0.6573038697242737\n",
      " epoch :1730----> train loss : 0.6588461399078369 || test loss : 0.65664142370224\n",
      " epoch :1740----> train loss : 0.6582242846488953 || test loss : 0.6559678912162781\n",
      " epoch :1750----> train loss : 0.6575926542282104 || test loss : 0.6552835702896118\n",
      " epoch :1760----> train loss : 0.656950831413269 || test loss : 0.6545877456665039\n",
      " epoch :1770----> train loss : 0.6562981009483337 || test loss : 0.6538798809051514\n",
      " epoch :1780----> train loss : 0.655634343624115 || test loss : 0.6531609296798706\n",
      " epoch :1790----> train loss : 0.6549598574638367 || test loss : 0.6524299383163452\n",
      " epoch :1800----> train loss : 0.6542747020721436 || test loss : 0.6516870260238647\n",
      " epoch :1810----> train loss : 0.6535788774490356 || test loss : 0.650932252407074\n",
      " epoch :1820----> train loss : 0.6528723239898682 || test loss : 0.6501652002334595\n",
      " epoch :1830----> train loss : 0.6521543860435486 || test loss : 0.6493852734565735\n",
      " epoch :1840----> train loss : 0.6514253616333008 || test loss : 0.6485932469367981\n",
      " epoch :1850----> train loss : 0.6506851315498352 || test loss : 0.647788941860199\n",
      " epoch :1860----> train loss : 0.6499332189559937 || test loss : 0.6469716429710388\n",
      " epoch :1870----> train loss : 0.6491691470146179 || test loss : 0.6461411714553833\n",
      " epoch :1880----> train loss : 0.6483932733535767 || test loss : 0.6452977657318115\n",
      " epoch :1890----> train loss : 0.6476057171821594 || test loss : 0.6444419622421265\n",
      " epoch :1900----> train loss : 0.6468064188957214 || test loss : 0.6435728669166565\n",
      " epoch :1910----> train loss : 0.6459951400756836 || test loss : 0.6426903009414673\n",
      " epoch :1920----> train loss : 0.6451718211174011 || test loss : 0.6417942643165588\n",
      " epoch :1930----> train loss : 0.6443362236022949 || test loss : 0.6408846378326416\n",
      " epoch :1940----> train loss : 0.6434882283210754 || test loss : 0.6399610638618469\n",
      " epoch :1950----> train loss : 0.6426281332969666 || test loss : 0.6390239596366882\n",
      " epoch :1960----> train loss : 0.6417557001113892 || test loss : 0.638073742389679\n",
      " epoch :1970----> train loss : 0.6408705711364746 || test loss : 0.6371097564697266\n",
      " epoch :1980----> train loss : 0.6399725079536438 || test loss : 0.6361319422721863\n",
      " epoch :1990----> train loss : 0.6390611529350281 || test loss : 0.6351396441459656\n",
      " epoch :2000----> train loss : 0.63813716173172 || test loss : 0.6341333389282227\n",
      " epoch :2010----> train loss : 0.6372002363204956 || test loss : 0.6331132054328918\n",
      " epoch :2020----> train loss : 0.6362505555152893 || test loss : 0.6320791244506836\n",
      " epoch :2030----> train loss : 0.6352877616882324 || test loss : 0.6310312151908875\n",
      " epoch :2040----> train loss : 0.6343119740486145 || test loss : 0.6299691796302795\n",
      " epoch :2050----> train loss : 0.6333228945732117 || test loss : 0.6288919448852539\n",
      " epoch :2060----> train loss : 0.6323204040527344 || test loss : 0.6278008222579956\n",
      " epoch :2070----> train loss : 0.6313042044639587 || test loss : 0.6266950368881226\n",
      " epoch :2080----> train loss : 0.6302744746208191 || test loss : 0.6255744695663452\n",
      " epoch :2090----> train loss : 0.6292315721511841 || test loss : 0.6244396567344666\n",
      " epoch :2100----> train loss : 0.6281756162643433 || test loss : 0.6232898831367493\n",
      " epoch :2110----> train loss : 0.6271060109138489 || test loss : 0.6221252679824829\n",
      " epoch :2120----> train loss : 0.6260230541229248 || test loss : 0.6209469437599182\n",
      " epoch :2130----> train loss : 0.6249269843101501 || test loss : 0.619754433631897\n",
      " epoch :2140----> train loss : 0.623817503452301 || test loss : 0.618547797203064\n",
      " epoch :2150----> train loss : 0.6226943135261536 || test loss : 0.6173269152641296\n",
      " epoch :2160----> train loss : 0.6215572357177734 || test loss : 0.6160917282104492\n",
      " epoch :2170----> train loss : 0.6204062700271606 || test loss : 0.6148424744606018\n",
      " epoch :2180----> train loss : 0.6192424893379211 || test loss : 0.6135790944099426\n",
      " epoch :2190----> train loss : 0.6180657148361206 || test loss : 0.6123020648956299\n",
      " epoch :2200----> train loss : 0.6168755292892456 || test loss : 0.6110111474990845\n",
      " epoch :2210----> train loss : 0.6156728267669678 || test loss : 0.6097068786621094\n",
      " epoch :2220----> train loss : 0.6144568920135498 || test loss : 0.608388364315033\n",
      " epoch :2230----> train loss : 0.6132270693778992 || test loss : 0.6070553064346313\n",
      " epoch :2240----> train loss : 0.6119840741157532 || test loss : 0.6057075262069702\n",
      " epoch :2250----> train loss : 0.6107280850410461 || test loss : 0.6043452620506287\n",
      " epoch :2260----> train loss : 0.6094584465026855 || test loss : 0.6029694080352783\n",
      " epoch :2270----> train loss : 0.60817551612854 || test loss : 0.6015796661376953\n",
      " epoch :2280----> train loss : 0.6068791151046753 || test loss : 0.6001755595207214\n",
      " epoch :2290----> train loss : 0.6055696606636047 || test loss : 0.5987577438354492\n",
      " epoch :2300----> train loss : 0.6042477488517761 || test loss : 0.5973272323608398\n",
      " epoch :2310----> train loss : 0.6029130816459656 || test loss : 0.5958844423294067\n",
      " epoch :2320----> train loss : 0.6015651226043701 || test loss : 0.5944279432296753\n",
      " epoch :2330----> train loss : 0.6002039313316345 || test loss : 0.5929571986198425\n",
      " epoch :2340----> train loss : 0.5988292098045349 || test loss : 0.5914729833602905\n",
      " epoch :2350----> train loss : 0.597441554069519 || test loss : 0.58997642993927\n",
      " epoch :2360----> train loss : 0.5960413217544556 || test loss : 0.5884675979614258\n",
      " epoch :2370----> train loss : 0.594628632068634 || test loss : 0.5869468450546265\n",
      " epoch :2380----> train loss : 0.5932032465934753 || test loss : 0.5854140520095825\n",
      " epoch :2390----> train loss : 0.5917655229568481 || test loss : 0.5838685035705566\n",
      " epoch :2400----> train loss : 0.5903152823448181 || test loss : 0.5823105573654175\n",
      " epoch :2410----> train loss : 0.5888524651527405 || test loss : 0.580739438533783\n",
      " epoch :2420----> train loss : 0.5873768925666809 || test loss : 0.5791569948196411\n",
      " epoch :2430----> train loss : 0.5858886241912842 || test loss : 0.577563464641571\n",
      " epoch :2440----> train loss : 0.5843878388404846 || test loss : 0.5759575366973877\n",
      " epoch :2450----> train loss : 0.5828756093978882 || test loss : 0.5743396282196045\n",
      " epoch :2460----> train loss : 0.5813519358634949 || test loss : 0.5727102160453796\n",
      " epoch :2470----> train loss : 0.579816460609436 || test loss : 0.5710695385932922\n",
      " epoch :2480----> train loss : 0.5782696604728699 || test loss : 0.5694183707237244\n",
      " epoch :2490----> train loss : 0.5767112970352173 || test loss : 0.567757248878479\n",
      " epoch :2500----> train loss : 0.5751419067382812 || test loss : 0.5660858154296875\n",
      " epoch :2510----> train loss : 0.5735608339309692 || test loss : 0.5644033551216125\n",
      " epoch :2520----> train loss : 0.571969211101532 || test loss : 0.5627107620239258\n",
      " epoch :2530----> train loss : 0.5703662037849426 || test loss : 0.5610087513923645\n",
      " epoch :2540----> train loss : 0.5687519311904907 || test loss : 0.559295654296875\n",
      " epoch :2550----> train loss : 0.5671272277832031 || test loss : 0.5575734376907349\n",
      " epoch :2560----> train loss : 0.5654915571212769 || test loss : 0.555841863155365\n",
      " epoch :2570----> train loss : 0.563846230506897 || test loss : 0.554101824760437\n",
      " epoch :2580----> train loss : 0.5621909499168396 || test loss : 0.5523532032966614\n",
      " epoch :2590----> train loss : 0.5605255961418152 || test loss : 0.5505958199501038\n",
      " epoch :2600----> train loss : 0.5588504672050476 || test loss : 0.548829197883606\n",
      " epoch :2610----> train loss : 0.557165801525116 || test loss : 0.5470553636550903\n",
      " epoch :2620----> train loss : 0.5554717183113098 || test loss : 0.5452743768692017\n",
      " epoch :2630----> train loss : 0.5537689924240112 || test loss : 0.5434860587120056\n",
      " epoch :2640----> train loss : 0.5520575046539307 || test loss : 0.5416902303695679\n",
      " epoch :2650----> train loss : 0.550337553024292 || test loss : 0.539887011051178\n",
      " epoch :2660----> train loss : 0.5486093759536743 || test loss : 0.538075864315033\n",
      " epoch :2670----> train loss : 0.5468729734420776 || test loss : 0.5362579822540283\n",
      " epoch :2680----> train loss : 0.5451281666755676 || test loss : 0.5344341993331909\n",
      " epoch :2690----> train loss : 0.5433754920959473 || test loss : 0.5326045751571655\n",
      " epoch :2700----> train loss : 0.5416156053543091 || test loss : 0.5307693481445312\n",
      " epoch :2710----> train loss : 0.5398480296134949 || test loss : 0.5289280414581299\n",
      " epoch :2720----> train loss : 0.5380739569664001 || test loss : 0.5270820260047913\n",
      " epoch :2730----> train loss : 0.5362929701805115 || test loss : 0.5252306461334229\n",
      " epoch :2740----> train loss : 0.5345051884651184 || test loss : 0.5233750343322754\n",
      " epoch :2750----> train loss : 0.5327104926109314 || test loss : 0.5215145945549011\n",
      " epoch :2760----> train loss : 0.5309092998504639 || test loss : 0.5196511149406433\n",
      " epoch :2770----> train loss : 0.5291017889976501 || test loss : 0.5177834033966064\n",
      " epoch :2780----> train loss : 0.5272882580757141 || test loss : 0.5159133076667786\n",
      " epoch :2790----> train loss : 0.5254689455032349 || test loss : 0.5140407681465149\n",
      " epoch :2800----> train loss : 0.5236444473266602 || test loss : 0.5121643543243408\n",
      " epoch :2810----> train loss : 0.5218144655227661 || test loss : 0.5102847814559937\n",
      " epoch :2820----> train loss : 0.5199799537658691 || test loss : 0.508403480052948\n",
      " epoch :2830----> train loss : 0.5181410908699036 || test loss : 0.5065207481384277\n",
      " epoch :2840----> train loss : 0.5162975788116455 || test loss : 0.5046373009681702\n",
      " epoch :2850----> train loss : 0.5144498944282532 || test loss : 0.5027520656585693\n",
      " epoch :2860----> train loss : 0.5125980973243713 || test loss : 0.5008653998374939\n",
      " epoch :2870----> train loss : 0.5107425451278687 || test loss : 0.49897804856300354\n",
      " epoch :2880----> train loss : 0.5088831186294556 || test loss : 0.4970909357070923\n",
      " epoch :2890----> train loss : 0.5070198178291321 || test loss : 0.4952039122581482\n",
      " epoch :2900----> train loss : 0.5051530003547668 || test loss : 0.4933164715766907\n",
      " epoch :2910----> train loss : 0.5032833814620972 || test loss : 0.4914301335811615\n",
      " epoch :2920----> train loss : 0.5014113187789917 || test loss : 0.4895439147949219\n",
      " epoch :2930----> train loss : 0.49953705072402954 || test loss : 0.4876580238342285\n",
      " epoch :2940----> train loss : 0.4976605772972107 || test loss : 0.4857725501060486\n",
      " epoch :2950----> train loss : 0.4957817792892456 || test loss : 0.4838862419128418\n",
      " epoch :2960----> train loss : 0.49390098452568054 || test loss : 0.48200124502182007\n",
      " epoch :2970----> train loss : 0.4920186400413513 || test loss : 0.48011818528175354\n",
      " epoch :2980----> train loss : 0.49013522267341614 || test loss : 0.47823652625083923\n",
      " epoch :2990----> train loss : 0.4882503151893616 || test loss : 0.476357638835907\n",
      " epoch :3000----> train loss : 0.48636478185653687 || test loss : 0.47448161244392395\n",
      " epoch :3010----> train loss : 0.48447856307029724 || test loss : 0.47260770201683044\n",
      " epoch :3020----> train loss : 0.48259103298187256 || test loss : 0.47073599696159363\n",
      " epoch :3030----> train loss : 0.4807032644748688 || test loss : 0.46886610984802246\n",
      " epoch :3040----> train loss : 0.4788171350955963 || test loss : 0.4670009911060333\n",
      " epoch :3050----> train loss : 0.4769318401813507 || test loss : 0.46513989567756653\n",
      " epoch :3060----> train loss : 0.4750477373600006 || test loss : 0.46328189969062805\n",
      " epoch :3070----> train loss : 0.4731651246547699 || test loss : 0.46142810583114624\n",
      " epoch :3080----> train loss : 0.4712836742401123 || test loss : 0.4595789611339569\n",
      " epoch :3090----> train loss : 0.469404399394989 || test loss : 0.45773470401763916\n",
      " epoch :3100----> train loss : 0.4675273895263672 || test loss : 0.45589539408683777\n",
      " epoch :3110----> train loss : 0.46565306186676025 || test loss : 0.45406076312065125\n",
      " epoch :3120----> train loss : 0.4637807309627533 || test loss : 0.45223259925842285\n",
      " epoch :3130----> train loss : 0.46191123127937317 || test loss : 0.45041126012802124\n",
      " epoch :3140----> train loss : 0.4600440561771393 || test loss : 0.4485953450202942\n",
      " epoch :3150----> train loss : 0.45818036794662476 || test loss : 0.44678643345832825\n",
      " epoch :3160----> train loss : 0.45632123947143555 || test loss : 0.44498518109321594\n",
      " epoch :3170----> train loss : 0.45446687936782837 || test loss : 0.4431914985179901\n",
      " epoch :3180----> train loss : 0.45261621475219727 || test loss : 0.4414055049419403\n",
      " epoch :3190----> train loss : 0.45077088475227356 || test loss : 0.439627468585968\n",
      " epoch :3200----> train loss : 0.44893017411231995 || test loss : 0.43785834312438965\n",
      " epoch :3210----> train loss : 0.447094589471817 || test loss : 0.43609729409217834\n",
      " epoch :3220----> train loss : 0.4452654719352722 || test loss : 0.43434378504753113\n",
      " epoch :3230----> train loss : 0.4434427320957184 || test loss : 0.4325977861881256\n",
      " epoch :3240----> train loss : 0.4416259527206421 || test loss : 0.4308605194091797\n",
      " epoch :3250----> train loss : 0.43981489539146423 || test loss : 0.42913055419921875\n",
      " epoch :3260----> train loss : 0.4380097985267639 || test loss : 0.42740923166275024\n",
      " epoch :3270----> train loss : 0.436210572719574 || test loss : 0.4256975054740906\n",
      " epoch :3280----> train loss : 0.4344169795513153 || test loss : 0.4239935278892517\n",
      " epoch :3290----> train loss : 0.4326293170452118 || test loss : 0.4222988486289978\n",
      " epoch :3300----> train loss : 0.4308478534221649 || test loss : 0.42061540484428406\n",
      " epoch :3310----> train loss : 0.4290725588798523 || test loss : 0.4189417064189911\n",
      " epoch :3320----> train loss : 0.42730385065078735 || test loss : 0.41727757453918457\n",
      " epoch :3330----> train loss : 0.4255419373512268 || test loss : 0.4156220555305481\n",
      " epoch :3340----> train loss : 0.42378658056259155 || test loss : 0.41397449374198914\n",
      " epoch :3350----> train loss : 0.4220377206802368 || test loss : 0.4123362600803375\n",
      " epoch :3360----> train loss : 0.4202958047389984 || test loss : 0.41070792078971863\n",
      " epoch :3370----> train loss : 0.41856104135513306 || test loss : 0.4090878367424011\n",
      " epoch :3380----> train loss : 0.4168334901332855 || test loss : 0.40747731924057007\n",
      " epoch :3390----> train loss : 0.4151133894920349 || test loss : 0.40587541460990906\n",
      " epoch :3400----> train loss : 0.4134007394313812 || test loss : 0.40428343415260315\n",
      " epoch :3410----> train loss : 0.41169503331184387 || test loss : 0.40270164608955383\n",
      " epoch :3420----> train loss : 0.4099963903427124 || test loss : 0.40112894773483276\n",
      " epoch :3430----> train loss : 0.40830501914024353 || test loss : 0.39956527948379517\n",
      " epoch :3440----> train loss : 0.4066210389137268 || test loss : 0.39801225066185\n",
      " epoch :3450----> train loss : 0.40494483709335327 || test loss : 0.3964684307575226\n",
      " epoch :3460----> train loss : 0.4032762944698334 || test loss : 0.3949333131313324\n",
      " epoch :3470----> train loss : 0.40161558985710144 || test loss : 0.3934069871902466\n",
      " epoch :3480----> train loss : 0.3999628722667694 || test loss : 0.3918914198875427\n",
      " epoch :3490----> train loss : 0.39831802248954773 || test loss : 0.3903857469558716\n",
      " epoch :3500----> train loss : 0.39668095111846924 || test loss : 0.3888908624649048\n",
      " epoch :3510----> train loss : 0.39505183696746826 || test loss : 0.38740479946136475\n",
      " epoch :3520----> train loss : 0.3934304118156433 || test loss : 0.38592857122421265\n",
      " epoch :3530----> train loss : 0.39181655645370483 || test loss : 0.38446173071861267\n",
      " epoch :3540----> train loss : 0.3902103900909424 || test loss : 0.383004367351532\n",
      " epoch :3550----> train loss : 0.38861170411109924 || test loss : 0.38155612349510193\n",
      " epoch :3560----> train loss : 0.3870207369327545 || test loss : 0.3801190257072449\n",
      " epoch :3570----> train loss : 0.38543763756752014 || test loss : 0.378690630197525\n",
      " epoch :3580----> train loss : 0.3838624954223633 || test loss : 0.3772694766521454\n",
      " epoch :3590----> train loss : 0.38229498267173767 || test loss : 0.3758585453033447\n",
      " epoch :3600----> train loss : 0.38073524832725525 || test loss : 0.3744572699069977\n",
      " epoch :3610----> train loss : 0.37918326258659363 || test loss : 0.3730647563934326\n",
      " epoch :3620----> train loss : 0.3776390850543976 || test loss : 0.37168145179748535\n",
      " epoch :3630----> train loss : 0.3761025369167328 || test loss : 0.37030670046806335\n",
      " epoch :3640----> train loss : 0.3745734989643097 || test loss : 0.368941068649292\n",
      " epoch :3650----> train loss : 0.3730521500110626 || test loss : 0.36758384108543396\n",
      " epoch :3660----> train loss : 0.371538370847702 || test loss : 0.3662368059158325\n",
      " epoch :3670----> train loss : 0.3700321316719055 || test loss : 0.3648984730243683\n",
      " epoch :3680----> train loss : 0.36853355169296265 || test loss : 0.3635684847831726\n",
      " epoch :3690----> train loss : 0.3670426309108734 || test loss : 0.3622457683086395\n",
      " epoch :3700----> train loss : 0.36555927991867065 || test loss : 0.36093300580978394\n",
      " epoch :3710----> train loss : 0.36408329010009766 || test loss : 0.3596290349960327\n",
      " epoch :3720----> train loss : 0.36261481046676636 || test loss : 0.3583342432975769\n",
      " epoch :3730----> train loss : 0.3611537218093872 || test loss : 0.3570486605167389\n",
      " epoch :3740----> train loss : 0.35969972610473633 || test loss : 0.35577112436294556\n",
      " epoch :3750----> train loss : 0.35825279355049133 || test loss : 0.35450127720832825\n",
      " epoch :3760----> train loss : 0.35681280493736267 || test loss : 0.35323965549468994\n",
      " epoch :3770----> train loss : 0.35537979006767273 || test loss : 0.35198673605918884\n",
      " epoch :3780----> train loss : 0.3539535403251648 || test loss : 0.350741982460022\n",
      " epoch :3790----> train loss : 0.3525342643260956 || test loss : 0.349504679441452\n",
      " epoch :3800----> train loss : 0.35112208127975464 || test loss : 0.3482752740383148\n",
      " epoch :3810----> train loss : 0.3497168719768524 || test loss : 0.34705379605293274\n",
      " epoch :3820----> train loss : 0.3483184278011322 || test loss : 0.3458411693572998\n",
      " epoch :3830----> train loss : 0.34692665934562683 || test loss : 0.34463632106781006\n",
      " epoch :3840----> train loss : 0.3455415368080139 || test loss : 0.3434397578239441\n",
      " epoch :3850----> train loss : 0.34416303038597107 || test loss : 0.34225142002105713\n",
      " epoch :3860----> train loss : 0.34279102087020874 || test loss : 0.3410695195198059\n",
      " epoch :3870----> train loss : 0.34142541885375977 || test loss : 0.339895099401474\n",
      " epoch :3880----> train loss : 0.34006619453430176 || test loss : 0.33872777223587036\n",
      " epoch :3890----> train loss : 0.33871322870254517 || test loss : 0.33756718039512634\n",
      " epoch :3900----> train loss : 0.33736661076545715 || test loss : 0.33641374111175537\n",
      " epoch :3910----> train loss : 0.33602607250213623 || test loss : 0.335267037153244\n",
      " epoch :3920----> train loss : 0.33469149470329285 || test loss : 0.33412837982177734\n",
      " epoch :3930----> train loss : 0.33336299657821655 || test loss : 0.33299514651298523\n",
      " epoch :3940----> train loss : 0.3320407271385193 || test loss : 0.3318692147731781\n",
      " epoch :3950----> train loss : 0.33072444796562195 || test loss : 0.3307495713233948\n",
      " epoch :3960----> train loss : 0.32941415905952454 || test loss : 0.32963716983795166\n",
      " epoch :3970----> train loss : 0.32810965180397034 || test loss : 0.3285316824913025\n",
      " epoch :3980----> train loss : 0.3268110156059265 || test loss : 0.327432781457901\n",
      " epoch :3990----> train loss : 0.3255181610584259 || test loss : 0.3263399302959442\n",
      " epoch :4000----> train loss : 0.3242309093475342 || test loss : 0.32525357604026794\n",
      " epoch :4010----> train loss : 0.32294920086860657 || test loss : 0.3241734802722931\n",
      " epoch :4020----> train loss : 0.3216729462146759 || test loss : 0.3230991065502167\n",
      " epoch :4030----> train loss : 0.32040220499038696 || test loss : 0.32203128933906555\n",
      " epoch :4040----> train loss : 0.3191368579864502 || test loss : 0.3209688663482666\n",
      " epoch :4050----> train loss : 0.3178768455982208 || test loss : 0.3199124336242676\n",
      " epoch :4060----> train loss : 0.3166220188140869 || test loss : 0.3188616633415222\n",
      " epoch :4070----> train loss : 0.31537240743637085 || test loss : 0.3178166449069977\n",
      " epoch :4080----> train loss : 0.31412792205810547 || test loss : 0.31677791476249695\n",
      " epoch :4090----> train loss : 0.31288859248161316 || test loss : 0.31574466824531555\n",
      " epoch :4100----> train loss : 0.3116544187068939 || test loss : 0.31471768021583557\n",
      " epoch :4110----> train loss : 0.31042540073394775 || test loss : 0.3136966824531555\n",
      " epoch :4120----> train loss : 0.3092012107372284 || test loss : 0.3126811385154724\n",
      " epoch :4130----> train loss : 0.307981938123703 || test loss : 0.3116714358329773\n",
      " epoch :4140----> train loss : 0.3067675232887268 || test loss : 0.3106662333011627\n",
      " epoch :4150----> train loss : 0.3055579364299774 || test loss : 0.3096669912338257\n",
      " epoch :4160----> train loss : 0.30435317754745483 || test loss : 0.30867376923561096\n",
      " epoch :4170----> train loss : 0.30315327644348145 || test loss : 0.30768486857414246\n",
      " epoch :4180----> train loss : 0.3019580543041229 || test loss : 0.3067006766796112\n",
      " epoch :4190----> train loss : 0.30076733231544495 || test loss : 0.30572131276130676\n",
      " epoch :4200----> train loss : 0.2995811402797699 || test loss : 0.30474743247032166\n",
      " epoch :4210----> train loss : 0.29839953780174255 || test loss : 0.3037790358066559\n",
      " epoch :4220----> train loss : 0.29722222685813904 || test loss : 0.3028154969215393\n",
      " epoch :4230----> train loss : 0.2960492968559265 || test loss : 0.301856130361557\n",
      " epoch :4240----> train loss : 0.294880747795105 || test loss : 0.30090078711509705\n",
      " epoch :4250----> train loss : 0.293716698884964 || test loss : 0.2999511957168579\n",
      " epoch :4260----> train loss : 0.292557030916214 || test loss : 0.29900622367858887\n",
      " epoch :4270----> train loss : 0.29140162467956543 || test loss : 0.2980648875236511\n",
      " epoch :4280----> train loss : 0.2902505099773407 || test loss : 0.2971287667751312\n",
      " epoch :4290----> train loss : 0.28910350799560547 || test loss : 0.2961978614330292\n",
      " epoch :4300----> train loss : 0.28796079754829407 || test loss : 0.29527172446250916\n",
      " epoch :4310----> train loss : 0.28682222962379456 || test loss : 0.29434990882873535\n",
      " epoch :4320----> train loss : 0.2856878638267517 || test loss : 0.29343241453170776\n",
      " epoch :4330----> train loss : 0.284557580947876 || test loss : 0.2925196588039398\n",
      " epoch :4340----> train loss : 0.2834312915802002 || test loss : 0.29161182045936584\n",
      " epoch :4350----> train loss : 0.28230905532836914 || test loss : 0.2907077968120575\n",
      " epoch :4360----> train loss : 0.2811908721923828 || test loss : 0.2898072600364685\n",
      " epoch :4370----> train loss : 0.28007668256759644 || test loss : 0.2889121472835541\n",
      " epoch :4380----> train loss : 0.27896636724472046 || test loss : 0.288021981716156\n",
      " epoch :4390----> train loss : 0.27785998582839966 || test loss : 0.28713512420654297\n",
      " epoch :4400----> train loss : 0.2767575681209564 || test loss : 0.28625237941741943\n",
      " epoch :4410----> train loss : 0.27565908432006836 || test loss : 0.2853745222091675\n",
      " epoch :4420----> train loss : 0.27456438541412354 || test loss : 0.28450074791908264\n",
      " epoch :4430----> train loss : 0.2734733521938324 || test loss : 0.28363052010536194\n",
      " epoch :4440----> train loss : 0.2723861038684845 || test loss : 0.2827652096748352\n",
      " epoch :4450----> train loss : 0.27130255103111267 || test loss : 0.2819046974182129\n",
      " epoch :4460----> train loss : 0.27022284269332886 || test loss : 0.28104671835899353\n",
      " epoch :4470----> train loss : 0.26914680004119873 || test loss : 0.2801930010318756\n",
      " epoch :4480----> train loss : 0.2680744528770447 || test loss : 0.2793434262275696\n",
      " epoch :4490----> train loss : 0.2670058012008667 || test loss : 0.2784973978996277\n",
      " epoch :4500----> train loss : 0.2659408152103424 || test loss : 0.2776558995246887\n",
      " epoch :4510----> train loss : 0.26487934589385986 || test loss : 0.27681824564933777\n",
      " epoch :4520----> train loss : 0.26382145285606384 || test loss : 0.2759840488433838\n",
      " epoch :4530----> train loss : 0.26276710629463196 || test loss : 0.27515336871147156\n",
      " epoch :4540----> train loss : 0.26171618700027466 || test loss : 0.27432581782341003\n",
      " epoch :4550----> train loss : 0.2606687843799591 || test loss : 0.2735026180744171\n",
      " epoch :4560----> train loss : 0.2596248984336853 || test loss : 0.27268263697624207\n",
      " epoch :4570----> train loss : 0.2585844397544861 || test loss : 0.27186599373817444\n",
      " epoch :4580----> train loss : 0.25754740834236145 || test loss : 0.27105358242988586\n",
      " epoch :4590----> train loss : 0.25651395320892334 || test loss : 0.27024489641189575\n",
      " epoch :4600----> train loss : 0.25548407435417175 || test loss : 0.26943978667259216\n",
      " epoch :4610----> train loss : 0.2544576823711395 || test loss : 0.2686383128166199\n",
      " epoch :4620----> train loss : 0.2534346282482147 || test loss : 0.26784056425094604\n",
      " epoch :4630----> train loss : 0.25241515040397644 || test loss : 0.267046183347702\n",
      " epoch :4640----> train loss : 0.2513989508152008 || test loss : 0.26625609397888184\n",
      " epoch :4650----> train loss : 0.25038617849349976 || test loss : 0.2654690146446228\n",
      " epoch :4660----> train loss : 0.2493768185377121 || test loss : 0.2646855413913727\n",
      " epoch :4670----> train loss : 0.24837082624435425 || test loss : 0.26390522718429565\n",
      " epoch :4680----> train loss : 0.24736812710762024 || test loss : 0.2631288170814514\n",
      " epoch :4690----> train loss : 0.24636878073215485 || test loss : 0.26235631108283997\n",
      " epoch :4700----> train loss : 0.24537281692028046 || test loss : 0.2615875005722046\n",
      " epoch :4710----> train loss : 0.24438010156154633 || test loss : 0.260821670293808\n",
      " epoch :4720----> train loss : 0.24339067935943604 || test loss : 0.2600594460964203\n",
      " epoch :4730----> train loss : 0.2424042671918869 || test loss : 0.2592999041080475\n",
      " epoch :4740----> train loss : 0.2414211630821228 || test loss : 0.2585444152355194\n",
      " epoch :4750----> train loss : 0.24044109880924225 || test loss : 0.25779154896736145\n",
      " epoch :4760----> train loss : 0.23946429789066315 || test loss : 0.2570420801639557\n",
      " epoch :4770----> train loss : 0.23849080502986908 || test loss : 0.2562966048717499\n",
      " epoch :4780----> train loss : 0.23752030730247498 || test loss : 0.2555546462535858\n",
      " epoch :4790----> train loss : 0.23655277490615845 || test loss : 0.2548152506351471\n",
      " epoch :4800----> train loss : 0.23558859527111053 || test loss : 0.25407904386520386\n",
      " epoch :4810----> train loss : 0.23462837934494019 || test loss : 0.25334620475769043\n",
      " epoch :4820----> train loss : 0.2336713969707489 || test loss : 0.2526160776615143\n",
      " epoch :4830----> train loss : 0.23271776735782623 || test loss : 0.2518891990184784\n",
      " epoch :4840----> train loss : 0.23176737129688263 || test loss : 0.25116536021232605\n",
      " epoch :4850----> train loss : 0.23082037270069122 || test loss : 0.2504444718360901\n",
      " epoch :4860----> train loss : 0.22987672686576843 || test loss : 0.24972687661647797\n",
      " epoch :4870----> train loss : 0.22893637418746948 || test loss : 0.24901269376277924\n",
      " epoch :4880----> train loss : 0.22799916565418243 || test loss : 0.2483016550540924\n",
      " epoch :4890----> train loss : 0.22706541419029236 || test loss : 0.24759389460086823\n",
      " epoch :4900----> train loss : 0.22613508999347687 || test loss : 0.24688957631587982\n",
      " epoch :4910----> train loss : 0.22520823776721954 || test loss : 0.24618908762931824\n",
      " epoch :4920----> train loss : 0.22428478300571442 || test loss : 0.2454914003610611\n",
      " epoch :4930----> train loss : 0.22336453199386597 || test loss : 0.24479620158672333\n",
      " epoch :4940----> train loss : 0.22244751453399658 || test loss : 0.24410390853881836\n",
      " epoch :4950----> train loss : 0.22153370082378387 || test loss : 0.24341541528701782\n",
      " epoch :4960----> train loss : 0.2206232100725174 || test loss : 0.24273064732551575\n",
      " epoch :4970----> train loss : 0.21971602737903595 || test loss : 0.24204836785793304\n",
      " epoch :4980----> train loss : 0.2188122421503067 || test loss : 0.24136972427368164\n",
      " epoch :4990----> train loss : 0.21791166067123413 || test loss : 0.24069422483444214\n",
      " epoch :5000----> train loss : 0.21701429784297943 || test loss : 0.24002186954021454\n",
      " epoch :5010----> train loss : 0.216120183467865 || test loss : 0.2393522560596466\n",
      " epoch :5020----> train loss : 0.215229332447052 || test loss : 0.2386847883462906\n",
      " epoch :5030----> train loss : 0.21434161067008972 || test loss : 0.2380206435918808\n",
      " epoch :5040----> train loss : 0.21345701813697815 || test loss : 0.23735995590686798\n",
      " epoch :5050----> train loss : 0.21257567405700684 || test loss : 0.23670266568660736\n",
      " epoch :5060----> train loss : 0.2116975337266922 || test loss : 0.23604755103588104\n",
      " epoch :5070----> train loss : 0.21082262694835663 || test loss : 0.23539592325687408\n",
      " epoch :5080----> train loss : 0.20995093882083893 || test loss : 0.23474717140197754\n",
      " epoch :5090----> train loss : 0.2090824991464615 || test loss : 0.23410142958164215\n",
      " epoch :5100----> train loss : 0.2082173228263855 || test loss : 0.23345838487148285\n",
      " epoch :5110----> train loss : 0.20735542476177216 || test loss : 0.232818603515625\n",
      " epoch :5120----> train loss : 0.2064967304468155 || test loss : 0.23218180239200592\n",
      " epoch :5130----> train loss : 0.2056412696838379 || test loss : 0.2315475046634674\n",
      " epoch :5140----> train loss : 0.20478898286819458 || test loss : 0.23091676831245422\n",
      " epoch :5150----> train loss : 0.2039397954940796 || test loss : 0.23028890788555145\n",
      " epoch :5160----> train loss : 0.20309382677078247 || test loss : 0.22966350615024567\n",
      " epoch :5170----> train loss : 0.202251136302948 || test loss : 0.2290406972169876\n",
      " epoch :5180----> train loss : 0.20141154527664185 || test loss : 0.22842085361480713\n",
      " epoch :5190----> train loss : 0.20057517290115356 || test loss : 0.2278037667274475\n",
      " epoch :5200----> train loss : 0.19974198937416077 || test loss : 0.22718995809555054\n",
      " epoch :5210----> train loss : 0.19891199469566345 || test loss : 0.22657941281795502\n",
      " epoch :5220----> train loss : 0.198085218667984 || test loss : 0.22597114741802216\n",
      " epoch :5230----> train loss : 0.19726161658763885 || test loss : 0.22536583244800568\n",
      " epoch :5240----> train loss : 0.19644123315811157 || test loss : 0.22476285696029663\n",
      " epoch :5250----> train loss : 0.1956239938735962 || test loss : 0.22416293621063232\n",
      " epoch :5260----> train loss : 0.1948099434375763 || test loss : 0.22356508672237396\n",
      " epoch :5270----> train loss : 0.1939990073442459 || test loss : 0.2229701578617096\n",
      " epoch :5280----> train loss : 0.19319123029708862 || test loss : 0.22237776219844818\n",
      " epoch :5290----> train loss : 0.19238652288913727 || test loss : 0.22178801894187927\n",
      " epoch :5300----> train loss : 0.19158490002155304 || test loss : 0.2212013453245163\n",
      " epoch :5310----> train loss : 0.19078639149665833 || test loss : 0.220617413520813\n",
      " epoch :5320----> train loss : 0.18999098241329193 || test loss : 0.2200363576412201\n",
      " epoch :5330----> train loss : 0.18919862806797028 || test loss : 0.21945863962173462\n",
      " epoch :5340----> train loss : 0.18840940296649933 || test loss : 0.21888285875320435\n",
      " epoch :5350----> train loss : 0.18762324750423431 || test loss : 0.2183091789484024\n",
      " epoch :5360----> train loss : 0.18684029579162598 || test loss : 0.21773836016654968\n",
      " epoch :5370----> train loss : 0.18606045842170715 || test loss : 0.21716976165771484\n",
      " epoch :5380----> train loss : 0.18528364598751068 || test loss : 0.216604545712471\n",
      " epoch :5390----> train loss : 0.18450988829135895 || test loss : 0.2160416692495346\n",
      " epoch :5400----> train loss : 0.1837393045425415 || test loss : 0.2154809832572937\n",
      " epoch :5410----> train loss : 0.1829717755317688 || test loss : 0.2149229347705841\n",
      " epoch :5420----> train loss : 0.1822074055671692 || test loss : 0.2143678367137909\n",
      " epoch :5430----> train loss : 0.18144617974758148 || test loss : 0.21381524205207825\n",
      " epoch :5440----> train loss : 0.18068808317184448 || test loss : 0.21326516568660736\n",
      " epoch :5450----> train loss : 0.17993320524692535 || test loss : 0.21271762251853943\n",
      " epoch :5460----> train loss : 0.17918142676353455 || test loss : 0.21217280626296997\n",
      " epoch :5470----> train loss : 0.17843274772167206 || test loss : 0.21162989735603333\n",
      " epoch :5480----> train loss : 0.17768721282482147 || test loss : 0.2110898494720459\n",
      " epoch :5490----> train loss : 0.1769448071718216 || test loss : 0.21055273711681366\n",
      " epoch :5500----> train loss : 0.17620554566383362 || test loss : 0.2100181132555008\n",
      " epoch :5510----> train loss : 0.17546935379505157 || test loss : 0.20948627591133118\n",
      " epoch :5520----> train loss : 0.17473629117012024 || test loss : 0.20895743370056152\n",
      " epoch :5530----> train loss : 0.17400631308555603 || test loss : 0.20843061804771423\n",
      " epoch :5540----> train loss : 0.17327938973903656 || test loss : 0.20790547132492065\n",
      " epoch :5550----> train loss : 0.17255550622940063 || test loss : 0.20738321542739868\n",
      " epoch :5560----> train loss : 0.17183466255664825 || test loss : 0.20686288177967072\n",
      " epoch :5570----> train loss : 0.17111682891845703 || test loss : 0.2063450962305069\n",
      " epoch :5580----> train loss : 0.17040207982063293 || test loss : 0.20583008229732513\n",
      " epoch :5590----> train loss : 0.16969038546085358 || test loss : 0.20531703531742096\n",
      " epoch :5600----> train loss : 0.16898173093795776 || test loss : 0.20480668544769287\n",
      " epoch :5610----> train loss : 0.16827614605426788 || test loss : 0.20429843664169312\n",
      " epoch :5620----> train loss : 0.16757360100746155 || test loss : 0.20379337668418884\n",
      " epoch :5630----> train loss : 0.16687408089637756 || test loss : 0.2032911628484726\n",
      " epoch :5640----> train loss : 0.16617757081985474 || test loss : 0.20279063284397125\n",
      " epoch :5650----> train loss : 0.16548408567905426 || test loss : 0.2022932469844818\n",
      " epoch :5660----> train loss : 0.16479362547397614 || test loss : 0.20179817080497742\n",
      " epoch :5670----> train loss : 0.16410619020462036 || test loss : 0.20130516588687897\n",
      " epoch :5680----> train loss : 0.16342176496982574 || test loss : 0.20081469416618347\n",
      " epoch :5690----> train loss : 0.1627402901649475 || test loss : 0.2003260850906372\n",
      " epoch :5700----> train loss : 0.16206179559230804 || test loss : 0.19983965158462524\n",
      " epoch :5710----> train loss : 0.16138631105422974 || test loss : 0.19935524463653564\n",
      " epoch :5720----> train loss : 0.16071385145187378 || test loss : 0.198872908949852\n",
      " epoch :5730----> train loss : 0.1600443571805954 || test loss : 0.19839292764663696\n",
      " epoch :5740----> train loss : 0.1593778431415558 || test loss : 0.19791536033153534\n",
      " epoch :5750----> train loss : 0.15871429443359375 || test loss : 0.19744005799293518\n",
      " epoch :5760----> train loss : 0.1580536961555481 || test loss : 0.19696736335754395\n",
      " epoch :5770----> train loss : 0.1573960781097412 || test loss : 0.1964966058731079\n",
      " epoch :5780----> train loss : 0.15674139559268951 || test loss : 0.19602838158607483\n",
      " epoch :5790----> train loss : 0.1560896933078766 || test loss : 0.1955624371767044\n",
      " epoch :5800----> train loss : 0.15544088184833527 || test loss : 0.19509859383106232\n",
      " epoch :5810----> train loss : 0.15479505062103271 || test loss : 0.19463729858398438\n",
      " epoch :5820----> train loss : 0.15415212512016296 || test loss : 0.19417783617973328\n",
      " epoch :5830----> train loss : 0.1535121500492096 || test loss : 0.1937199980020523\n",
      " epoch :5840----> train loss : 0.15287503600120544 || test loss : 0.19326487183570862\n",
      " epoch :5850----> train loss : 0.1522408425807953 || test loss : 0.19281186163425446\n",
      " epoch :5860----> train loss : 0.15160952508449554 || test loss : 0.1923612654209137\n",
      " epoch :5870----> train loss : 0.1509810835123062 || test loss : 0.19191224873065948\n",
      " epoch :5880----> train loss : 0.1503555029630661 || test loss : 0.191465824842453\n",
      " epoch :5890----> train loss : 0.149732768535614 || test loss : 0.19102133810520172\n",
      " epoch :5900----> train loss : 0.14911289513111115 || test loss : 0.19057892262935638\n",
      " epoch :5910----> train loss : 0.1484958380460739 || test loss : 0.1901387721300125\n",
      " epoch :5920----> train loss : 0.14788158237934113 || test loss : 0.18970023095607758\n",
      " epoch :5930----> train loss : 0.14727015793323517 || test loss : 0.18926407396793365\n",
      " epoch :5940----> train loss : 0.14666154980659485 || test loss : 0.18883013725280762\n",
      " epoch :5950----> train loss : 0.14605575799942017 || test loss : 0.1883988380432129\n",
      " epoch :5960----> train loss : 0.14545276761054993 || test loss : 0.18796925246715546\n",
      " epoch :5970----> train loss : 0.14485260844230652 || test loss : 0.1875418722629547\n",
      " epoch :5980----> train loss : 0.14425525069236755 || test loss : 0.1871163547039032\n",
      " epoch :5990----> train loss : 0.14366070926189423 || test loss : 0.18669365346431732\n",
      " epoch :6000----> train loss : 0.14306896924972534 || test loss : 0.18627257645130157\n",
      " epoch :6010----> train loss : 0.1424800008535385 || test loss : 0.18585346639156342\n",
      " epoch :6020----> train loss : 0.14189378917217255 || test loss : 0.1854362040758133\n",
      " epoch :6030----> train loss : 0.14131031930446625 || test loss : 0.18502099812030792\n",
      " epoch :6040----> train loss : 0.14072957634925842 || test loss : 0.18460771441459656\n",
      " epoch :6050----> train loss : 0.14015156030654907 || test loss : 0.18419639766216278\n",
      " epoch :6060----> train loss : 0.1395762860774994 || test loss : 0.18378739058971405\n",
      " epoch :6070----> train loss : 0.13900373876094818 || test loss : 0.18338032066822052\n",
      " epoch :6080----> train loss : 0.13843387365341187 || test loss : 0.1829761415719986\n",
      " epoch :6090----> train loss : 0.13786675035953522 || test loss : 0.18257345259189606\n",
      " epoch :6100----> train loss : 0.13730230927467346 || test loss : 0.18217231333255768\n",
      " epoch :6110----> train loss : 0.1367405205965042 || test loss : 0.18177296221256256\n",
      " epoch :6120----> train loss : 0.13618142902851105 || test loss : 0.18137557804584503\n",
      " epoch :6130----> train loss : 0.1356249898672104 || test loss : 0.18097993731498718\n",
      " epoch :6140----> train loss : 0.13507120311260223 || test loss : 0.1805865615606308\n",
      " epoch :6150----> train loss : 0.1345200538635254 || test loss : 0.18019479513168335\n",
      " epoch :6160----> train loss : 0.13397152721881866 || test loss : 0.17980509996414185\n",
      " epoch :6170----> train loss : 0.13342556357383728 || test loss : 0.1794171780347824\n",
      " epoch :6180----> train loss : 0.132882222533226 || test loss : 0.17903122305870056\n",
      " epoch :6190----> train loss : 0.13234145939350128 || test loss : 0.1786470115184784\n",
      " epoch :6200----> train loss : 0.13180330395698547 || test loss : 0.17826491594314575\n",
      " epoch :6210----> train loss : 0.1312676966190338 || test loss : 0.17788422107696533\n",
      " epoch :6220----> train loss : 0.13073468208312988 || test loss : 0.17750592529773712\n",
      " epoch :6230----> train loss : 0.1302042305469513 || test loss : 0.1771288812160492\n",
      " epoch :6240----> train loss : 0.12967632710933685 || test loss : 0.1767539381980896\n",
      " epoch :6250----> train loss : 0.12915095686912537 || test loss : 0.17638114094734192\n",
      " epoch :6260----> train loss : 0.12862814962863922 || test loss : 0.1760099232196808\n",
      " epoch :6270----> train loss : 0.12810783088207245 || test loss : 0.1756405532360077\n",
      " epoch :6280----> train loss : 0.12759003043174744 || test loss : 0.17527294158935547\n",
      " epoch :6290----> train loss : 0.1270747184753418 || test loss : 0.1749071180820465\n",
      " epoch :6300----> train loss : 0.12656190991401672 || test loss : 0.17454296350479126\n",
      " epoch :6310----> train loss : 0.12605158984661102 || test loss : 0.17418064177036285\n",
      " epoch :6320----> train loss : 0.1255437433719635 || test loss : 0.17381969094276428\n",
      " epoch :6330----> train loss : 0.12503838539123535 || test loss : 0.1734606772661209\n",
      " epoch :6340----> train loss : 0.12453552335500717 || test loss : 0.17310366034507751\n",
      " epoch :6350----> train loss : 0.12403513491153717 || test loss : 0.1727483868598938\n",
      " epoch :6360----> train loss : 0.12353719025850296 || test loss : 0.17239464819431305\n",
      " epoch :6370----> train loss : 0.12304172664880753 || test loss : 0.1720423549413681\n",
      " epoch :6380----> train loss : 0.1225486546754837 || test loss : 0.17169195413589478\n",
      " epoch :6390----> train loss : 0.12205803394317627 || test loss : 0.17134307324886322\n",
      " epoch :6400----> train loss : 0.12156981229782104 || test loss : 0.17099575698375702\n",
      " epoch :6410----> train loss : 0.12108396738767624 || test loss : 0.17065031826496124\n",
      " epoch :6420----> train loss : 0.12060052901506424 || test loss : 0.1703069657087326\n",
      " epoch :6430----> train loss : 0.12011947482824326 || test loss : 0.1699654459953308\n",
      " epoch :6440----> train loss : 0.1196407750248909 || test loss : 0.1696251481771469\n",
      " epoch :6450----> train loss : 0.11916446685791016 || test loss : 0.1692870855331421\n",
      " epoch :6460----> train loss : 0.11869049817323685 || test loss : 0.16895011067390442\n",
      " epoch :6470----> train loss : 0.11821889132261276 || test loss : 0.16861514747142792\n",
      " epoch :6480----> train loss : 0.11774957925081253 || test loss : 0.16828159987926483\n",
      " epoch :6490----> train loss : 0.11728261411190033 || test loss : 0.1679498851299286\n",
      " epoch :6500----> train loss : 0.11681795120239258 || test loss : 0.16761994361877441\n",
      " epoch :6510----> train loss : 0.11635559052228928 || test loss : 0.16729161143302917\n",
      " epoch :6520----> train loss : 0.11589551717042923 || test loss : 0.16696502268314362\n",
      " epoch :6530----> train loss : 0.11543771624565125 || test loss : 0.16663961112499237\n",
      " epoch :6540----> train loss : 0.11498219519853592 || test loss : 0.16631600260734558\n",
      " epoch :6550----> train loss : 0.11452891677618027 || test loss : 0.16599388420581818\n",
      " epoch :6560----> train loss : 0.11407791823148727 || test loss : 0.1656729131937027\n",
      " epoch :6570----> train loss : 0.11362910270690918 || test loss : 0.16535378992557526\n",
      " epoch :6580----> train loss : 0.11318252980709076 || test loss : 0.16503629088401794\n",
      " epoch :6590----> train loss : 0.11273818463087082 || test loss : 0.16472052037715912\n",
      " epoch :6600----> train loss : 0.11229606717824936 || test loss : 0.16440626978874207\n",
      " epoch :6610----> train loss : 0.11185614764690399 || test loss : 0.16409362852573395\n",
      " epoch :6620----> train loss : 0.11141843348741531 || test loss : 0.16378255188465118\n",
      " epoch :6630----> train loss : 0.11098290979862213 || test loss : 0.16347312927246094\n",
      " epoch :6640----> train loss : 0.11054955422878265 || test loss : 0.16316518187522888\n",
      " epoch :6650----> train loss : 0.11011835187673569 || test loss : 0.16285847127437592\n",
      " epoch :6660----> train loss : 0.10968929529190063 || test loss : 0.16255322098731995\n",
      " epoch :6670----> train loss : 0.1092623770236969 || test loss : 0.16224941611289978\n",
      " epoch :6680----> train loss : 0.10883758217096329 || test loss : 0.16194716095924377\n",
      " epoch :6690----> train loss : 0.10841493308544159 || test loss : 0.1616465002298355\n",
      " epoch :6700----> train loss : 0.10799439251422882 || test loss : 0.16134746372699738\n",
      " epoch :6710----> train loss : 0.10757595300674438 || test loss : 0.16104984283447266\n",
      " epoch :6720----> train loss : 0.10715961456298828 || test loss : 0.16075356304645538\n",
      " epoch :6730----> train loss : 0.10674534738063812 || test loss : 0.16045884788036346\n",
      " epoch :6740----> train loss : 0.10633315145969391 || test loss : 0.16016586124897003\n",
      " epoch :6750----> train loss : 0.10592301934957504 || test loss : 0.15987394750118256\n",
      " epoch :6760----> train loss : 0.10551495850086212 || test loss : 0.15958382189273834\n",
      " epoch :6770----> train loss : 0.10510893166065216 || test loss : 0.15929491817951202\n",
      " epoch :6780----> train loss : 0.10470492392778397 || test loss : 0.1590074896812439\n",
      " epoch :6790----> train loss : 0.10430297255516052 || test loss : 0.15872156620025635\n",
      " epoch :6800----> train loss : 0.10390303283929825 || test loss : 0.1584368348121643\n",
      " epoch :6810----> train loss : 0.10350509732961655 || test loss : 0.15815365314483643\n",
      " epoch :6820----> train loss : 0.10310912877321243 || test loss : 0.1578717976808548\n",
      " epoch :6830----> train loss : 0.1027151420712471 || test loss : 0.1575917750597\n",
      " epoch :6840----> train loss : 0.10232314467430115 || test loss : 0.15731269121170044\n",
      " epoch :6850----> train loss : 0.10193312913179398 || test loss : 0.15703493356704712\n",
      " epoch :6860----> train loss : 0.1015450656414032 || test loss : 0.15675851702690125\n",
      " epoch :6870----> train loss : 0.10115894675254822 || test loss : 0.1564835011959076\n",
      " epoch :6880----> train loss : 0.10077476501464844 || test loss : 0.15620958805084229\n",
      " epoch :6890----> train loss : 0.10039248317480087 || test loss : 0.15593703091144562\n",
      " epoch :6900----> train loss : 0.10001207143068314 || test loss : 0.1556662917137146\n",
      " epoch :6910----> train loss : 0.0996335819363594 || test loss : 0.15539668500423431\n",
      " epoch :6920----> train loss : 0.09925699234008789 || test loss : 0.15512816607952118\n",
      " epoch :6930----> train loss : 0.098882295191288 || test loss : 0.15486101806163788\n",
      " epoch :6940----> train loss : 0.09850949048995972 || test loss : 0.15459561347961426\n",
      " epoch :6950----> train loss : 0.09813855588436127 || test loss : 0.1543315052986145\n",
      " epoch :6960----> train loss : 0.09776949882507324 || test loss : 0.15406830608844757\n",
      " epoch :6970----> train loss : 0.09740230441093445 || test loss : 0.15380656719207764\n",
      " epoch :6980----> train loss : 0.09703697264194489 || test loss : 0.15354599058628082\n",
      " epoch :6990----> train loss : 0.09667346626520157 || test loss : 0.1532871127128601\n",
      " epoch :7000----> train loss : 0.09631180018186569 || test loss : 0.15302930772304535\n",
      " epoch :7010----> train loss : 0.09595195204019547 || test loss : 0.15277308225631714\n",
      " epoch :7020----> train loss : 0.09559393674135208 || test loss : 0.1525179147720337\n",
      " epoch :7030----> train loss : 0.09523770958185196 || test loss : 0.15226419270038605\n",
      " epoch :7040----> train loss : 0.09488330036401749 || test loss : 0.15201140940189362\n",
      " epoch :7050----> train loss : 0.09453068673610687 || test loss : 0.15175995230674744\n",
      " epoch :7060----> train loss : 0.09417984634637833 || test loss : 0.15150970220565796\n",
      " epoch :7070----> train loss : 0.09383077174425125 || test loss : 0.1512604057788849\n",
      " epoch :7080----> train loss : 0.09348345547914505 || test loss : 0.15101252496242523\n",
      " epoch :7090----> train loss : 0.09313787519931793 || test loss : 0.15076638758182526\n",
      " epoch :7100----> train loss : 0.09279404580593109 || test loss : 0.15052103996276855\n",
      " epoch :7110----> train loss : 0.09245195239782333 || test loss : 0.1502770185470581\n",
      " epoch :7120----> train loss : 0.09211155772209167 || test loss : 0.1500343680381775\n",
      " epoch :7130----> train loss : 0.0917728915810585 || test loss : 0.14979246258735657\n",
      " epoch :7140----> train loss : 0.09143591672182083 || test loss : 0.14955173432826996\n",
      " epoch :7150----> train loss : 0.09110064059495926 || test loss : 0.1493125706911087\n",
      " epoch :7160----> train loss : 0.090767040848732 || test loss : 0.1490744650363922\n",
      " epoch :7170----> train loss : 0.09043513983488083 || test loss : 0.14883723855018616\n",
      " epoch :7180----> train loss : 0.09010490775108337 || test loss : 0.14860153198242188\n",
      " epoch :7190----> train loss : 0.08977635204792023 || test loss : 0.14836695790290833\n",
      " epoch :7200----> train loss : 0.0894494280219078 || test loss : 0.14813368022441864\n",
      " epoch :7210----> train loss : 0.0891241505742073 || test loss : 0.1479015350341797\n",
      " epoch :7220----> train loss : 0.08880051970481873 || test loss : 0.14767074584960938\n",
      " epoch :7230----> train loss : 0.08847852051258087 || test loss : 0.14744062721729279\n",
      " epoch :7240----> train loss : 0.08815814554691315 || test loss : 0.14721176028251648\n",
      " epoch :7250----> train loss : 0.08783937990665436 || test loss : 0.14698410034179688\n",
      " epoch :7260----> train loss : 0.0875222235918045 || test loss : 0.14675772190093994\n",
      " epoch :7270----> train loss : 0.0872066542506218 || test loss : 0.14653204381465912\n",
      " epoch :7280----> train loss : 0.08689266443252563 || test loss : 0.14630760252475739\n",
      " epoch :7290----> train loss : 0.08658025413751602 || test loss : 0.14608432352542877\n",
      " epoch :7300----> train loss : 0.08626940101385117 || test loss : 0.14586226642131805\n",
      " epoch :7310----> train loss : 0.08596013486385345 || test loss : 0.14564119279384613\n",
      " epoch :7320----> train loss : 0.0856524184346199 || test loss : 0.1454210877418518\n",
      " epoch :7330----> train loss : 0.08534625172615051 || test loss : 0.14520224928855896\n",
      " epoch :7340----> train loss : 0.08504162728786469 || test loss : 0.14498433470726013\n",
      " epoch :7350----> train loss : 0.08473852276802063 || test loss : 0.14476773142814636\n",
      " epoch :7360----> train loss : 0.08443695306777954 || test loss : 0.1445518285036087\n",
      " epoch :7370----> train loss : 0.08413688093423843 || test loss : 0.14433731138706207\n",
      " epoch :7380----> train loss : 0.0838383361697197 || test loss : 0.14412382245063782\n",
      " epoch :7390----> train loss : 0.08354128152132034 || test loss : 0.14391136169433594\n",
      " epoch :7400----> train loss : 0.08324574679136276 || test loss : 0.14370010793209076\n",
      " epoch :7410----> train loss : 0.08295168727636337 || test loss : 0.14348965883255005\n",
      " epoch :7420----> train loss : 0.08265910297632217 || test loss : 0.14328032732009888\n",
      " epoch :7430----> train loss : 0.08236799389123917 || test loss : 0.14307212829589844\n",
      " epoch :7440----> train loss : 0.08207834511995316 || test loss : 0.14286470413208008\n",
      " epoch :7450----> train loss : 0.08179014921188354 || test loss : 0.14265848696231842\n",
      " epoch :7460----> train loss : 0.08150339871644974 || test loss : 0.14245326817035675\n",
      " epoch :7470----> train loss : 0.08121808618307114 || test loss : 0.14224928617477417\n",
      " epoch :7480----> train loss : 0.08093420416116714 || test loss : 0.142046257853508\n",
      " epoch :7490----> train loss : 0.08065174520015717 || test loss : 0.1418444961309433\n",
      " epoch :7500----> train loss : 0.08037067949771881 || test loss : 0.1416434943675995\n",
      " epoch :7510----> train loss : 0.08009101450443268 || test loss : 0.1414431482553482\n",
      " epoch :7520----> train loss : 0.07981275022029877 || test loss : 0.1412440687417984\n",
      " epoch :7530----> train loss : 0.07953588664531708 || test loss : 0.14104583859443665\n",
      " epoch :7540----> train loss : 0.07926039397716522 || test loss : 0.14084883034229279\n",
      " epoch :7550----> train loss : 0.07898630201816559 || test loss : 0.14065241813659668\n",
      " epoch :7560----> train loss : 0.07871358841657639 || test loss : 0.14045731723308563\n",
      " epoch :7570----> train loss : 0.07844223082065582 || test loss : 0.14026285707950592\n",
      " epoch :7580----> train loss : 0.0781722292304039 || test loss : 0.14006927609443665\n",
      " epoch :7590----> train loss : 0.07790357619524002 || test loss : 0.13987672328948975\n",
      " epoch :7600----> train loss : 0.07763627171516418 || test loss : 0.13968534767627716\n",
      " epoch :7610----> train loss : 0.0773702934384346 || test loss : 0.1394946128129959\n",
      " epoch :7620----> train loss : 0.07710564881563187 || test loss : 0.13930505514144897\n",
      " epoch :7630----> train loss : 0.07684232294559479 || test loss : 0.1391165852546692\n",
      " epoch :7640----> train loss : 0.07658033818006516 || test loss : 0.1389288455247879\n",
      " epoch :7650----> train loss : 0.07631964981555939 || test loss : 0.13874199986457825\n",
      " epoch :7660----> train loss : 0.07606025785207748 || test loss : 0.13855625689029694\n",
      " epoch :7670----> train loss : 0.07580216228961945 || test loss : 0.1383712887763977\n",
      " epoch :7680----> train loss : 0.07554536312818527 || test loss : 0.1381872296333313\n",
      " epoch :7690----> train loss : 0.07528985291719437 || test loss : 0.13800407946109772\n",
      " epoch :7700----> train loss : 0.07503558695316315 || test loss : 0.13782189786434174\n",
      " epoch :7710----> train loss : 0.0747826099395752 || test loss : 0.13764043152332306\n",
      " epoch :7720----> train loss : 0.07453088462352753 || test loss : 0.13746024668216705\n",
      " epoch :7730----> train loss : 0.07428041100502014 || test loss : 0.1372806578874588\n",
      " epoch :7740----> train loss : 0.07403115928173065 || test loss : 0.13710173964500427\n",
      " epoch :7750----> train loss : 0.07378315180540085 || test loss : 0.13692384958267212\n",
      " epoch :7760----> train loss : 0.07353638857603073 || test loss : 0.13674680888652802\n",
      " epoch :7770----> train loss : 0.07329083234071732 || test loss : 0.13657060265541077\n",
      " epoch :7780----> train loss : 0.0730464980006218 || test loss : 0.13639546930789948\n",
      " epoch :7790----> train loss : 0.07280338555574417 || test loss : 0.13622117042541504\n",
      " epoch :7800----> train loss : 0.07256149500608444 || test loss : 0.13604791462421417\n",
      " epoch :7810----> train loss : 0.07232078909873962 || test loss : 0.1358751803636551\n",
      " epoch :7820----> train loss : 0.07208129018545151 || test loss : 0.13570334017276764\n",
      " epoch :7830----> train loss : 0.0718429759144783 || test loss : 0.1355324238538742\n",
      " epoch :7840----> train loss : 0.0716058537364006 || test loss : 0.13536234200000763\n",
      " epoch :7850----> train loss : 0.07136990129947662 || test loss : 0.13519297540187836\n",
      " epoch :7860----> train loss : 0.07113513350486755 || test loss : 0.1350245326757431\n",
      " epoch :7870----> train loss : 0.07090152055025101 || test loss : 0.13485698401927948\n",
      " epoch :7880----> train loss : 0.07066906243562698 || test loss : 0.13469038903713226\n",
      " epoch :7890----> train loss : 0.07043775916099548 || test loss : 0.13452450931072235\n",
      " epoch :7900----> train loss : 0.07020760327577591 || test loss : 0.1343594640493393\n",
      " epoch :7910----> train loss : 0.06997857987880707 || test loss : 0.13419510424137115\n",
      " epoch :7920----> train loss : 0.06975070387125015 || test loss : 0.13403154909610748\n",
      " epoch :7930----> train loss : 0.06952395290136337 || test loss : 0.13386870920658112\n",
      " epoch :7940----> train loss : 0.06929831206798553 || test loss : 0.1337067037820816\n",
      " epoch :7950----> train loss : 0.06907378137111664 || test loss : 0.1335456818342209\n",
      " epoch :7960----> train loss : 0.06885035336017609 || test loss : 0.13338537514209747\n",
      " epoch :7970----> train loss : 0.06862805038690567 || test loss : 0.13322578370571136\n",
      " epoch :7980----> train loss : 0.06840682774782181 || test loss : 0.13306686282157898\n",
      " epoch :7990----> train loss : 0.0681866928935051 || test loss : 0.13290861248970032\n",
      " epoch :8000----> train loss : 0.06796763837337494 || test loss : 0.13275127112865448\n",
      " epoch :8010----> train loss : 0.06774967163801193 || test loss : 0.13259471952915192\n",
      " epoch :8020----> train loss : 0.06753276288509369 || test loss : 0.1324390023946762\n",
      " epoch :8030----> train loss : 0.06731691211462021 || test loss : 0.13228397071361542\n",
      " epoch :8040----> train loss : 0.06710212677717209 || test loss : 0.13212960958480835\n",
      " epoch :8050----> train loss : 0.06688839197158813 || test loss : 0.131975919008255\n",
      " epoch :8060----> train loss : 0.06667570024728775 || test loss : 0.13182340562343597\n",
      " epoch :8070----> train loss : 0.06646406650543213 || test loss : 0.13167144358158112\n",
      " epoch :8080----> train loss : 0.06625345349311829 || test loss : 0.13152025640010834\n",
      " epoch :8090----> train loss : 0.06604389101266861 || test loss : 0.1313696652650833\n",
      " epoch :8100----> train loss : 0.06583534181118011 || test loss : 0.13121995329856873\n",
      " epoch :8110----> train loss : 0.065627820789814 || test loss : 0.13107100129127502\n",
      " epoch :8120----> train loss : 0.06542130559682846 || test loss : 0.13092267513275146\n",
      " epoch :8130----> train loss : 0.0652158185839653 || test loss : 0.13077501952648163\n",
      " epoch :8140----> train loss : 0.06501132249832153 || test loss : 0.13062800467014313\n",
      " epoch :8150----> train loss : 0.06480781733989716 || test loss : 0.13048161566257477\n",
      " epoch :8160----> train loss : 0.06460531055927277 || test loss : 0.13033609092235565\n",
      " epoch :8170----> train loss : 0.06440379470586777 || test loss : 0.1301914006471634\n",
      " epoch :8180----> train loss : 0.06420326232910156 || test loss : 0.1300472766160965\n",
      " epoch :8190----> train loss : 0.06400366872549057 || test loss : 0.12990382313728333\n",
      " epoch :8200----> train loss : 0.06380505859851837 || test loss : 0.1297609508037567\n",
      " epoch :8210----> train loss : 0.06360743939876556 || test loss : 0.12961874902248383\n",
      " epoch :8220----> train loss : 0.06341075897216797 || test loss : 0.12947717308998108\n",
      " epoch :8230----> train loss : 0.06321503221988678 || test loss : 0.1293364316225052\n",
      " epoch :8240----> train loss : 0.0630202442407608 || test loss : 0.12919628620147705\n",
      " epoch :8250----> train loss : 0.06282638758420944 || test loss : 0.12905694544315338\n",
      " epoch :8260----> train loss : 0.06263347715139389 || test loss : 0.12891831994056702\n",
      " epoch :8270----> train loss : 0.062441494315862656 || test loss : 0.12878043949604034\n",
      " epoch :8280----> train loss : 0.06225043535232544 || test loss : 0.1286430060863495\n",
      " epoch :8290----> train loss : 0.06206030026078224 || test loss : 0.12850631773471832\n",
      " epoch :8300----> train loss : 0.061871081590652466 || test loss : 0.12837013602256775\n",
      " epoch :8310----> train loss : 0.06168277561664581 || test loss : 0.1282348334789276\n",
      " epoch :8320----> train loss : 0.061495378613471985 || test loss : 0.12810008227825165\n",
      " epoch :8330----> train loss : 0.061308883130550385 || test loss : 0.1279657930135727\n",
      " epoch :8340----> train loss : 0.061123285442590714 || test loss : 0.12783224880695343\n",
      " epoch :8350----> train loss : 0.06093858182430267 || test loss : 0.1276993602514267\n",
      " epoch :8360----> train loss : 0.06075476109981537 || test loss : 0.12756706774234772\n",
      " epoch :8370----> train loss : 0.0605718269944191 || test loss : 0.1274355798959732\n",
      " epoch :8380----> train loss : 0.06038975715637207 || test loss : 0.1273045837879181\n",
      " epoch :8390----> train loss : 0.06020857021212578 || test loss : 0.1271742582321167\n",
      " epoch :8400----> train loss : 0.06002824753522873 || test loss : 0.12704455852508545\n",
      " epoch :8410----> train loss : 0.05984878912568092 || test loss : 0.1269153505563736\n",
      " epoch :8420----> train loss : 0.05967018008232117 || test loss : 0.12678690254688263\n",
      " epoch :8430----> train loss : 0.05949242413043976 || test loss : 0.12665888667106628\n",
      " epoch :8440----> train loss : 0.059315524995326996 || test loss : 0.1265314668416977\n",
      " epoch :8450----> train loss : 0.059139467775821686 || test loss : 0.12640473246574402\n",
      " epoch :8460----> train loss : 0.05896424129605293 || test loss : 0.12627847492694855\n",
      " epoch :8470----> train loss : 0.05878985673189163 || test loss : 0.12615303695201874\n",
      " epoch :8480----> train loss : 0.058616310358047485 || test loss : 0.12602819502353668\n",
      " epoch :8490----> train loss : 0.0584435760974884 || test loss : 0.12590400874614716\n",
      " epoch :8500----> train loss : 0.05827167630195618 || test loss : 0.12578049302101135\n",
      " epoch :8510----> train loss : 0.058100584894418716 || test loss : 0.12565742433071136\n",
      " epoch :8520----> train loss : 0.05793030187487602 || test loss : 0.1255350261926651\n",
      " epoch :8530----> train loss : 0.057760823518037796 || test loss : 0.12541314959526062\n",
      " epoch :8540----> train loss : 0.05759216472506523 || test loss : 0.12529176473617554\n",
      " epoch :8550----> train loss : 0.05742429941892624 || test loss : 0.1251710206270218\n",
      " epoch :8560----> train loss : 0.057257216423749924 || test loss : 0.12505091726779938\n",
      " epoch :8570----> train loss : 0.05709093436598778 || test loss : 0.12493135035037994\n",
      " epoch :8580----> train loss : 0.05692543834447861 || test loss : 0.1248122900724411\n",
      " epoch :8590----> train loss : 0.05676073580980301 || test loss : 0.12469392269849777\n",
      " epoch :8600----> train loss : 0.05659680813550949 || test loss : 0.12457609921693802\n",
      " epoch :8610----> train loss : 0.05643364042043686 || test loss : 0.12445885688066483\n",
      " epoch :8620----> train loss : 0.0562712587416172 || test loss : 0.12434210628271103\n",
      " epoch :8630----> train loss : 0.056109629571437836 || test loss : 0.12422607094049454\n",
      " epoch :8640----> train loss : 0.05594876408576965 || test loss : 0.12411048263311386\n",
      " epoch :8650----> train loss : 0.05578865110874176 || test loss : 0.12399543076753616\n",
      " epoch :8660----> train loss : 0.055629294365644455 || test loss : 0.12388087064027786\n",
      " epoch :8670----> train loss : 0.05547068268060684 || test loss : 0.12376687675714493\n",
      " epoch :8680----> train loss : 0.05531281605362892 || test loss : 0.12365353852510452\n",
      " epoch :8690----> train loss : 0.055155690759420395 || test loss : 0.12354056537151337\n",
      " epoch :8700----> train loss : 0.05499928444623947 || test loss : 0.12342821806669235\n",
      " epoch :8710----> train loss : 0.05484361574053764 || test loss : 0.1233161985874176\n",
      " epoch :8720----> train loss : 0.05468866601586342 || test loss : 0.1232047826051712\n",
      " epoch :8730----> train loss : 0.054534438997507095 || test loss : 0.12309379875659943\n",
      " epoch :8740----> train loss : 0.05438093841075897 || test loss : 0.12298361957073212\n",
      " epoch :8750----> train loss : 0.05422814190387726 || test loss : 0.12287381291389465\n",
      " epoch :8760----> train loss : 0.05407606437802315 || test loss : 0.1227644830942154\n",
      " epoch :8770----> train loss : 0.053924690932035446 || test loss : 0.12265584617853165\n",
      " epoch :8780----> train loss : 0.053774021565914154 || test loss : 0.12254755944013596\n",
      " epoch :8790----> train loss : 0.053624045103788376 || test loss : 0.12243974953889847\n",
      " epoch :8800----> train loss : 0.05347477272152901 || test loss : 0.12233244627714157\n",
      " epoch :8810----> train loss : 0.05332618206739426 || test loss : 0.12222571671009064\n",
      " epoch :8820----> train loss : 0.05317828804254532 || test loss : 0.12211942672729492\n",
      " epoch :8830----> train loss : 0.0530310720205307 || test loss : 0.12201370298862457\n",
      " epoch :8840----> train loss : 0.05288451910018921 || test loss : 0.12190859019756317\n",
      " epoch :8850----> train loss : 0.052738651633262634 || test loss : 0.12180384248495102\n",
      " epoch :8860----> train loss : 0.05259346961975098 || test loss : 0.12169965356588364\n",
      " epoch :8870----> train loss : 0.052448950707912445 || test loss : 0.12159594148397446\n",
      " epoch :8880----> train loss : 0.05230509862303734 || test loss : 0.12149259448051453\n",
      " epoch :8890----> train loss : 0.05216190591454506 || test loss : 0.12138967216014862\n",
      " epoch :8900----> train loss : 0.05201937258243561 || test loss : 0.12128736078739166\n",
      " epoch :8910----> train loss : 0.05187749117612839 || test loss : 0.12118551880121231\n",
      " epoch :8920----> train loss : 0.0517362542450428 || test loss : 0.1210840716958046\n",
      " epoch :8930----> train loss : 0.051595669239759445 || test loss : 0.12098316848278046\n",
      " epoch :8940----> train loss : 0.051455721259117126 || test loss : 0.12088258564472198\n",
      " epoch :8950----> train loss : 0.05131641402840614 || test loss : 0.12078244984149933\n",
      " epoch :8960----> train loss : 0.051177747547626495 || test loss : 0.12068280577659607\n",
      " epoch :8970----> train loss : 0.051039718091487885 || test loss : 0.12058374285697937\n",
      " epoch :8980----> train loss : 0.05090233311057091 || test loss : 0.12048507481813431\n",
      " epoch :8990----> train loss : 0.05076557770371437 || test loss : 0.12038683891296387\n",
      " epoch :9000----> train loss : 0.050629448145627975 || test loss : 0.12028910219669342\n",
      " epoch :9010----> train loss : 0.050493933260440826 || test loss : 0.12019174546003342\n",
      " epoch :9020----> train loss : 0.05035903677344322 || test loss : 0.12009488046169281\n",
      " epoch :9030----> train loss : 0.05022476240992546 || test loss : 0.11999844014644623\n",
      " epoch :9040----> train loss : 0.05009108781814575 || test loss : 0.11990254372358322\n",
      " epoch :9050----> train loss : 0.04995802044868469 || test loss : 0.11980701982975006\n",
      " epoch :9060----> train loss : 0.04982556402683258 || test loss : 0.11971192806959152\n",
      " epoch :9070----> train loss : 0.049693699926137924 || test loss : 0.11961720883846283\n",
      " epoch :9080----> train loss : 0.049562424421310425 || test loss : 0.11952294409275055\n",
      " epoch :9090----> train loss : 0.049431733787059784 || test loss : 0.11942937225103378\n",
      " epoch :9100----> train loss : 0.04930161312222481 || test loss : 0.11933604627847672\n",
      " epoch :9110----> train loss : 0.049172088503837585 || test loss : 0.11924321204423904\n",
      " epoch :9120----> train loss : 0.049043141305446625 || test loss : 0.11915097385644913\n",
      " epoch :9130----> train loss : 0.048914775252342224 || test loss : 0.11905903369188309\n",
      " epoch :9140----> train loss : 0.04878697171807289 || test loss : 0.11896758526563644\n",
      " epoch :9150----> train loss : 0.04865975305438042 || test loss : 0.1188763901591301\n",
      " epoch :9160----> train loss : 0.04853309690952301 || test loss : 0.11878566443920135\n",
      " epoch :9170----> train loss : 0.04840700700879097 || test loss : 0.1186954453587532\n",
      " epoch :9180----> train loss : 0.0482814721763134 || test loss : 0.11860570311546326\n",
      " epoch :9190----> train loss : 0.048156507313251495 || test loss : 0.11851610243320465\n",
      " epoch :9200----> train loss : 0.04803209751844406 || test loss : 0.11842702329158783\n",
      " epoch :9210----> train loss : 0.0479082427918911 || test loss : 0.11833855509757996\n",
      " epoch :9220----> train loss : 0.04778493195772171 || test loss : 0.11825034022331238\n",
      " epoch :9230----> train loss : 0.0476621612906456 || test loss : 0.1181626096367836\n",
      " epoch :9240----> train loss : 0.047539953142404556 || test loss : 0.11807522922754288\n",
      " epoch :9250----> train loss : 0.04741828143596649 || test loss : 0.117988221347332\n",
      " epoch :9260----> train loss : 0.0472971610724926 || test loss : 0.11790156364440918\n",
      " epoch :9270----> train loss : 0.047176580876111984 || test loss : 0.11781540513038635\n",
      " epoch :9280----> train loss : 0.04705653712153435 || test loss : 0.11772952973842621\n",
      " epoch :9290----> train loss : 0.04693702235817909 || test loss : 0.11764401942491531\n",
      " epoch :9300----> train loss : 0.04681804031133652 || test loss : 0.11755909025669098\n",
      " epoch :9310----> train loss : 0.04669957235455513 || test loss : 0.11747439205646515\n",
      " epoch :9320----> train loss : 0.046581629663705826 || test loss : 0.11739003658294678\n",
      " epoch :9330----> train loss : 0.046464189887046814 || test loss : 0.1173061653971672\n",
      " epoch :9340----> train loss : 0.04634727165102959 || test loss : 0.11722256243228912\n",
      " epoch :9350----> train loss : 0.04623087868094444 || test loss : 0.11713932454586029\n",
      " epoch :9360----> train loss : 0.04611499235033989 || test loss : 0.11705648899078369\n",
      " epoch :9370----> train loss : 0.045999620109796524 || test loss : 0.11697404086589813\n",
      " epoch :9380----> train loss : 0.04588475823402405 || test loss : 0.11689192056655884\n",
      " epoch :9390----> train loss : 0.04577040299773216 || test loss : 0.11681020259857178\n",
      " epoch :9400----> train loss : 0.04565654322504997 || test loss : 0.1167287528514862\n",
      " epoch :9410----> train loss : 0.04554317891597748 || test loss : 0.11664782464504242\n",
      " epoch :9420----> train loss : 0.04543031007051468 || test loss : 0.1165672242641449\n",
      " epoch :9430----> train loss : 0.04531793296337128 || test loss : 0.1164868026971817\n",
      " epoch :9440----> train loss : 0.04520603269338608 || test loss : 0.11640684306621552\n",
      " epoch :9450----> train loss : 0.04509463161230087 || test loss : 0.11632704734802246\n",
      " epoch :9460----> train loss : 0.04498371109366417 || test loss : 0.11624772101640701\n",
      " epoch :9470----> train loss : 0.04487326741218567 || test loss : 0.11616884917020798\n",
      " epoch :9480----> train loss : 0.04476331174373627 || test loss : 0.11609009653329849\n",
      " epoch :9490----> train loss : 0.04465382173657417 || test loss : 0.11601192504167557\n",
      " epoch :9500----> train loss : 0.04454481229186058 || test loss : 0.11593399941921234\n",
      " epoch :9510----> train loss : 0.04443627968430519 || test loss : 0.11585649847984314\n",
      " epoch :9520----> train loss : 0.04432820528745651 || test loss : 0.11577938497066498\n",
      " epoch :9530----> train loss : 0.04422060772776604 || test loss : 0.11570252478122711\n",
      " epoch :9540----> train loss : 0.04411346837878227 || test loss : 0.11562588810920715\n",
      " epoch :9550----> train loss : 0.04400679096579552 || test loss : 0.11554960161447525\n",
      " epoch :9560----> train loss : 0.043900568038225174 || test loss : 0.11547376960515976\n",
      " epoch :9570----> train loss : 0.04379480704665184 || test loss : 0.11539819836616516\n",
      " epoch :9580----> train loss : 0.04368949681520462 || test loss : 0.11532308906316757\n",
      " epoch :9590----> train loss : 0.043584633618593216 || test loss : 0.11524809896945953\n",
      " epoch :9600----> train loss : 0.043480224907398224 || test loss : 0.11517354100942612\n",
      " epoch :9610----> train loss : 0.04337625950574875 || test loss : 0.11509934067726135\n",
      " epoch :9620----> train loss : 0.04327273368835449 || test loss : 0.1150256022810936\n",
      " epoch :9630----> train loss : 0.043169643729925156 || test loss : 0.11495200544595718\n",
      " epoch :9640----> train loss : 0.043067000806331635 || test loss : 0.1148788183927536\n",
      " epoch :9650----> train loss : 0.04296479374170303 || test loss : 0.1148059219121933\n",
      " epoch :9660----> train loss : 0.04286302626132965 || test loss : 0.11473347246646881\n",
      " epoch :9670----> train loss : 0.04276168346405029 || test loss : 0.1146610826253891\n",
      " epoch :9680----> train loss : 0.04266076162457466 || test loss : 0.11458921432495117\n",
      " epoch :9690----> train loss : 0.04256025329232216 || test loss : 0.1145176887512207\n",
      " epoch :9700----> train loss : 0.04246016964316368 || test loss : 0.11444637924432755\n",
      " epoch :9710----> train loss : 0.042360514402389526 || test loss : 0.11437545716762543\n",
      " epoch :9720----> train loss : 0.0422612801194191 || test loss : 0.1143047958612442\n",
      " epoch :9730----> train loss : 0.0421624593436718 || test loss : 0.11423449963331223\n",
      " epoch :9740----> train loss : 0.042064059525728226 || test loss : 0.11416444182395935\n",
      " epoch :9750----> train loss : 0.041966069489717484 || test loss : 0.11409474164247513\n",
      " epoch :9760----> train loss : 0.04186849296092987 || test loss : 0.11402557045221329\n",
      " epoch :9770----> train loss : 0.04177132621407509 || test loss : 0.11395657807588577\n",
      " epoch :9780----> train loss : 0.04167456179857254 || test loss : 0.11388793587684631\n",
      " epoch :9790----> train loss : 0.04157820716500282 || test loss : 0.11381956934928894\n",
      " epoch :9800----> train loss : 0.04148224741220474 || test loss : 0.11375141888856888\n",
      " epoch :9810----> train loss : 0.041386689990758896 || test loss : 0.11368364095687866\n",
      " epoch :9820----> train loss : 0.04129151999950409 || test loss : 0.11361613869667053\n",
      " epoch :9830----> train loss : 0.04119674861431122 || test loss : 0.11354902386665344\n",
      " epoch :9840----> train loss : 0.041102368384599686 || test loss : 0.11348199099302292\n",
      " epoch :9850----> train loss : 0.04100837931036949 || test loss : 0.1134154349565506\n",
      " epoch :9860----> train loss : 0.04091478884220123 || test loss : 0.11334912478923798\n",
      " epoch :9870----> train loss : 0.04082157835364342 || test loss : 0.11328306794166565\n",
      " epoch :9880----> train loss : 0.04072875156998634 || test loss : 0.11321721971035004\n",
      " epoch :9890----> train loss : 0.04063631594181061 || test loss : 0.11315171420574188\n",
      " epoch :9900----> train loss : 0.04054425284266472 || test loss : 0.11308660358190536\n",
      " epoch :9910----> train loss : 0.04045256972312927 || test loss : 0.11302170902490616\n",
      " epoch :9920----> train loss : 0.04036126285791397 || test loss : 0.11295709758996964\n",
      " epoch :9930----> train loss : 0.04027031734585762 || test loss : 0.11289282143115997\n",
      " epoch :9940----> train loss : 0.04017975926399231 || test loss : 0.11282871663570404\n",
      " epoch :9950----> train loss : 0.04008956253528595 || test loss : 0.11276507377624512\n",
      " epoch :9960----> train loss : 0.039999738335609436 || test loss : 0.11270137131214142\n",
      " epoch :9970----> train loss : 0.03991027921438217 || test loss : 0.11263822764158249\n",
      " epoch :9980----> train loss : 0.039821188896894455 || test loss : 0.11257534474134445\n",
      " epoch :9990----> train loss : 0.03973245248198509 || test loss : 0.11251253634691238\n",
      " epoch :10000----> train loss : 0.03964408114552498 || test loss : 0.11245012283325195\n",
      " epoch :10010----> train loss : 0.03955607861280441 || test loss : 0.11238802969455719\n",
      " epoch :10020----> train loss : 0.039468422532081604 || test loss : 0.11232618242502213\n",
      " epoch :10030----> train loss : 0.039381127804517746 || test loss : 0.11226456612348557\n",
      " epoch :10040----> train loss : 0.03929419070482254 || test loss : 0.11220323294401169\n",
      " epoch :10050----> train loss : 0.03920760378241539 || test loss : 0.11214211583137512\n",
      " epoch :10060----> train loss : 0.039121370762586594 || test loss : 0.11208103597164154\n",
      " epoch :10070----> train loss : 0.03903549164533615 || test loss : 0.1120203509926796\n",
      " epoch :10080----> train loss : 0.03894995525479317 || test loss : 0.11196009814739227\n",
      " epoch :10090----> train loss : 0.03886476159095764 || test loss : 0.11189998686313629\n",
      " epoch :10100----> train loss : 0.038779910653829575 || test loss : 0.11184021830558777\n",
      " epoch :10110----> train loss : 0.038695402443408966 || test loss : 0.11178075522184372\n",
      " epoch :10120----> train loss : 0.03861123323440552 || test loss : 0.11172149330377579\n",
      " epoch :10130----> train loss : 0.03852739930152893 || test loss : 0.11166249215602875\n",
      " epoch :10140----> train loss : 0.038443900644779205 || test loss : 0.11160354316234589\n",
      " epoch :10150----> train loss : 0.03836073353886604 || test loss : 0.11154502630233765\n",
      " epoch :10160----> train loss : 0.03827790170907974 || test loss : 0.11148679256439209\n",
      " epoch :10170----> train loss : 0.03819539025425911 || test loss : 0.11142885684967041\n",
      " epoch :10180----> train loss : 0.03811321035027504 || test loss : 0.11137118190526962\n",
      " epoch :10190----> train loss : 0.038031354546546936 || test loss : 0.11131373047828674\n",
      " epoch :10200----> train loss : 0.0379498191177845 || test loss : 0.11125646531581879\n",
      " epoch :10210----> train loss : 0.03786861151456833 || test loss : 0.11119958758354187\n",
      " epoch :10220----> train loss : 0.03778773173689842 || test loss : 0.11114272475242615\n",
      " epoch :10230----> train loss : 0.037707164883613586 || test loss : 0.11108612269163132\n",
      " epoch :10240----> train loss : 0.03762692213058472 || test loss : 0.11102981865406036\n",
      " epoch :10250----> train loss : 0.03754698857665062 || test loss : 0.11097367107868195\n",
      " epoch :10260----> train loss : 0.037467364221811295 || test loss : 0.11091797053813934\n",
      " epoch :10270----> train loss : 0.03738803416490555 || test loss : 0.11086234450340271\n",
      " epoch :10280----> train loss : 0.037309009581804276 || test loss : 0.11080707609653473\n",
      " epoch :10290----> train loss : 0.037230297923088074 || test loss : 0.11075209826231003\n",
      " epoch :10300----> train loss : 0.03715190663933754 || test loss : 0.11069730669260025\n",
      " epoch :10310----> train loss : 0.03707380220293999 || test loss : 0.11064275354146957\n",
      " epoch :10320----> train loss : 0.036996014416217804 || test loss : 0.11058841645717621\n",
      " epoch :10330----> train loss : 0.036918532103300095 || test loss : 0.11053434759378433\n",
      " epoch :10340----> train loss : 0.03684135526418686 || test loss : 0.11048047989606857\n",
      " epoch :10350----> train loss : 0.0367644727230072 || test loss : 0.11042679101228714\n",
      " epoch :10360----> train loss : 0.03668789193034172 || test loss : 0.1103733628988266\n",
      " epoch :10370----> train loss : 0.036611609160900116 || test loss : 0.110320083796978\n",
      " epoch :10380----> train loss : 0.036535631865262985 || test loss : 0.11026709526777267\n",
      " epoch :10390----> train loss : 0.036459945142269135 || test loss : 0.1102142333984375\n",
      " epoch :10400----> train loss : 0.03638455644249916 || test loss : 0.11016160994768143\n",
      " epoch :10410----> train loss : 0.036309465765953064 || test loss : 0.11010918766260147\n",
      " epoch :10420----> train loss : 0.03623466193675995 || test loss : 0.11005716025829315\n",
      " epoch :10430----> train loss : 0.03616014122962952 || test loss : 0.11000527441501617\n",
      " epoch :10440----> train loss : 0.03608591854572296 || test loss : 0.10995344817638397\n",
      " epoch :10450----> train loss : 0.03601197525858879 || test loss : 0.10990205407142639\n",
      " epoch :10460----> train loss : 0.0359383262693882 || test loss : 0.10985084623098373\n",
      " epoch :10470----> train loss : 0.03586495667695999 || test loss : 0.10979970544576645\n",
      " epoch :10480----> train loss : 0.03579186648130417 || test loss : 0.10974908620119095\n",
      " epoch :10490----> train loss : 0.03571906313300133 || test loss : 0.1096985936164856\n",
      " epoch :10500----> train loss : 0.03564654290676117 || test loss : 0.10964813083410263\n",
      " epoch :10510----> train loss : 0.0355742909014225 || test loss : 0.10959810763597488\n",
      " epoch :10520----> train loss : 0.035502322018146515 || test loss : 0.10954812169075012\n",
      " epoch :10530----> train loss : 0.035430628806352615 || test loss : 0.10949840396642685\n",
      " epoch :10540----> train loss : 0.0353592149913311 || test loss : 0.10944901406764984\n",
      " epoch :10550----> train loss : 0.03528806194663048 || test loss : 0.10939979553222656\n",
      " epoch :10560----> train loss : 0.03521719202399254 || test loss : 0.10935078561306\n",
      " epoch :10570----> train loss : 0.03514658659696579 || test loss : 0.10930182039737701\n",
      " epoch :10580----> train loss : 0.03507625684142113 || test loss : 0.10925319045782089\n",
      " epoch :10590----> train loss : 0.03500618785619736 || test loss : 0.10920483618974686\n",
      " epoch :10600----> train loss : 0.03493639454245567 || test loss : 0.10915658622980118\n",
      " epoch :10610----> train loss : 0.03486686572432518 || test loss : 0.10910847038030624\n",
      " epoch :10620----> train loss : 0.034797605127096176 || test loss : 0.10906066745519638\n",
      " epoch :10630----> train loss : 0.03472859412431717 || test loss : 0.10901302099227905\n",
      " epoch :10640----> train loss : 0.03465985506772995 || test loss : 0.10896554589271545\n",
      " epoch :10650----> train loss : 0.03459136560559273 || test loss : 0.10891834646463394\n",
      " epoch :10660----> train loss : 0.0345231369137764 || test loss : 0.10887128114700317\n",
      " epoch :10670----> train loss : 0.03445516154170036 || test loss : 0.10882439464330673\n",
      " epoch :10680----> train loss : 0.03438744693994522 || test loss : 0.10877793282270432\n",
      " epoch :10690----> train loss : 0.034319981932640076 || test loss : 0.10873153805732727\n",
      " epoch :10700----> train loss : 0.034252773970365524 || test loss : 0.10868517309427261\n",
      " epoch :10710----> train loss : 0.03418581560254097 || test loss : 0.10863907635211945\n",
      " epoch :10720----> train loss : 0.03411911427974701 || test loss : 0.10859320312738419\n",
      " epoch :10730----> train loss : 0.03405265882611275 || test loss : 0.10854752361774445\n",
      " epoch :10740----> train loss : 0.033986449241638184 || test loss : 0.10850204527378082\n",
      " epoch :10750----> train loss : 0.033920496702194214 || test loss : 0.10845673829317093\n",
      " epoch :10760----> train loss : 0.03385477885603905 || test loss : 0.10841158777475357\n",
      " epoch :10770----> train loss : 0.033789318054914474 || test loss : 0.10836658626794815\n",
      " epoch :10780----> train loss : 0.033724091947078705 || test loss : 0.10832180082798004\n",
      " epoch :10790----> train loss : 0.033659107983112335 || test loss : 0.10827726870775223\n",
      " epoch :10800----> train loss : 0.033594366163015366 || test loss : 0.108232781291008\n",
      " epoch :10810----> train loss : 0.0335298553109169 || test loss : 0.10818853229284286\n",
      " epoch :10820----> train loss : 0.03346557915210724 || test loss : 0.10814455151557922\n",
      " epoch :10830----> train loss : 0.03340154141187668 || test loss : 0.10810072720050812\n",
      " epoch :10840----> train loss : 0.03333773463964462 || test loss : 0.10805710405111313\n",
      " epoch :10850----> train loss : 0.03327416628599167 || test loss : 0.10801362991333008\n",
      " epoch :10860----> train loss : 0.03321083262562752 || test loss : 0.10797037184238434\n",
      " epoch :10870----> train loss : 0.03314772620797157 || test loss : 0.10792722553014755\n",
      " epoch :10880----> train loss : 0.03308485448360443 || test loss : 0.10788439214229584\n",
      " epoch :10890----> train loss : 0.033022213727235794 || test loss : 0.10784167051315308\n",
      " epoch :10900----> train loss : 0.032959796488285065 || test loss : 0.10779908299446106\n",
      " epoch :10910----> train loss : 0.03289761021733284 || test loss : 0.10775669664144516\n",
      " epoch :10920----> train loss : 0.03283564746379852 || test loss : 0.10771452635526657\n",
      " epoch :10930----> train loss : 0.03277391940355301 || test loss : 0.10767246037721634\n",
      " epoch :10940----> train loss : 0.03271240368485451 || test loss : 0.10763062536716461\n",
      " epoch :10950----> train loss : 0.03265112265944481 || test loss : 0.10758907347917557\n",
      " epoch :10960----> train loss : 0.03259005397558212 || test loss : 0.10754746198654175\n",
      " epoch :10970----> train loss : 0.03252921625971794 || test loss : 0.10750604420900345\n",
      " epoch :10980----> train loss : 0.03246859088540077 || test loss : 0.10746490955352783\n",
      " epoch :10990----> train loss : 0.03240818530321121 || test loss : 0.10742386430501938\n",
      " epoch :11000----> train loss : 0.03234800696372986 || test loss : 0.1073831245303154\n",
      " epoch :11010----> train loss : 0.03228802978992462 || test loss : 0.10734263807535172\n",
      " epoch :11020----> train loss : 0.032228272408246994 || test loss : 0.10730211436748505\n",
      " epoch :11030----> train loss : 0.032168738543987274 || test loss : 0.10726185888051987\n",
      " epoch :11040----> train loss : 0.03210941329598427 || test loss : 0.10722174495458603\n",
      " epoch :11050----> train loss : 0.032050300389528275 || test loss : 0.10718179494142532\n",
      " epoch :11060----> train loss : 0.031991396099328995 || test loss : 0.10714203864336014\n",
      " epoch :11070----> train loss : 0.03193270042538643 || test loss : 0.10710257291793823\n",
      " epoch :11080----> train loss : 0.031874217092990875 || test loss : 0.10706305503845215\n",
      " epoch :11090----> train loss : 0.03181593865156174 || test loss : 0.10702373832464218\n",
      " epoch :11100----> train loss : 0.03175787255167961 || test loss : 0.10698452591896057\n",
      " epoch :11110----> train loss : 0.0317000113427639 || test loss : 0.10694551467895508\n",
      " epoch :11120----> train loss : 0.031642355024814606 || test loss : 0.10690674185752869\n",
      " epoch :11130----> train loss : 0.03158489614725113 || test loss : 0.10686807334423065\n",
      " epoch :11140----> train loss : 0.03152765333652496 || test loss : 0.10682966560125351\n",
      " epoch :11150----> train loss : 0.031470611691474915 || test loss : 0.10679135471582413\n",
      " epoch :11160----> train loss : 0.03141375631093979 || test loss : 0.10675311088562012\n",
      " epoch :11170----> train loss : 0.03135712072253227 || test loss : 0.1067151203751564\n",
      " epoch :11180----> train loss : 0.03130067512392998 || test loss : 0.10667719691991806\n",
      " epoch :11190----> train loss : 0.031244434416294098 || test loss : 0.10663940757513046\n",
      " epoch :11200----> train loss : 0.03118838742375374 || test loss : 0.10660182684659958\n",
      " epoch :11210----> train loss : 0.031132536008954048 || test loss : 0.10656432807445526\n",
      " epoch :11220----> train loss : 0.031076868996024132 || test loss : 0.10652698576450348\n",
      " epoch :11230----> train loss : 0.03102140501141548 || test loss : 0.1064898669719696\n",
      " epoch :11240----> train loss : 0.03096613474190235 || test loss : 0.10645286738872528\n",
      " epoch :11250----> train loss : 0.03091105818748474 || test loss : 0.10641597956418991\n",
      " epoch :11260----> train loss : 0.030856166034936905 || test loss : 0.10637933015823364\n",
      " epoch :11270----> train loss : 0.030801473185420036 || test loss : 0.10634273290634155\n",
      " epoch :11280----> train loss : 0.030746962875127792 || test loss : 0.10630623251199722\n",
      " epoch :11290----> train loss : 0.030692651867866516 || test loss : 0.10626993328332901\n",
      " epoch :11300----> train loss : 0.030638523399829865 || test loss : 0.10623376071453094\n",
      " epoch :11310----> train loss : 0.030584588646888733 || test loss : 0.1061977669596672\n",
      " epoch :11320----> train loss : 0.03053082898259163 || test loss : 0.10616201907396317\n",
      " epoch :11330----> train loss : 0.030477264896035194 || test loss : 0.10612615197896957\n",
      " epoch :11340----> train loss : 0.030423881486058235 || test loss : 0.10609063506126404\n",
      " epoch :11350----> train loss : 0.0303706806153059 || test loss : 0.10605516284704208\n",
      " epoch :11360----> train loss : 0.030317671597003937 || test loss : 0.10601986199617386\n",
      " epoch :11370----> train loss : 0.03026483952999115 || test loss : 0.10598478466272354\n",
      " epoch :11380----> train loss : 0.030212191864848137 || test loss : 0.10594972968101501\n",
      " epoch :11390----> train loss : 0.030159715563058853 || test loss : 0.10591486096382141\n",
      " epoch :11400----> train loss : 0.03010742925107479 || test loss : 0.10588011890649796\n",
      " epoch :11410----> train loss : 0.030055319890379906 || test loss : 0.105845607817173\n",
      " epoch :11420----> train loss : 0.0300033837556839 || test loss : 0.105811208486557\n",
      " epoch :11430----> train loss : 0.029951632022857666 || test loss : 0.10577690601348877\n",
      " epoch :11440----> train loss : 0.029900049790740013 || test loss : 0.10574276745319366\n",
      " epoch :11450----> train loss : 0.029848646372556686 || test loss : 0.1057087779045105\n",
      " epoch :11460----> train loss : 0.029797421768307686 || test loss : 0.1056748703122139\n",
      " epoch :11470----> train loss : 0.029746361076831818 || test loss : 0.10564108937978745\n",
      " epoch :11480----> train loss : 0.029695481061935425 || test loss : 0.1056075245141983\n",
      " epoch :11490----> train loss : 0.02964477241039276 || test loss : 0.1055741086602211\n",
      " epoch :11500----> train loss : 0.02959422580897808 || test loss : 0.10554077476263046\n",
      " epoch :11510----> train loss : 0.029543858021497726 || test loss : 0.10550756007432938\n",
      " epoch :11520----> train loss : 0.029493656009435654 || test loss : 0.1054745614528656\n",
      " epoch :11530----> train loss : 0.029443612322211266 || test loss : 0.10544165968894958\n",
      " epoch :11540----> train loss : 0.029393741860985756 || test loss : 0.10540875792503357\n",
      " epoch :11550----> train loss : 0.029344039037823677 || test loss : 0.10537617653608322\n",
      " epoch :11560----> train loss : 0.02929449826478958 || test loss : 0.10534369945526123\n",
      " epoch :11570----> train loss : 0.029245130717754364 || test loss : 0.1053113341331482\n",
      " epoch :11580----> train loss : 0.02919592894613743 || test loss : 0.10527908056974411\n",
      " epoch :11590----> train loss : 0.029146883636713028 || test loss : 0.1052468940615654\n",
      " epoch :11600----> train loss : 0.029098007827997208 || test loss : 0.1052149087190628\n",
      " epoch :11610----> train loss : 0.02904929406940937 || test loss : 0.10518311709165573\n",
      " epoch :11620----> train loss : 0.029000740498304367 || test loss : 0.10515131056308746\n",
      " epoch :11630----> train loss : 0.028952356427907944 || test loss : 0.10511966794729233\n",
      " epoch :11640----> train loss : 0.028904128819704056 || test loss : 0.10508815199136734\n",
      " epoch :11650----> train loss : 0.02885606326162815 || test loss : 0.10505682229995728\n",
      " epoch :11660----> train loss : 0.028808152303099632 || test loss : 0.10502550005912781\n",
      " epoch :11670----> train loss : 0.028760401532053947 || test loss : 0.1049942821264267\n",
      " epoch :11680----> train loss : 0.028712812811136246 || test loss : 0.10496331751346588\n",
      " epoch :11690----> train loss : 0.02866537682712078 || test loss : 0.10493249446153641\n",
      " epoch :11700----> train loss : 0.02861810103058815 || test loss : 0.10490183532238007\n",
      " epoch :11710----> train loss : 0.028570977970957756 || test loss : 0.10487116128206253\n",
      " epoch :11720----> train loss : 0.028524009510874748 || test loss : 0.10484068840742111\n",
      " epoch :11730----> train loss : 0.028477195650339127 || test loss : 0.10481040179729462\n",
      " epoch :11740----> train loss : 0.02843053825199604 || test loss : 0.10478013753890991\n",
      " epoch :11750----> train loss : 0.02838403359055519 || test loss : 0.1047501340508461\n",
      " epoch :11760----> train loss : 0.02833767607808113 || test loss : 0.10472013801336288\n",
      " epoch :11770----> train loss : 0.028291471302509308 || test loss : 0.10469025373458862\n",
      " epoch :11780----> train loss : 0.028245415538549423 || test loss : 0.10466060042381287\n",
      " epoch :11790----> train loss : 0.028199512511491776 || test loss : 0.10463086515665054\n",
      " epoch :11800----> train loss : 0.02815375290811062 || test loss : 0.10460136830806732\n",
      " epoch :11810----> train loss : 0.028108151629567146 || test loss : 0.10457208007574081\n",
      " epoch :11820----> train loss : 0.028062695637345314 || test loss : 0.1045427918434143\n",
      " epoch :11830----> train loss : 0.028017377480864525 || test loss : 0.10451368242502213\n",
      " epoch :11840----> train loss : 0.027972213923931122 || test loss : 0.10448465496301651\n",
      " epoch :11850----> train loss : 0.02792719565331936 || test loss : 0.10445569455623627\n",
      " epoch :11860----> train loss : 0.027882322669029236 || test loss : 0.1044268012046814\n",
      " epoch :11870----> train loss : 0.0278376005589962 || test loss : 0.10439811646938324\n",
      " epoch :11880----> train loss : 0.02779301069676876 || test loss : 0.10436949133872986\n",
      " epoch :11890----> train loss : 0.02774856984615326 || test loss : 0.10434110462665558\n",
      " epoch :11900----> train loss : 0.027704274281859398 || test loss : 0.10431274771690369\n",
      " epoch :11910----> train loss : 0.02766011655330658 || test loss : 0.10428446531295776\n",
      " epoch :11920----> train loss : 0.027616098523139954 || test loss : 0.10425637662410736\n",
      " epoch :11930----> train loss : 0.02757222391664982 || test loss : 0.10422836244106293\n",
      " epoch :11940----> train loss : 0.027528492733836174 || test loss : 0.10420048236846924\n",
      " epoch :11950----> train loss : 0.027484895661473274 || test loss : 0.10417284071445465\n",
      " epoch :11960----> train loss : 0.027441436424851418 || test loss : 0.10414516180753708\n",
      " epoch :11970----> train loss : 0.02739812433719635 || test loss : 0.10411755740642548\n",
      " epoch :11980----> train loss : 0.02735494077205658 || test loss : 0.10409008711576462\n",
      " epoch :11990----> train loss : 0.027311896905303 || test loss : 0.1040627509355545\n",
      " epoch :12000----> train loss : 0.027268989011645317 || test loss : 0.1040356308221817\n",
      " epoch :12010----> train loss : 0.027226218953728676 || test loss : 0.10400855541229248\n",
      " epoch :12020----> train loss : 0.02718357741832733 || test loss : 0.10398157685995102\n",
      " epoch :12030----> train loss : 0.02714107558131218 || test loss : 0.10395468771457672\n",
      " epoch :12040----> train loss : 0.027098704129457474 || test loss : 0.10392801463603973\n",
      " epoch :12050----> train loss : 0.02705647423863411 || test loss : 0.10390140861272812\n",
      " epoch :12060----> train loss : 0.02701437473297119 || test loss : 0.1038748249411583\n",
      " epoch :12070----> train loss : 0.02697240188717842 || test loss : 0.10384833812713623\n",
      " epoch :12080----> train loss : 0.026930561289191246 || test loss : 0.10382205247879028\n",
      " epoch :12090----> train loss : 0.026888858526945114 || test loss : 0.10379579663276672\n",
      " epoch :12100----> train loss : 0.02684728056192398 || test loss : 0.1037696897983551\n",
      " epoch :12110----> train loss : 0.026805829256772995 || test loss : 0.10374359786510468\n",
      " epoch :12120----> train loss : 0.02676451951265335 || test loss : 0.10371778905391693\n",
      " epoch :12130----> train loss : 0.026723332703113556 || test loss : 0.10369201004505157\n",
      " epoch :12140----> train loss : 0.02668226696550846 || test loss : 0.10366630554199219\n",
      " epoch :12150----> train loss : 0.02664133720099926 || test loss : 0.10364071279764175\n",
      " epoch :12160----> train loss : 0.02660052850842476 || test loss : 0.10361519455909729\n",
      " epoch :12170----> train loss : 0.026559850201010704 || test loss : 0.10358980298042297\n",
      " epoch :12180----> train loss : 0.02651929296553135 || test loss : 0.10356452316045761\n",
      " epoch :12190----> train loss : 0.02647886984050274 || test loss : 0.10353931784629822\n",
      " epoch :12200----> train loss : 0.026438573375344276 || test loss : 0.10351426154375076\n",
      " epoch :12210----> train loss : 0.026398401707410812 || test loss : 0.10348931699991226\n",
      " epoch :12220----> train loss : 0.02635834738612175 || test loss : 0.10346449911594391\n",
      " epoch :12230----> train loss : 0.026318423449993134 || test loss : 0.10343962907791138\n",
      " epoch :12240----> train loss : 0.026278618723154068 || test loss : 0.10341495275497437\n",
      " epoch :12250----> train loss : 0.026238935068249702 || test loss : 0.10339035838842392\n",
      " epoch :12260----> train loss : 0.026199379935860634 || test loss : 0.10336589813232422\n",
      " epoch :12270----> train loss : 0.02615993283689022 || test loss : 0.1033414825797081\n",
      " epoch :12280----> train loss : 0.0261206217110157 || test loss : 0.10331717878580093\n",
      " epoch :12290----> train loss : 0.026081427931785583 || test loss : 0.1032930389046669\n",
      " epoch :12300----> train loss : 0.02604234404861927 || test loss : 0.10326902568340302\n",
      " epoch :12310----> train loss : 0.026003392413258553 || test loss : 0.1032448559999466\n",
      " epoch :12320----> train loss : 0.02596454881131649 || test loss : 0.10322095453739166\n",
      " epoch :12330----> train loss : 0.025925828143954277 || test loss : 0.10319715738296509\n",
      " epoch :12340----> train loss : 0.025887232273817062 || test loss : 0.1031734049320221\n",
      " epoch :12350----> train loss : 0.025848744437098503 || test loss : 0.10314983874559402\n",
      " epoch :12360----> train loss : 0.025810381397604942 || test loss : 0.10312632471323013\n",
      " epoch :12370----> train loss : 0.025772124528884888 || test loss : 0.10310281068086624\n",
      " epoch :12380----> train loss : 0.02573399245738983 || test loss : 0.10307944566011429\n",
      " epoch :12390----> train loss : 0.02569597028195858 || test loss : 0.10305608808994293\n",
      " epoch :12400----> train loss : 0.02565806172788143 || test loss : 0.10303278267383575\n",
      " epoch :12410----> train loss : 0.025620270520448685 || test loss : 0.10300960391759872\n",
      " epoch :12420----> train loss : 0.025582589209079742 || test loss : 0.10298657417297363\n",
      " epoch :12430----> train loss : 0.0255450252443552 || test loss : 0.10296355187892914\n",
      " epoch :12440----> train loss : 0.025507571175694466 || test loss : 0.1029406413435936\n",
      " epoch :12450----> train loss : 0.025470228865742683 || test loss : 0.10291789472103119\n",
      " epoch :12460----> train loss : 0.025432996451854706 || test loss : 0.10289518535137177\n",
      " epoch :12470----> train loss : 0.02539587765932083 || test loss : 0.1028725877404213\n",
      " epoch :12480----> train loss : 0.02535887062549591 || test loss : 0.10284999012947083\n",
      " epoch :12490----> train loss : 0.025321975350379944 || test loss : 0.1028275340795517\n",
      " epoch :12500----> train loss : 0.025285188108682632 || test loss : 0.10280518233776093\n",
      " epoch :12510----> train loss : 0.025248505175113678 || test loss : 0.1027829647064209\n",
      " epoch :12520----> train loss : 0.025211935862898827 || test loss : 0.10276075452566147\n",
      " epoch :12530----> train loss : 0.025175470858812332 || test loss : 0.10273867845535278\n",
      " epoch :12540----> train loss : 0.02513911761343479 || test loss : 0.10271663218736649\n",
      " epoch :12550----> train loss : 0.025102872401475906 || test loss : 0.10269476473331451\n",
      " epoch :12560----> train loss : 0.02506672963500023 || test loss : 0.10267283022403717\n",
      " epoch :12570----> train loss : 0.025030698627233505 || test loss : 0.10265111178159714\n",
      " epoch :12580----> train loss : 0.024994775652885437 || test loss : 0.10262928903102875\n",
      " epoch :12590----> train loss : 0.02495894767343998 || test loss : 0.10260777175426483\n",
      " epoch :12600----> train loss : 0.024923237040638924 || test loss : 0.10258626937866211\n",
      " epoch :12610----> train loss : 0.02488762140274048 || test loss : 0.10256481170654297\n",
      " epoch :12620----> train loss : 0.02485210821032524 || test loss : 0.10254345089197159\n",
      " epoch :12630----> train loss : 0.02481670305132866 || test loss : 0.10252223908901215\n",
      " epoch :12640----> train loss : 0.024781392887234688 || test loss : 0.10250117629766464\n",
      " epoch :12650----> train loss : 0.02474619634449482 || test loss : 0.10248010605573654\n",
      " epoch :12660----> train loss : 0.024711092934012413 || test loss : 0.10245911031961441\n",
      " epoch :12670----> train loss : 0.02467610314488411 || test loss : 0.10243818908929825\n",
      " epoch :12680----> train loss : 0.02464120276272297 || test loss : 0.10241725295782089\n",
      " epoch :12690----> train loss : 0.024606408551335335 || test loss : 0.10239654034376144\n",
      " epoch :12700----> train loss : 0.024571718648076057 || test loss : 0.10237590223550797\n",
      " epoch :12710----> train loss : 0.02453712373971939 || test loss : 0.10235518962144852\n",
      " epoch :12720----> train loss : 0.02450263313949108 || test loss : 0.1023346409201622\n",
      " epoch :12730----> train loss : 0.02446824312210083 || test loss : 0.10231418907642365\n",
      " epoch :12740----> train loss : 0.02443394809961319 || test loss : 0.10229381173849106\n",
      " epoch :12750----> train loss : 0.02439975179731846 || test loss : 0.10227348655462265\n",
      " epoch :12760----> train loss : 0.024365656077861786 || test loss : 0.10225336998701096\n",
      " epoch :12770----> train loss : 0.02433166280388832 || test loss : 0.10223318636417389\n",
      " epoch :12780----> train loss : 0.02429775521159172 || test loss : 0.10221323370933533\n",
      " epoch :12790----> train loss : 0.024263953790068626 || test loss : 0.10219326615333557\n",
      " epoch :12800----> train loss : 0.024230249226093292 || test loss : 0.10217329859733582\n",
      " epoch :12810----> train loss : 0.02419663593173027 || test loss : 0.1021534651517868\n",
      " epoch :12820----> train loss : 0.024163128808140755 || test loss : 0.10213365405797958\n",
      " epoch :12830----> train loss : 0.024129709228873253 || test loss : 0.1021139919757843\n",
      " epoch :12840----> train loss : 0.024096384644508362 || test loss : 0.10209444910287857\n",
      " epoch :12850----> train loss : 0.02406315691769123 || test loss : 0.10207491368055344\n",
      " epoch :12860----> train loss : 0.02403002604842186 || test loss : 0.10205555707216263\n",
      " epoch :12870----> train loss : 0.0239969864487648 || test loss : 0.10203620791435242\n",
      " epoch :12880----> train loss : 0.023964038118720055 || test loss : 0.102016881108284\n",
      " epoch :12890----> train loss : 0.023931194096803665 || test loss : 0.10199767351150513\n",
      " epoch :12900----> train loss : 0.023898428305983543 || test loss : 0.10197851806879044\n",
      " epoch :12910----> train loss : 0.02386576682329178 || test loss : 0.10195953398942947\n",
      " epoch :12920----> train loss : 0.02383318915963173 || test loss : 0.10194051265716553\n",
      " epoch :12930----> train loss : 0.02380070835351944 || test loss : 0.10192158818244934\n",
      " epoch :12940----> train loss : 0.023768320679664612 || test loss : 0.10190276056528091\n",
      " epoch :12950----> train loss : 0.0237360130995512 || test loss : 0.10188400000333786\n",
      " epoch :12960----> train loss : 0.02370380610227585 || test loss : 0.10186527669429779\n",
      " epoch :12970----> train loss : 0.02367168851196766 || test loss : 0.10184672474861145\n",
      " epoch :12980----> train loss : 0.023639656603336334 || test loss : 0.10182826220989227\n",
      " epoch :12990----> train loss : 0.023607710376381874 || test loss : 0.1018097847700119\n",
      " epoch :13000----> train loss : 0.02357584983110428 || test loss : 0.10179135948419571\n",
      " epoch :13010----> train loss : 0.023544078692793846 || test loss : 0.10177292674779892\n",
      " epoch :13020----> train loss : 0.023512398824095726 || test loss : 0.1017548069357872\n",
      " epoch :13030----> train loss : 0.023480791598558426 || test loss : 0.1017366275191307\n",
      " epoch :13040----> train loss : 0.023449279367923737 || test loss : 0.10171864926815033\n",
      " epoch :13050----> train loss : 0.023417847231030464 || test loss : 0.10170053690671921\n",
      " epoch :13060----> train loss : 0.0233865175396204 || test loss : 0.10168275237083435\n",
      " epoch :13070----> train loss : 0.02335526794195175 || test loss : 0.10166490823030472\n",
      " epoch :13080----> train loss : 0.023324109613895416 || test loss : 0.10164718329906464\n",
      " epoch :13090----> train loss : 0.02329302951693535 || test loss : 0.10162942856550217\n",
      " epoch :13100----> train loss : 0.023262042552232742 || test loss : 0.1016118973493576\n",
      " epoch :13110----> train loss : 0.023231141269207 || test loss : 0.10159436613321304\n",
      " epoch :13120----> train loss : 0.02320031449198723 || test loss : 0.10157671570777893\n",
      " epoch :13130----> train loss : 0.023169586434960365 || test loss : 0.10155924409627914\n",
      " epoch :13140----> train loss : 0.02313893847167492 || test loss : 0.10154180973768234\n",
      " epoch :13150----> train loss : 0.02310837246477604 || test loss : 0.10152453929185867\n",
      " epoch :13160----> train loss : 0.023077895864844322 || test loss : 0.10150720179080963\n",
      " epoch :13170----> train loss : 0.02304750494658947 || test loss : 0.10148997604846954\n",
      " epoch :13180----> train loss : 0.023017199710011482 || test loss : 0.10147291421890259\n",
      " epoch :13190----> train loss : 0.02298698015511036 || test loss : 0.10145588964223862\n",
      " epoch :13200----> train loss : 0.022956836968660355 || test loss : 0.10143889486789703\n",
      " epoch :13210----> train loss : 0.022926773875951767 || test loss : 0.10142182558774948\n",
      " epoch :13220----> train loss : 0.022896794602274895 || test loss : 0.10140503942966461\n",
      " epoch :13230----> train loss : 0.022866902872920036 || test loss : 0.10138820856809616\n",
      " epoch :13240----> train loss : 0.02283708192408085 || test loss : 0.10137148946523666\n",
      " epoch :13250----> train loss : 0.022807350382208824 || test loss : 0.10135488212108612\n",
      " epoch :13260----> train loss : 0.02277769148349762 || test loss : 0.10133825242519379\n",
      " epoch :13270----> train loss : 0.02274811826646328 || test loss : 0.10132180154323578\n",
      " epoch :13280----> train loss : 0.02271861955523491 || test loss : 0.10130535811185837\n",
      " epoch :13290----> train loss : 0.0226892102509737 || test loss : 0.10128886252641678\n",
      " epoch :13300----> train loss : 0.022659873589873314 || test loss : 0.10127246379852295\n",
      " epoch :13310----> train loss : 0.022630611434578896 || test loss : 0.10125631839036942\n",
      " epoch :13320----> train loss : 0.022601431235671043 || test loss : 0.10123999416828156\n",
      " epoch :13330----> train loss : 0.022572338581085205 || test loss : 0.1012239083647728\n",
      " epoch :13340----> train loss : 0.02254331484436989 || test loss : 0.10120788216590881\n",
      " epoch :13350----> train loss : 0.02251436933875084 || test loss : 0.1011919379234314\n",
      " epoch :13360----> train loss : 0.02248550020158291 || test loss : 0.10117597877979279\n",
      " epoch :13370----> train loss : 0.022456716746091843 || test loss : 0.10116003453731537\n",
      " epoch :13380----> train loss : 0.02242800034582615 || test loss : 0.10114418715238571\n",
      " epoch :13390----> train loss : 0.02239936590194702 || test loss : 0.10112836956977844\n",
      " epoch :13400----> train loss : 0.022370805963873863 || test loss : 0.10111270844936371\n",
      " epoch :13410----> train loss : 0.022342324256896973 || test loss : 0.10109696537256241\n",
      " epoch :13420----> train loss : 0.0223139189183712 || test loss : 0.10108131915330887\n",
      " epoch :13430----> train loss : 0.0222855843603611 || test loss : 0.10106594860553741\n",
      " epoch :13440----> train loss : 0.022257324308156967 || test loss : 0.10105036944150925\n",
      " epoch :13450----> train loss : 0.022229140624403954 || test loss : 0.10103504359722137\n",
      " epoch :13460----> train loss : 0.02220103330910206 || test loss : 0.10101980715990067\n",
      " epoch :13470----> train loss : 0.022172996774315834 || test loss : 0.1010044515132904\n",
      " epoch :13480----> train loss : 0.02214503288269043 || test loss : 0.10098936408758163\n",
      " epoch :13490----> train loss : 0.022117149084806442 || test loss : 0.10097420960664749\n",
      " epoch :13500----> train loss : 0.022089334204792976 || test loss : 0.1009591594338417\n",
      " epoch :13510----> train loss : 0.022061601281166077 || test loss : 0.10094410181045532\n",
      " epoch :13520----> train loss : 0.02203393168747425 || test loss : 0.1009291410446167\n",
      " epoch :13530----> train loss : 0.022006336599588394 || test loss : 0.10091429948806763\n",
      " epoch :13540----> train loss : 0.02197881042957306 || test loss : 0.10089946538209915\n",
      " epoch :13550----> train loss : 0.021951358765363693 || test loss : 0.10088467597961426\n",
      " epoch :13560----> train loss : 0.0219239741563797 || test loss : 0.10086999833583832\n",
      " epoch :13570----> train loss : 0.02189667336642742 || test loss : 0.10085529834032059\n",
      " epoch :13580----> train loss : 0.021869435906410217 || test loss : 0.1008407473564148\n",
      " epoch :13590----> train loss : 0.021842271089553833 || test loss : 0.100826196372509\n",
      " epoch :13600----> train loss : 0.021815182641148567 || test loss : 0.10081165283918381\n",
      " epoch :13610----> train loss : 0.021788155660033226 || test loss : 0.10079722851514816\n",
      " epoch :13620----> train loss : 0.021761203184723854 || test loss : 0.10078276693820953\n",
      " epoch :13630----> train loss : 0.021734319627285004 || test loss : 0.10076848417520523\n",
      " epoch :13640----> train loss : 0.021707506850361824 || test loss : 0.10075430572032928\n",
      " epoch :13650----> train loss : 0.021680764853954315 || test loss : 0.10074007511138916\n",
      " epoch :13660----> train loss : 0.02165408991277218 || test loss : 0.10072588175535202\n",
      " epoch :13670----> train loss : 0.021627487614750862 || test loss : 0.10071182996034622\n",
      " epoch :13680----> train loss : 0.021600952371954918 || test loss : 0.100697822868824\n",
      " epoch :13690----> train loss : 0.021574482321739197 || test loss : 0.1006837710738182\n",
      " epoch :13700----> train loss : 0.0215480774641037 || test loss : 0.10066977888345718\n",
      " epoch :13710----> train loss : 0.02152175083756447 || test loss : 0.10065590590238571\n",
      " epoch :13720----> train loss : 0.021495483815670013 || test loss : 0.10064207762479782\n",
      " epoch :13730----> train loss : 0.021469291299581528 || test loss : 0.10062835365533829\n",
      " epoch :13740----> train loss : 0.02144315280020237 || test loss : 0.10061463713645935\n",
      " epoch :13750----> train loss : 0.021417083218693733 || test loss : 0.10060097277164459\n",
      " epoch :13760----> train loss : 0.021391084417700768 || test loss : 0.1005874052643776\n",
      " epoch :13770----> train loss : 0.021365152671933174 || test loss : 0.10057388246059418\n",
      " epoch :13780----> train loss : 0.02133927308022976 || test loss : 0.10056032985448837\n",
      " epoch :13790----> train loss : 0.021313469856977463 || test loss : 0.10054686665534973\n",
      " epoch :13800----> train loss : 0.02128772996366024 || test loss : 0.10053347796201706\n",
      " epoch :13810----> train loss : 0.021262049674987793 || test loss : 0.10052012652158737\n",
      " epoch :13820----> train loss : 0.021236438304185867 || test loss : 0.10050686448812485\n",
      " epoch :13830----> train loss : 0.021210895851254463 || test loss : 0.1004936620593071\n",
      " epoch :13840----> train loss : 0.021185418590903282 || test loss : 0.10048051178455353\n",
      " epoch :13850----> train loss : 0.021159999072551727 || test loss : 0.10046746581792831\n",
      " epoch :13860----> train loss : 0.021134641021490097 || test loss : 0.1004544198513031\n",
      " epoch :13870----> train loss : 0.021109359338879585 || test loss : 0.10044130682945251\n",
      " epoch :13880----> train loss : 0.0210841316729784 || test loss : 0.10042841732501984\n",
      " epoch :13890----> train loss : 0.02105896919965744 || test loss : 0.10041540861129761\n",
      " epoch :13900----> train loss : 0.021033858880400658 || test loss : 0.10040255635976791\n",
      " epoch :13910----> train loss : 0.021008824929594994 || test loss : 0.10038971155881882\n",
      " epoch :13920----> train loss : 0.020983852446079254 || test loss : 0.10037694126367569\n",
      " epoch :13930----> train loss : 0.02095893584191799 || test loss : 0.10036422312259674\n",
      " epoch :13940----> train loss : 0.020934075117111206 || test loss : 0.10035170614719391\n",
      " epoch :13950----> train loss : 0.020909272134304047 || test loss : 0.10033917427062988\n",
      " epoch :13960----> train loss : 0.02088453806936741 || test loss : 0.10032666474580765\n",
      " epoch :13970----> train loss : 0.020859863609075546 || test loss : 0.100314199924469\n",
      " epoch :13980----> train loss : 0.02083524689078331 || test loss : 0.10030177235603333\n",
      " epoch :13990----> train loss : 0.020810691639780998 || test loss : 0.1002894937992096\n",
      " epoch :14000----> train loss : 0.020786195993423462 || test loss : 0.1002771183848381\n",
      " epoch :14010----> train loss : 0.020761763677001 || test loss : 0.10026485472917557\n",
      " epoch :14020----> train loss : 0.020737387239933014 || test loss : 0.100252665579319\n",
      " epoch :14030----> train loss : 0.0207130815833807 || test loss : 0.10024052113294601\n",
      " epoch :14040----> train loss : 0.020688826218247414 || test loss : 0.10022840648889542\n",
      " epoch :14050----> train loss : 0.020664628595113754 || test loss : 0.100216343998909\n",
      " epoch :14060----> train loss : 0.020640499889850616 || test loss : 0.10020436346530914\n",
      " epoch :14070----> train loss : 0.02061641961336136 || test loss : 0.1001923531293869\n",
      " epoch :14080----> train loss : 0.020592408254742622 || test loss : 0.10018052905797958\n",
      " epoch :14090----> train loss : 0.020568449050188065 || test loss : 0.10016859322786331\n",
      " epoch :14100----> train loss : 0.02054455317556858 || test loss : 0.1001567617058754\n",
      " epoch :14110----> train loss : 0.020520715042948723 || test loss : 0.10014493763446808\n",
      " epoch :14120----> train loss : 0.020496930927038193 || test loss : 0.10013330727815628\n",
      " epoch :14130----> train loss : 0.02047320269048214 || test loss : 0.10012161731719971\n",
      " epoch :14140----> train loss : 0.02044953964650631 || test loss : 0.1001100018620491\n",
      " epoch :14150----> train loss : 0.020425934344530106 || test loss : 0.10009831935167313\n",
      " epoch :14160----> train loss : 0.02040238119661808 || test loss : 0.10008684545755386\n",
      " epoch :14170----> train loss : 0.02037888579070568 || test loss : 0.10007532685995102\n",
      " epoch :14180----> train loss : 0.020355448126792908 || test loss : 0.10006387531757355\n",
      " epoch :14190----> train loss : 0.02033206820487976 || test loss : 0.10005246102809906\n",
      " epoch :14200----> train loss : 0.02030874602496624 || test loss : 0.10004118829965591\n",
      " epoch :14210----> train loss : 0.020285477861762047 || test loss : 0.100029855966568\n",
      " epoch :14220----> train loss : 0.02026226557791233 || test loss : 0.10001860558986664\n",
      " epoch :14230----> train loss : 0.02023911662399769 || test loss : 0.10000737756490707\n",
      " epoch :14240----> train loss : 0.020216014236211777 || test loss : 0.09999622404575348\n",
      " epoch :14250----> train loss : 0.020192965865135193 || test loss : 0.09998510777950287\n",
      " epoch :14260----> train loss : 0.020169978961348534 || test loss : 0.09997416287660599\n",
      " epoch :14270----> train loss : 0.0201470535248518 || test loss : 0.09996321052312851\n",
      " epoch :14280----> train loss : 0.020124172791838646 || test loss : 0.09995225071907043\n",
      " epoch :14290----> train loss : 0.020101351663470268 || test loss : 0.09994138777256012\n",
      " epoch :14300----> train loss : 0.020078586414456367 || test loss : 0.0999305322766304\n",
      " epoch :14310----> train loss : 0.020055869594216347 || test loss : 0.09991966187953949\n",
      " epoch :14320----> train loss : 0.020033206790685654 || test loss : 0.09990881383419037\n",
      " epoch :14330----> train loss : 0.02001059800386429 || test loss : 0.09989809989929199\n",
      " epoch :14340----> train loss : 0.0199880488216877 || test loss : 0.09988739341497421\n",
      " epoch :14350----> train loss : 0.019965551793575287 || test loss : 0.09987670183181763\n",
      " epoch :14360----> train loss : 0.01994311809539795 || test loss : 0.09986595064401627\n",
      " epoch :14370----> train loss : 0.01992073841392994 || test loss : 0.09985532611608505\n",
      " epoch :14380----> train loss : 0.01989840716123581 || test loss : 0.09984476119279861\n",
      " epoch :14390----> train loss : 0.019876128062605858 || test loss : 0.09983419626951218\n",
      " epoch :14400----> train loss : 0.019853904843330383 || test loss : 0.09982378035783768\n",
      " epoch :14410----> train loss : 0.019831741228699684 || test loss : 0.09981328994035721\n",
      " epoch :14420----> train loss : 0.019809624180197716 || test loss : 0.09980285167694092\n",
      " epoch :14430----> train loss : 0.019787559285759926 || test loss : 0.09979245811700821\n",
      " epoch :14440----> train loss : 0.019765548408031464 || test loss : 0.09978210926055908\n",
      " epoch :14450----> train loss : 0.01974358782172203 || test loss : 0.0997719094157219\n",
      " epoch :14460----> train loss : 0.019721677526831627 || test loss : 0.09976168721914291\n",
      " epoch :14470----> train loss : 0.019699815660715103 || test loss : 0.09975141286849976\n",
      " epoch :14480----> train loss : 0.019678011536598206 || test loss : 0.09974124282598495\n",
      " epoch :14490----> train loss : 0.019656261429190636 || test loss : 0.09973100572824478\n",
      " epoch :14500----> train loss : 0.019634559750556946 || test loss : 0.09972097724676132\n",
      " epoch :14510----> train loss : 0.019612904638051987 || test loss : 0.09971079230308533\n",
      " epoch :14520----> train loss : 0.019591299816966057 || test loss : 0.0997006818652153\n",
      " epoch :14530----> train loss : 0.019569747149944305 || test loss : 0.09969072043895721\n",
      " epoch :14540----> train loss : 0.019548244774341583 || test loss : 0.09968069940805435\n",
      " epoch :14550----> train loss : 0.019526798278093338 || test loss : 0.09967079013586044\n",
      " epoch :14560----> train loss : 0.019505389034748077 || test loss : 0.09966099262237549\n",
      " epoch :14570----> train loss : 0.019484033808112144 || test loss : 0.09965118765830994\n",
      " epoch :14580----> train loss : 0.01946273259818554 || test loss : 0.09964142739772797\n",
      " epoch :14590----> train loss : 0.019441476091742516 || test loss : 0.09963160008192062\n",
      " epoch :14600----> train loss : 0.019420264288783073 || test loss : 0.09962193667888641\n",
      " epoch :14610----> train loss : 0.019399110227823257 || test loss : 0.09961222857236862\n",
      " epoch :14620----> train loss : 0.01937800832092762 || test loss : 0.09960261732339859\n",
      " epoch :14630----> train loss : 0.019356947392225266 || test loss : 0.09959306567907333\n",
      " epoch :14640----> train loss : 0.01933593861758709 || test loss : 0.0995834618806839\n",
      " epoch :14650----> train loss : 0.019314980134367943 || test loss : 0.09957382827997208\n",
      " epoch :14660----> train loss : 0.019294071942567825 || test loss : 0.09956430643796921\n",
      " epoch :14670----> train loss : 0.01927320472896099 || test loss : 0.09955478459596634\n",
      " epoch :14680----> train loss : 0.019252393394708633 || test loss : 0.09954540431499481\n",
      " epoch :14690----> train loss : 0.019231626763939857 || test loss : 0.09953609853982925\n",
      " epoch :14700----> train loss : 0.019210906699299812 || test loss : 0.09952669590711594\n",
      " epoch :14710----> train loss : 0.019190235063433647 || test loss : 0.09951746463775635\n",
      " epoch :14720----> train loss : 0.01916961558163166 || test loss : 0.09950821846723557\n",
      " epoch :14730----> train loss : 0.01914903335273266 || test loss : 0.09949899464845657\n",
      " epoch :14740----> train loss : 0.019128507003188133 || test loss : 0.09948988258838654\n",
      " epoch :14750----> train loss : 0.01910802349448204 || test loss : 0.09948063641786575\n",
      " epoch :14760----> train loss : 0.019087588414549828 || test loss : 0.09947151690721512\n",
      " epoch :14770----> train loss : 0.019067205488681793 || test loss : 0.09946244210004807\n",
      " epoch :14780----> train loss : 0.019046859815716743 || test loss : 0.0994533821940422\n",
      " epoch :14790----> train loss : 0.019026566296815872 || test loss : 0.09944446384906769\n",
      " epoch :14800----> train loss : 0.019006311893463135 || test loss : 0.09943556785583496\n",
      " epoch :14810----> train loss : 0.01898609846830368 || test loss : 0.09942664951086044\n",
      " epoch :14820----> train loss : 0.018965933471918106 || test loss : 0.09941770881414413\n",
      " epoch :14830----> train loss : 0.01894582435488701 || test loss : 0.0994088351726532\n",
      " epoch :14840----> train loss : 0.018925748765468597 || test loss : 0.09939998388290405\n",
      " epoch :14850----> train loss : 0.01890571601688862 || test loss : 0.09939117729663849\n",
      " epoch :14860----> train loss : 0.018885737285017967 || test loss : 0.09938250482082367\n",
      " epoch :14870----> train loss : 0.01886579766869545 || test loss : 0.09937373548746109\n",
      " epoch :14880----> train loss : 0.018845899030566216 || test loss : 0.09936501830816269\n",
      " epoch :14890----> train loss : 0.01882605068385601 || test loss : 0.09935649484395981\n",
      " epoch :14900----> train loss : 0.018806232139468193 || test loss : 0.09934790432453156\n",
      " epoch :14910----> train loss : 0.018786467611789703 || test loss : 0.09933944791555405\n",
      " epoch :14920----> train loss : 0.018766749650239944 || test loss : 0.09933091700077057\n",
      " epoch :14930----> train loss : 0.018747076392173767 || test loss : 0.09932252019643784\n",
      " epoch :14940----> train loss : 0.018727442249655724 || test loss : 0.0993141457438469\n",
      " epoch :14950----> train loss : 0.01870785839855671 || test loss : 0.09930576384067535\n",
      " epoch :14960----> train loss : 0.01868831366300583 || test loss : 0.09929738938808441\n",
      " epoch :14970----> train loss : 0.018668808043003082 || test loss : 0.09928905963897705\n",
      " epoch :14980----> train loss : 0.018649350851774216 || test loss : 0.09928080439567566\n",
      " epoch :14990----> train loss : 0.01862993836402893 || test loss : 0.09927249699831009\n"
     ]
    }
   ],
   "source": [
    "epochs =15000\n",
    "for epoch in range(epochs):\n",
    "  model1.train()\n",
    "  y_pred = model1(X_train_tensor).squeeze()\n",
    "  loss = loss_fn(y_pred , y_train_tensor)\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  model1.eval()\n",
    "  with torch.no_grad():\n",
    "    y_pred = model1(X_test_tensor).squeeze()\n",
    "    test_loss = loss_fn(y_pred , y_test_tensor)\n",
    "    if epoch%10==0:\n",
    "      print(f\" epoch :{ epoch }----> train loss : {loss} || test loss : {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "140fa887-5be0-4369-904b-d64c6407b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "with torch.inference_mode():\n",
    "  y_pred_logits = model1(X_test_tensor).squeeze()\n",
    "  y_pred_prob = torch.sigmoid(y_pred_logits)\n",
    "  y_pred_label = torch.round(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95807f68-af71-4491-ac97-bf6ad34772ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_label= y_pred_label.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6202e391-6926-496a-933d-aa21f6fede90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1430f43d-b831-40a0-bd8f-4fb4990e62b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 95.96899224806201 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score  = accuracy_score(y_test , y_pred_label)\n",
    "print(f\"accuracy : {score*100} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
