{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53121025-ba79-40b3-be73-5c58ad8a0510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'CARD'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/XzwHan/CARD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb11b44-072c-4934-be6c-aa49b9ddb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from CARD.regression.model import (\n",
    "    DeterministicFeedForwardNeuralNetwork,\n",
    "    ConditionalLinear,\n",
    ")\n",
    "from CARD.regression.diffusion_utils import (\n",
    "    make_beta_schedule,\n",
    "    q_sample,\n",
    "    p_sample_loop,\n",
    "    p_sample,\n",
    ")\n",
    "\n",
    "scl = StandardScaler()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23d1f64-6921-49ea-bcc5-cc2d3accfd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\dell\\Desktop\\MyDocs\\Docs\\MK\\protein.csv')\n",
    "X = (df.drop(columns=[\"RMSD\"], axis=1)).values\n",
    "y = (df[\"RMSD\"]).values\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "X, y = torch.from_numpy(X).to(torch.float32), torch.from_numpy(y).to(torch.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1\n",
    ")\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(val_data, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e23ea05-6942-4630-9a47-cc469966414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 500\n",
    "cat_x = True\n",
    "cat_y_pred = True\n",
    "\n",
    "x_dim = X_train.shape[1]\n",
    "y_dim = 1\n",
    "z_dim = 2\n",
    "\n",
    "hid_layers = [100, 50]\n",
    "\n",
    "beta_schedule = \"linear\"\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5227f4-bf78-469d-b3b5-96cc88db0aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionalGuidedModel(\n",
       "  (lin1): ConditionalLinear(\n",
       "    (lin): Linear(in_features=11, out_features=128, bias=True)\n",
       "    (embed): Embedding(500, 128)\n",
       "  )\n",
       "  (lin2): ConditionalLinear(\n",
       "    (lin): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (embed): Embedding(500, 128)\n",
       "  )\n",
       "  (lin3): ConditionalLinear(\n",
       "    (lin): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (embed): Embedding(500, 128)\n",
       "  )\n",
       "  (lin4): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConditionalGuidedModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_steps: int,\n",
    "        cat_x: bool,\n",
    "        cat_y_pred: bool,\n",
    "        x_dim: int,\n",
    "        y_dim: int,\n",
    "        z_dim: int,\n",
    "    ):\n",
    "        super(ConditionalGuidedModel, self).__init__()\n",
    "        self.cat_x = cat_x\n",
    "        self.cat_y_pred = cat_y_pred\n",
    "        data_dim = y_dim\n",
    "        if self.cat_x:\n",
    "            data_dim += x_dim\n",
    "        if self.cat_y_pred:\n",
    "            data_dim += y_dim\n",
    "        self.lin1 = ConditionalLinear(data_dim, 128, n_steps)\n",
    "        self.lin2 = ConditionalLinear(128, 128, n_steps)\n",
    "        self.lin3 = ConditionalLinear(128, 128, n_steps)\n",
    "        self.lin4 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, y_t, y_0_hat, t):\n",
    "        if self.cat_x:\n",
    "            if self.cat_y_pred:\n",
    "                eps_pred = torch.cat((y_t, y_0_hat, x), dim=1)\n",
    "            else:\n",
    "                eps_pred = torch.cat((y_t, x), dim=1)\n",
    "        else:\n",
    "            if self.cat_y_pred:\n",
    "                eps_pred = torch.cat((y_t, y_0_hat), dim=1)\n",
    "            else:\n",
    "                eps_pred = y_t\n",
    "        eps_pred = F.softplus(self.lin1(eps_pred, t))\n",
    "        eps_pred = F.softplus(self.lin2(eps_pred, t))\n",
    "        eps_pred = F.softplus(self.lin3(eps_pred, t))\n",
    "        return self.lin4(eps_pred)\n",
    "\n",
    "\n",
    "diff_model = ConditionalGuidedModel(\n",
    "    n_steps=n_steps,\n",
    "    cat_x=cat_x,\n",
    "    cat_y_pred=cat_y_pred,\n",
    "    x_dim=x_dim,\n",
    "    y_dim=y_dim,\n",
    "    z_dim=z_dim,\n",
    ")\n",
    "diff_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6949a0b2-9883-43ef-8d7e-c7bbf7d3c277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeterministicFeedForwardNeuralNetwork(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=100, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0, inplace=False)\n",
       "    (3): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0, inplace=False)\n",
       "    (6): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_pred_model = DeterministicFeedForwardNeuralNetwork(dim_in=x_dim, \n",
    "                                                        dim_out=y_dim, \n",
    "                                                        hid_layers=hid_layers\n",
    "                                                        )\n",
    "cond_pred_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2de83fb-ef9a-4498-85fc-4ca1b8bf262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 46.64861297607422: 100%|██████████| 10/10 [00:15<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "n_pretrain_epochs = 10\n",
    "aux_optimizer = Adam(cond_pred_model.parameters(), lr=0.01)\n",
    "aux_cost_fn = nn.MSELoss()\n",
    "cond_pred_model.train()\n",
    "\n",
    "bar = trange(n_pretrain_epochs, leave=True)\n",
    "for epoch in bar:\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = cond_pred_model(x)\n",
    "        aux_cost = aux_cost_fn(y_pred, y)\n",
    "\n",
    "        aux_optimizer.zero_grad()\n",
    "        aux_cost.backward()\n",
    "        aux_optimizer.step()\n",
    "        bar.set_description(f\"Loss: {aux_cost.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f0ae04f-9c28-4f94-b121-4ba5bf3c10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = make_beta_schedule(beta_schedule, n_steps, beta_start, beta_end).to(device)\n",
    "betas_sqrt = torch.sqrt(betas)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = alphas.cumprod(dim=0)\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_cumprod)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a53250b3-27de-452b-b1d3-c7d3d262a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 24.112424850463867: 100%|██████████| 10/10 [00:37<00:00,  3.78s/it]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "optimizer = Adam(diff_model.parameters(), lr=0.01)\n",
    "\n",
    "diff_bar = trange(n_epochs, leave=True)\n",
    "diff_model.train()\n",
    "\n",
    "for epoch in diff_bar:\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # antithetic sampling\n",
    "        ant_samples_t = torch.randint(\n",
    "            low=0, high=n_steps, size=(batch_size // 2 + 1,)\n",
    "        ).to(device)\n",
    "        ant_samples_t = torch.cat([ant_samples_t, n_steps - 1 - ant_samples_t], dim=0)[\n",
    "            :batch_size\n",
    "        ]\n",
    "\n",
    "        # noise estimation loss\n",
    "        y_0_hat = cond_pred_model(x)\n",
    "\n",
    "        e = torch.randn_like(y)\n",
    "\n",
    "        y_t_sample = q_sample(\n",
    "            y,\n",
    "            y_0_hat,\n",
    "            alphas_bar_sqrt,\n",
    "            one_minus_alphas_bar_sqrt,\n",
    "            ant_samples_t,\n",
    "            noise=e,\n",
    "        )\n",
    "\n",
    "        model_output = diff_model(x, y_t_sample, y_0_hat, ant_samples_t)\n",
    "\n",
    "        # use the same noise sample e during training to compute loss\n",
    "        loss = (e - model_output).square().mean()\n",
    "\n",
    "        # optimize diffusion model that predicts eps_theta\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # optimize non-linear guidance model\n",
    "        aux_cost = aux_cost_fn(cond_pred_model(x), y)\n",
    "        aux_optimizer.zero_grad()\n",
    "        aux_cost.backward()\n",
    "        aux_optimizer.step()\n",
    "\n",
    "        diff_bar.set_description(f\"Loss: {loss.item()}\", refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5678b653-4460-4432-a838-4e68554d3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_z_samples = 10\n",
    "\n",
    "y_0_hat = cond_pred_model(X_test.to(device))\n",
    "\n",
    "# obtain y samples through reverse diffusion -- some pytorch version might not have torch.tile\n",
    "# y_0_tile = torch.tile(y, (n_z_samples, 1))\n",
    "y_0_hat_tile = torch.tile(y_0_hat, (n_z_samples, 1)).to(device)\n",
    "test_x_tile = torch.tile(X_test, (n_z_samples, 1)).to(device)\n",
    "\n",
    "z = torch.randn_like(y_0_hat_tile).to(device)\n",
    "\n",
    "y_t = y_0_hat_tile + z\n",
    "\n",
    "def extract(input, t, x):\n",
    "    shape = x.shape\n",
    "    out = torch.gather(input, 0, t)\n",
    "    reshape = [t.shape[0]] + [1] * (len(shape) - 1)\n",
    "    return out.reshape(*reshape)\n",
    "\n",
    "\n",
    "def p_sample(\n",
    "    x, y, y_0_hat, y_T_mean, t: int, alphas, one_minus_alphas_bar_sqrt, guidance_model\n",
    "):\n",
    "    z = torch.randn_like(y)\n",
    "    t = torch.tensor([t]).to(device)\n",
    "    alpha_t = extract(alphas, t, y)\n",
    "    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)\n",
    "    sqrt_one_minus_alpha_bar_t_m_1 = extract(one_minus_alphas_bar_sqrt, t - 1, y)\n",
    "    sqrt_alpha_bar_t = (1 - sqrt_one_minus_alpha_bar_t.square()).sqrt()\n",
    "    sqrt_alpha_bar_t_m_1 = (1 - sqrt_one_minus_alpha_bar_t_m_1.square()).sqrt()\n",
    "    # y_t_m_1 posterior mean component coefficients\n",
    "    gamma_0 = (\n",
    "        (1 - alpha_t) * sqrt_alpha_bar_t_m_1 / (sqrt_one_minus_alpha_bar_t.square())\n",
    "    )\n",
    "    gamma_1 = (\n",
    "        (sqrt_one_minus_alpha_bar_t_m_1.square())\n",
    "        * (alpha_t.sqrt())\n",
    "        / (sqrt_one_minus_alpha_bar_t.square())\n",
    "    )\n",
    "    gamma_2 = 1 + (sqrt_alpha_bar_t - 1) * (alpha_t.sqrt() + sqrt_alpha_bar_t_m_1) / (\n",
    "        sqrt_one_minus_alpha_bar_t.square()\n",
    "    )\n",
    "    eps_theta = guidance_model(x, y, y_0_hat, t).detach()\n",
    "    # y_0 reparameterization\n",
    "    y_0_reparam = (\n",
    "        1\n",
    "        / sqrt_alpha_bar_t\n",
    "        * (\n",
    "            y\n",
    "            - (1 - sqrt_alpha_bar_t) * y_T_mean\n",
    "            - eps_theta * sqrt_one_minus_alpha_bar_t\n",
    "        )\n",
    "    )\n",
    "    y_t_m_1_hat = gamma_0 * y_0_reparam + gamma_1 * y + gamma_2 * y_T_mean\n",
    "\n",
    "    beta_t_hat = (\n",
    "        (sqrt_one_minus_alpha_bar_t_m_1.square())\n",
    "        / (sqrt_one_minus_alpha_bar_t.square())\n",
    "        * (1 - alpha_t)\n",
    "    )\n",
    "    y_t_m_1 = y_t_m_1_hat.to(device) + beta_t_hat.sqrt().to(device) * z.to(device)\n",
    "    return y_t_m_1\n",
    "\n",
    "\n",
    "def p_sample_loop(\n",
    "    x,\n",
    "    y_0_hat,\n",
    "    y_T_mean,\n",
    "    n_steps,\n",
    "    alphas,\n",
    "    one_minus_alphas_bar_sqrt,\n",
    "    only_last_sample,\n",
    "    guidance_model,\n",
    "):\n",
    "    num_t, y_p_seq = None, None\n",
    "    z = torch.randn_like(y_T_mean).to(device)\n",
    "    cur_y = z + y_T_mean  # sampled y_T\n",
    "    if only_last_sample:\n",
    "        num_t = 1\n",
    "    else:\n",
    "        y_p_seq = [cur_y]\n",
    "    for t in reversed(range(1, n_steps)):\n",
    "        y_t = cur_y\n",
    "        cur_y = p_sample(\n",
    "            x,\n",
    "            y_t,\n",
    "            y_0_hat,\n",
    "            y_T_mean,\n",
    "            t,\n",
    "            alphas,\n",
    "            one_minus_alphas_bar_sqrt,\n",
    "            guidance_model,\n",
    "        )  # y_{t-1}\n",
    "        if only_last_sample:\n",
    "            num_t += 1\n",
    "        else:\n",
    "            y_p_seq.append(cur_y)\n",
    "    if only_last_sample:\n",
    "        assert num_t == n_steps\n",
    "        y_0 = p_sample_t_1to0(\n",
    "            x, cur_y, y_0_hat, y_T_mean, one_minus_alphas_bar_sqrt, guidance_model\n",
    "        )\n",
    "        return y_0\n",
    "    else:\n",
    "        assert len(y_p_seq) == n_steps\n",
    "        y_0 = p_sample_t_1to0(\n",
    "            x, y_p_seq[-1], y_0_hat, y_T_mean, one_minus_alphas_bar_sqrt, guidance_model\n",
    "        )\n",
    "        y_p_seq.append(y_0)\n",
    "        return y_p_seq\n",
    "\n",
    "\n",
    "def p_sample_t_1to0(x, y, y_0_hat, y_T_mean, one_minus_alphas_bar_sqrt, guidance_model):\n",
    "    # corresponding to timestep 1 (i.e., t=1 in diffusion models)\n",
    "    t = torch.tensor([0]).to(device)\n",
    "    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)\n",
    "    sqrt_alpha_bar_t = (1 - sqrt_one_minus_alpha_bar_t.square()).sqrt()\n",
    "    eps_theta = guidance_model(x, y, y_0_hat, t).detach()\n",
    "    # y_0 reparameterization\n",
    "    y_0_reparam = (\n",
    "        1\n",
    "        / sqrt_alpha_bar_t\n",
    "        * (\n",
    "            y\n",
    "            - (1 - sqrt_alpha_bar_t) * y_T_mean\n",
    "            - eps_theta * sqrt_one_minus_alpha_bar_t\n",
    "        )\n",
    "    )\n",
    "    y_t_m_1 = y_0_reparam.to(device)\n",
    "    return y_t_m_1\n",
    "\n",
    "\n",
    "def predict_step(\n",
    "    x,\n",
    "    y_0_hat,\n",
    "    alphas,\n",
    "    one_minus_alphas_bar_sqrt,\n",
    "    cond_pred_model,\n",
    "    n_z_samples,\n",
    "    n_steps,\n",
    "    guidance_model,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        y_0_hat = cond_pred_model(x)\n",
    "\n",
    "        y_0_hat_tile = torch.tile(y_0_hat, (n_z_samples, 1)).to(device)\n",
    "        test_x_tile = torch.tile(x, (n_z_samples, 1)).to(device)\n",
    "\n",
    "        z = torch.randn_like(y_0_hat_tile).to(device)\n",
    "\n",
    "        # generate samples from all time steps for the mini-batch\n",
    "        y_tile_seq = p_sample_loop(\n",
    "            test_x_tile,\n",
    "            y_0_hat_tile,\n",
    "            y_0_hat_tile,\n",
    "            n_steps,\n",
    "            alphas.to(device),\n",
    "            one_minus_alphas_bar_sqrt.to(device),\n",
    "            False,\n",
    "            guidance_model,\n",
    "        )\n",
    "\n",
    "        # put in shape [n_z_samples, batch_size, output_dimension]\n",
    "        y_tile_seq = [\n",
    "            arr.reshape(n_z_samples, x.shape[0], y_t.shape[-1]) for arr in y_tile_seq\n",
    "        ]\n",
    "\n",
    "        final_recoverd = y_tile_seq[-1]\n",
    "\n",
    "        mean_pred = final_recoverd.mean(dim=0).detach().cpu().squeeze()\n",
    "        std_pred = final_recoverd.std(dim=0).detach().cpu().squeeze()\n",
    "\n",
    "        return {\n",
    "            \"pred\": mean_pred,\n",
    "            \"pred_uct\": std_pred,\n",
    "            \"aleatoric_uct\": std_pred,\n",
    "            \"samples\": y_tile_seq,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5fe1d6b-0b1f-483a-89bb-512c808d3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0_hat = cond_pred_model(x.to(device))\n",
    "pred = predict_step(\n",
    "    X_test,\n",
    "    y_0_hat,\n",
    "    alphas,\n",
    "    one_minus_alphas_bar_sqrt,\n",
    "    cond_pred_model,\n",
    "    n_z_samples,\n",
    "    n_steps,\n",
    "    diff_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb231eb-ba2d-4057-aec2-43182e1b6264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score 190.108276\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE score {mean_squared_error(y_test.detach().numpy().squeeze(), pred[\"pred\"].detach().numpy().squeeze()):5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
