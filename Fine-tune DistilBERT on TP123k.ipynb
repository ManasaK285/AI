{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf6d112-b7e2-4ed1-8068-0928aec6acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers evaluate accelerate cleantext -qqqqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b671b2-cde3-48ec-874f-26bdbc8b5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_MODE\"]=\"offline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1065b8-d940-402a-86ca-91bdb940bf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fbfffafbf74a21850e7393b9257276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset as a huggingface dataset object\n",
    "ds = load_dataset('csv', data_files={'train': r\"C:\\Users\\dell\\Desktop\\MyDocs\\Docs\\MK\\trustpilot_reviews_2005.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57420e8-572e-4bf0-b552-c99a7689932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(examples):\n",
    "    label = 0 # Default to Negative\n",
    "    if examples['stars'] == 5 or examples['stars'] == 4:\n",
    "        label = 2 # Positive\n",
    "    if examples['stars'] == 3:\n",
    "        label = 1 # Neutral\n",
    "\n",
    "    examples['label'] = label\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d93c5179-d080-446e-ba1b-bd33337685b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "\n",
    "def clean_text(examples):\n",
    "    examples['clean_review'] = clean(examples['review'], clean_all=False, lowercase=True, extra_spaces=True)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2c4b2af-4e6d-4f32-bb0c-80c5c36d2f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2101e820a69f4490ae562f37dd4e86e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123181 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labeled_ds = ds.map(create_labels, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed3f7d5c-bb6a-4d2a-950a-b78e3251989c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928376339de14404ac628529e3292931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123181 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labeled_ds = labeled_ds.map(clean_text, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a0e165d-3ab3-4b6f-bcff-4aad6d3c75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds = labeled_ds['train'].train_test_split(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f1b5c3-3406-467e-8647-255e90271daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['category', 'company', 'description', 'title', 'review', 'stars', 'label', 'clean_review'],\n",
       "        num_rows: 92385\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['category', 'company', 'description', 'title', 'review', 'stars', 'label', 'clean_review'],\n",
       "        num_rows: 30796\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe906e1e-d944-4296-aae1-5a9a776ca2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': ['Electronics & Technology', 'Sports', 'Restaurants & Bars'],\n",
      " 'clean_review': ['i wrote a review on here earlier and explained several '\n",
      "                  \"times that i'm deaf. they then replied and asked me to \"\n",
      "                  'phone them .............i think shark employees need a '\n",
      "                  'refresher on how to deal with disabled customers. lesson 1 '\n",
      "                  \"would go as follows: don't ask customers who have explained \"\n",
      "                  'that they are deaf, and can only email you to phone '\n",
      "                  \"customer services. i can only assume that they don't \"\n",
      "                  'actually read the reviews and instead use the same copy and '\n",
      "                  'paste reply for all negative reviews. no customer should '\n",
      "                  'have to resort to begging a company via a negative review '\n",
      "                  \"to reply to their email. i didn't think my experience with \"\n",
      "                  'shark could get any more frustrating, but here we are. '\n",
      "                  'please email me back. thank you.',\n",
      "                  'the ultimate leggings have no high waist band when i sat '\n",
      "                  'down they pulled down , felt very cheap and uncomfortable '\n",
      "                  'seem very expensive for this sort of fitthe ultimate base '\n",
      "                  'layer fits well , but itchy but oki wasn’t wowed by the '\n",
      "                  'quality compared to amazon products',\n",
      "                  'this is the best company for taking your money and not '\n",
      "                  'giving you goods and giving you a story how they sent it '\n",
      "                  'but it must have got lost in the post , but will send an '\n",
      "                  'other and still waiting , i paid on 11th march now it’s the '\n",
      "                  '25th march .. please stay away from this company .'],\n",
      " 'company': ['sharkclean.co.uk', 'zerofit.co.uk', 'cateringpacks.co.uk'],\n",
      " 'description': ['\\n'\n",
      "                 ' \\n'\n",
      "                 ' Shark, the leading brand in vacuum cleaners in the UK*, is '\n",
      "                 'committed to crafting high-quality household products. Our '\n",
      "                 'range includes a wide selection of corded, cordless, and '\n",
      "                 'handheld vacuum cleaners, as well as steam mops, air '\n",
      "                 'purifiers, hair dryers, and other innovative solutions.\\n'\n",
      "                 '\\n'\n",
      "                 '\\n'\n",
      "                 ' Shop Upright Vacuums\\n'\n",
      "                 ' Shop Cordless Vacuums \\n'\n",
      "                 ' Shop Stratos \\n'\n",
      "                 ' Shop Shark Beauty \\n'\n",
      "                 ' Shop Offers\\n'\n",
      "                 ' Contact Us \\n'\n",
      "                 'Free Next Day Delivery\\n'\n",
      "                 '7 days a week*  \\n'\n",
      "                 ' Automatic 5-Year Guarantee*  \\n'\n",
      "                 'Confidence in our design and quality \\n'\n",
      "                 '30 Day Returns  \\n'\n",
      "                 'Free returns on main units \\n'\n",
      "                 ' Pay-in-3 \\n'\n",
      "                 'Shop now, pay later with Klarna ',\n",
      "                 'Zerofit is a Japanese specialist baselayer brand which was '\n",
      "                 'founded by Mr Koji Higashi in 2005. It launched in the UK '\n",
      "                 'and Europe in 2018 before further expansion into North '\n",
      "                 'America in 2020 and Australasia in 2021. \\n'\n",
      "                 'The award-winning Heatrub Ultimate baselayer was unveiled to '\n",
      "                 'the Japanese public in 2011 and since then hundreds of '\n",
      "                 'thousands of outdoor enthusiasts have discovered the '\n",
      "                 'benefits of that best-in-class product. Independently tested '\n",
      "                 'and proven to be five times warmer than a standard '\n",
      "                 'baselayer, the Heatrub Ultimate has become a must-have item '\n",
      "                 'in the coldest months of the year and, such was the '\n",
      "                 'popularity of the baselayer, Zerofit now offer leggings and '\n",
      "                 'socks made from the same material.\\n'\n",
      "                 'Other key products include the Heatrub Move baselayer, which '\n",
      "                 'features Adaptable Warmth Technology, keeping you warm while '\n",
      "                 'at a standstill but also temperature regulation when you’re '\n",
      "                 'working up a sweat. For the summer months, the Cold Skin '\n",
      "                 'baselayer is three times colder than a competitor product, '\n",
      "                 'while the Ice Weave is the master when it comes to wicking '\n",
      "                 'excess sweat in the hottest conditions.\\n'\n",
      "                 'Following exponential growth across industries including '\n",
      "                 'motorcycling, angling, running, hiking, sailing, golf, '\n",
      "                 'gardening, birdwatching and equestrian sports, Zerofit '\n",
      "                 'welcomed a new shareholder and ambassador in February 2023 - '\n",
      "                 \"Brian O'Driscoll, the former Leinster, Ireland and British & \"\n",
      "                 'Irish Lions rugby captain in order to help further enhance '\n",
      "                 'the business across multiple categories. And since November '\n",
      "                 '2023, Bennetts British Superbikes Runner-Up Glenn Irwin has '\n",
      "                 'joined the brand as an ambassador. He will rely on Zerofit '\n",
      "                 'baselayers while racing during the 2024 season. \\n'\n",
      "                 'The European HQ of Zerofit is in County Antrim, Northern '\n",
      "                 'Ireland, with further offices in Adelaide, Australia and San '\n",
      "                 'Diego in the US. Specialist customer service teams in each '\n",
      "                 'country ensure prompt and accurate delivery of products as '\n",
      "                 'the brand continues to grow globally.\\n'\n",
      "                 'Whatever your sport or activity, we hope Zerofit will become '\n",
      "                 'your baselayer brand of choice. \\n',\n",
      "                 'All of your favourites in bulk sizes'],\n",
      " 'label': [0, 1, 0],\n",
      " 'review': ['I wrote a review on here earlier and explained several times that '\n",
      "            \"I'm deaf. They then replied and asked me to phone them \"\n",
      "            '.............I think shark employees need a refresher on how to '\n",
      "            \"deal with disabled customers. Lesson 1 would go as follows: Don't \"\n",
      "            'ask customers who have explained that they are deaf, and can only '\n",
      "            'email you to phone customer services. I can only assume that they '\n",
      "            \"don't actually read the reviews and instead use the same copy and \"\n",
      "            'paste reply for all negative reviews. No customer should have to '\n",
      "            'resort to begging a company via a negative review to reply to '\n",
      "            \"their email. I didn't think my experience with shark could get \"\n",
      "            'any more frustrating, but here we are. Please email me back. '\n",
      "            'Thank you.',\n",
      "            'The ultimate leggings have no high waist band when I sat down '\n",
      "            'they pulled down , felt very cheap and uncomfortable Seem very '\n",
      "            'expensive for this sort of fitThe ultimate base layer fits well , '\n",
      "            'but itchy but okI wasn’t wowed by the quality compared to Amazon '\n",
      "            'products',\n",
      "            'This is the best company for taking your money and not giving you '\n",
      "            'goods and giving you a story how they sent it but it must have '\n",
      "            'got lost in the post , but will send an other and still waiting , '\n",
      "            'I paid on 11th March now it’s the 25th March  .. please stay away '\n",
      "            'from this Company . '],\n",
      " 'stars': [1, 3, 1],\n",
      " 'title': ['I wrote a review on here earlier and…',\n",
      "           'The ultimate leggings have no high…',\n",
      "           'This is the best company for taking…']}\n",
      "{'category': ['Beauty & Well-being',\n",
      "              'Public & Local Services',\n",
      "              'Hobbies & Crafts'],\n",
      " 'clean_review': [\"the vast majority of reviews on here are from people who've \"\n",
      "                  'had surgery in the last few months. i thought it more '\n",
      "                  'useful to leave a review 1 year post surgery. the overall '\n",
      "                  'look of my hair is better than before but the reason for '\n",
      "                  'leaving a 3 star review is because there is a large patch '\n",
      "                  \"on one side were the hairs haven't taken very well and is \"\n",
      "                  'almost bald. i can make it look ok by brushing the hair '\n",
      "                  'across. i followed the post surgery advice exactly. '\n",
      "                  'apparently i have to wait for 15 months for full results '\n",
      "                  'but no more hairs are going to grow are they !',\n",
      "                  'i have had fitted 9 fireplaces all certified by hetas '\n",
      "                  'certificates everyone is dangerous and does not comply with '\n",
      "                  'building regulations, i have been in contact with hetas for '\n",
      "                  'many months to try to resolve this situation, today they '\n",
      "                  'have contacted me to say there is nothing they can do . '\n",
      "                  'looking at their reviews i can see that i am not the only '\n",
      "                  'house owner to be fobbed off by hetas with dangerous life '\n",
      "                  'threatening fires. lets see what hse and the courts think',\n",
      "                  'the thing i valued most about this course is the clear, '\n",
      "                  'grounded technique taught by committed astrologers with '\n",
      "                  'excellence. at sta, i found the practical tools i was '\n",
      "                  'searching for to provide clear, solid consultations to '\n",
      "                  'those in times of need. this course is a gift.'],\n",
      " 'company': ['wanthair.co.uk', 'hetas.co.uk', 'sta.co'],\n",
      " 'description': ['\\n'\n",
      "                 '\\n'\n",
      "                 \"We are UK's Premier Bespoke Hair Transplant packager with \"\n",
      "                 'clinics in the UK & Turkey.\\n'\n",
      "                 'With offices in Leeds, Manchester & Brighton and a team that '\n",
      "                 'provides 24/7 UK support we can give our clients a FIRST '\n",
      "                 'CLASS service!\\n'\n",
      "                 'All our clinics have been vetted to UK Care Quality '\n",
      "                 'Commission (CQC) standards so whether your surgery is here '\n",
      "                 \"or abroad there's nothing for you to worry about!\\n\",\n",
      "                 'HETAS is the national organisation working for consumer '\n",
      "                 'safety and the wider public interest in safe, efficient and '\n",
      "                 'environmentally responsible use of biomass and other solid '\n",
      "                 \"fuels. Our purpose is 'Working together for a cleaner, safer \"\n",
      "                 \"environment'. Through our website you can search for \"\n",
      "                 'approved and registered installers, chimney sweeps, '\n",
      "                 'retailers, servicing technicians and products ranging from '\n",
      "                 'stoves through to biomass boilers. HETAS operates it’s '\n",
      "                 'competent person scheme under authorisation from The '\n",
      "                 'Department for Levelling Up, Housing and Communities (DLUHC) '\n",
      "                 'and overseen by The United Kingdom Accreditation Services '\n",
      "                 '(UKAS) under the certification standard BSENISO 17065:2012.',\n",
      "                 'The School of Traditional Astrology (STA) places its '\n",
      "                 'educational focus on the integration of traditional '\n",
      "                 'astrological principles within contemporary astrological '\n",
      "                 'practice. \\n'\n",
      "                 '\\n'\n",
      "                 'Founded by Deborah Houlding in 1996, the STA offers  a '\n",
      "                 'variety of specialist courses for astrologers, including a '\n",
      "                 'practitioners and masters level program in horary astrology '\n",
      "                 '(in English, Spanish and Greek), a program of courses '\n",
      "                 'written by J. Lee Lehman PhD, which specialise in the '\n",
      "                 'traditional principles of Medical Astrology, an Astronomy '\n",
      "                 'for Astrologers course written by Wade Caves, and a Magical '\n",
      "                 'Elections course written and delivered by Nina Gryphon.'],\n",
      " 'label': [1, 0, 2],\n",
      " 'review': [\"The vast majority of reviews on here are from people who've had \"\n",
      "            'surgery in the last few months. I thought it more useful to leave '\n",
      "            'a review 1 year post surgery. the overall look of my hair is '\n",
      "            'better than before BUT the reason for leaving a 3 star review is '\n",
      "            \"because there is a large patch on one side were the hairs haven't \"\n",
      "            'taken very well and is almost bald. I can make it look ok by '\n",
      "            'brushing the hair across. I followed the post surgery advice '\n",
      "            'exactly. apparently I have to wait for 15 months for full results '\n",
      "            'but no more hairs are going to grow are they !',\n",
      "            'I have had fitted 9 fireplaces all certified by HETAS  '\n",
      "            'Certificates everyone is DANGEROUS   and does not comply with '\n",
      "            'Building Regulations, I have been in contact with Hetas for many '\n",
      "            'months to try to resolve this situation, Today they have '\n",
      "            'contacted me to say there is nothing they can do . Looking at '\n",
      "            'their reviews i can see that i am not the only house owner to be '\n",
      "            'fobbed off by  HETAS  with Dangerous life threatening Fires.  '\n",
      "            'LETS SEE WHAT HSE AND THE COURTS THINK',\n",
      "            'The thing I valued most about this course is the clear, grounded '\n",
      "            'technique taught by committed Astrologers with excellence. At '\n",
      "            'STA, I found the practical tools I was searching for to provide '\n",
      "            'clear, solid consultations to those in times of need. This course '\n",
      "            'is a gift.'],\n",
      " 'stars': [3, 1, 5],\n",
      " 'title': ['Manchester hair transplant',\n",
      "           'Do not trust this companies certificates  they are a joke',\n",
      "           'A gift!']}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(labeled_ds['test'][:3])\n",
    "pprint(labeled_ds['train'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d6f1163-dbd0-40b6-8167-267a84c22e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e636eb2df584460a581013e7b7d583c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dell\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e7b65f42f84b3b871ed8844b27744f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64dda0f8a0754626ac9bf327b4ba85b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bec842c689a424c9ce50198bea0771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcd8850cef14b49810cd0d4cdfc04f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/92385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbd86e3325b4fd38e696d463e7138f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"clean_review\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = labeled_ds.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d9e7a5d-ae54-4d23-a346-3c578c2ef7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(3000)) # Use more samples for better accuracy\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(750)) # Or run the cell below instead to use the full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fda8318-d41e-4f00-9f75-e8feb2007b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982d390922c64af3a5e2fab018945c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "881db52d-b245-4ee3-b0ae-d6f0262516dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0988cb75ef4a3a8eaf35819727a544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65a7a0e9-249e-4299-bf88-8d5e52566945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2418a463-7d8b-49d9-832f-c86cd4bd9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=r\"C:\\Users\\dell\\Desktop\\MyDocs\\Docs\\MK\\distilbert_base_tp_2005\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  per_device_eval_batch_size=16,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  weight_decay=0.01,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_dir=r\"C:\\Users\\dell\\Desktop\\MyDocs\\Docs\\MK\\distilbert_base_tp_2005/logs\",\n",
    "                                  run_name = \"distil_bert_uncased_tp_2025\",\n",
    "                                  report_to=\"wandb\",\n",
    "                                  logging_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c6afce8-a11a-453c-b44f-76f58df0c96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.19.3-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wandb) (2.10.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.20.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.4-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Downloading wandb-0.19.3-py3-none-win_amd64.whl (19.7 MB)\n",
      "   ---------------------------------------- 0.0/19.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 3.1/19.7 MB 16.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 7.1/19.7 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 11.0/19.7 MB 18.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 15.2/19.7 MB 18.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 19.1/19.7 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 19.7/19.7 MB 17.8 MB/s eta 0:00:00\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading sentry_sdk-2.20.0-py2.py3-none-any.whl (322 kB)\n",
      "Downloading setproctitle-1.3.4-cp312-cp312-win_amd64.whl (12 kB)\n",
      "Installing collected packages: setproctitle, sentry-sdk, docker-pycreds, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 sentry-sdk-2.20.0 setproctitle-1.3.4 wandb-0.19.3\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6edeb623-121f-4100-b0b5-a8394f0e58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27f8bbf0-8ab0-45f6-8049-2c7b0210e74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 30/564 09:10 < 2:55:01, 0.05 it/s, Epoch 0.15/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2165\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2166\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2167\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2169\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2522\u001b[0m )\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2530\u001b[0m ):\n\u001b[0;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3654\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3660\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3708\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3706\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3707\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3708\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3709\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3710\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:978\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    976\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 978\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[0;32m    979\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    980\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    981\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    982\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    983\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    984\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    985\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    986\u001b[0m )\n\u001b[0;32m    987\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    988\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:798\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[0;32m    794\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[0;32m    795\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    796\u001b[0m         )\n\u001b[1;32m--> 798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    799\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    800\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    801\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    802\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    803\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    804\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    805\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:551\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    543\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    544\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    545\u001b[0m         hidden_state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    548\u001b[0m         output_attentions,\n\u001b[0;32m    549\u001b[0m     )\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    552\u001b[0m         hidden_state,\n\u001b[0;32m    553\u001b[0m         attn_mask,\n\u001b[0;32m    554\u001b[0m         head_mask[i],\n\u001b[0;32m    555\u001b[0m         output_attentions,\n\u001b[0;32m    556\u001b[0m     )\n\u001b[0;32m    558\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:477\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    478\u001b[0m     query\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    479\u001b[0m     key\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    480\u001b[0m     value\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    481\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m    482\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    483\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    484\u001b[0m )\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    486\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:391\u001b[0m, in \u001b[0;36mDistilBertSdpaAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"group heads\"\"\"\u001b[39;00m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m*\u001b[39m dim_per_head)\n\u001b[1;32m--> 391\u001b[0m q \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_lin(query))  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    392\u001b[0m k \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_lin(key))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    393\u001b[0m v \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_lin(value))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952b553-1d47-4fb1-b35a-e1aa2aad70da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(r'C:\\Users\\dell\\Desktop\\MyDocs\\Docs\\MK\\distilbert-tp-123k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c916b61-848f-42ab-9e51-c253dcdb28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task=\"sentiment-analysis\", model=\"C:\\Users\\dell\\Desktop\\MyDocs\\Docs\\MK\\distilbert-tp-123k\", tokenizer=tokenizer, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61541d-f9c9-4841-ac13-d02c160f7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2text = {'LABEL_0': 'Negative', 'LABEL_1': 'Neutral', 'LABEL_2': 'Positive'}\n",
    "reviews = [\"I’ve purchased first of those coats in May2020. Still goes strong although my girl is not with us anymore her little sister is still using it. I’ve bought a limited edition this year as a Christmas gift for my current dog so she has also one of her own.My dog is notorious at rolling so needs to be washed pretty much every day. Those coats are a life saver. Dry dog and no shivering within 20 min. So glad I’ve found the brand. Highly recommend it for any dog parent.\",\n",
    "           \"Took my son and friends for a birthday party 6 children under 12. A week later get a £200 fine for parking in a free car park!!Apparently you have to type your reg number into something to validate your stay. They don’t care as a third party operate the car park so it’s not there problem if you get a fine!! Everywhere I’ve ever been with one of these systems the person at reception will tell you when booking to enter your details into the machine.  But not Hollywood bowl Oxford!!I see they have replied to me saying to get in touch I already did they said it’s not their problem due to them not owning the car park!\",\n",
    "           \"Beyond brilliant! Rachel's energy is something else and she entertains them children thoroughly throughout the entire time of booking, plenty of dancing and music and she matches her routine to the ages of the children.I've used for a couple of years now and have no hesitation in booking her for every kids event we have, She makes it a proper party and the adults love it too!\"]\n",
    "\n",
    "sentiments = pipe(reviews)\n",
    "i = 0\n",
    "for sentiment in sentiments:\n",
    "  print(f\"Review:\\n{reviews[i]}\")\n",
    "  print(f\"Category: {label2text[sentiment['label']]}\")\n",
    "  print(f\"Confidence: {round(sentiment['score'] * 100, 2)}%\")\n",
    "  print(\"=\" * 40)\n",
    "  i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
