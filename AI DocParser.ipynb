{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb8c68f-8f89-4705-83fb-460355546486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da40c6c-15be-4b6a-8f50-3a039063a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r'C:\\Users\\dell\\Desktop\\MyDocs\\Docs\\MK\\reliance_disney_agreement.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed6cc96-bc30-46c4-b81c-aa35ef9ff1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prettytable in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.12.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dell\\anaconda3\\lib\\site-packages (from prettytable) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0caa0f8f-36c6-4116-953d-0380448a969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable \n",
    "def print_pretty_table(extracted_values):\n",
    "    \"\"\"\n",
    "    Prints the extracted key-value pairs in a formatted table.\n",
    "\n",
    "    Args:\n",
    "        extracted_values (dict): Dictionary of extracted key-value pairs.\n",
    "    \"\"\"\n",
    "    # Create a PrettyTable instance\n",
    "    table = PrettyTable()\n",
    "\n",
    "    # Set column names\n",
    "    table.field_names = [\"Key\", \"Value\"]\n",
    "\n",
    "    # Add rows\n",
    "    for key, value in extracted_values.items():\n",
    "        table.add_row([key, value])\n",
    "\n",
    "    # Print the table\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95cd1a85-68c7-436d-b55e-1c286a626976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.25.1)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.11.5)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 PyMuPDF pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e955214-32c0-46a0-b687-9a7b9de6881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, method=\"fitz\"):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF using the specified library (fitz, pdfplumber, or PyPDF2).\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        method (str): The library to use for extraction ('fitz', 'pdfplumber', 'pypdf2').\n",
    "                      Defaults to 'fitz' (PyMuPDF).\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted and cleaned text from the PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw_text = \"\"\n",
    "        \n",
    "        # pdfplumber method\n",
    "        if method == \"pdfplumber\":\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    raw_text += page.extract_text() or \"\"\n",
    "\n",
    "        # PyPDF2 method\n",
    "        elif method == \"pypdf2\":\n",
    "            reader = PdfReader(pdf_path)\n",
    "            for page in reader.pages:\n",
    "                raw_text += page.extract_text() or \"\"\n",
    "\n",
    "        # PyMuPDF (fitz) method\n",
    "        else:\n",
    "            document = fitz.open(pdf_path)\n",
    "            for page_num in range(len(document)):\n",
    "                page = document[page_num]\n",
    "                raw_text += page.get_text()\n",
    "            document.close()\n",
    "\n",
    "        if not raw_text.strip():\n",
    "            raise ValueError(\"No extractable text found in the PDF.\")\n",
    "        else:\n",
    "            return raw_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error processing the PDF with {method}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de07097-bc68-4257-83e6-d6576cbc7b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\dell\\Desktop\\MyDocs\\Docs\\MK\\reliance_disney_ocr.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0477e7f6-a81e-4d3e-af3a-f0da22a11b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read ground truth from a file\n",
    "def read_ground_truth(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            ground_truth = file.read().strip() \n",
    "        return ground_truth\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at '{file_path}' was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09091a21-fa4b-4e8b-bc94-9e23f4996d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by:\n",
    "    - Removing extra whitespaces\n",
    "    - Normalizing newlines\n",
    "    - Converting to lowercase\n",
    "    - Optional: Removing punctuation or stopwords\n",
    "    \"\"\"\n",
    "    # Remove extra spaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces and trim\n",
    "    text = text.replace('\\n', ' ')  # Replace newlines with a space\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Optional: Remove punctuation (if needed)\n",
    "    # text = re.sub(r'[^\\w\\s]', '', text)  # Uncomment if punctuation removal is desired\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed1dc38-1dbf-4d6c-aaed-fb9314b4a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def word_match_accuracy(extracted_text, ground_truth):\n",
    "    # Preprocess both texts\n",
    "    extracted_text = preprocess_text(extracted_text)\n",
    "    ground_truth = preprocess_text(ground_truth)\n",
    "\n",
    "    # Convert the texts into sets of words\n",
    "    extracted_words = set(extracted_text.split())\n",
    "    ground_truth_words = set(ground_truth.split())\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    intersection = len(extracted_words & ground_truth_words)\n",
    "    precision = intersection / len(extracted_words) if len(extracted_words) > 0 else 0\n",
    "    recall = intersection / len(ground_truth_words) if len(ground_truth_words) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb60a65-1c67-419a-8427-598b9dfa7d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.26.1)\n",
      "Requirement already satisfied: Levenshtein==0.26.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-Levenshtein) (0.26.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3242610a-084a-406f-8a0a-7de9847ae101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def levenshtein_accuracy(extracted_text, ground_truth):\n",
    "    # Preprocess both texts\n",
    "    extracted_text = preprocess_text(extracted_text)\n",
    "    ground_truth = preprocess_text(ground_truth)\n",
    "    \n",
    "    # Compute Levenshtein distance and return the similarity ratio\n",
    "    distance = Levenshtein.distance(extracted_text, ground_truth)\n",
    "    max_len = max(len(extracted_text), len(ground_truth))\n",
    "    \n",
    "    # Similarity ratio\n",
    "    similarity_ratio = 1 - (distance / max_len)\n",
    "    return similarity_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ee855df-aab7-4f81-90a9-723af88cc5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    raw_data = file.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "\n",
    "with open(file_path, 'r', encoding=encoding) as file:\n",
    "    ground_truth = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46a4e70f-3202-41f0-ba86-1e9d3b667211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Similarity: 98.32%\n",
      "Precision: 0.953, Recall: 0.953, F1-Score: 0.953\n"
     ]
    }
   ],
   "source": [
    "extracted_text = extract_text_from_pdf(pdf_path, method=\"fitz\")\n",
    "\n",
    "# Check Levenshtein similarity\n",
    "similarity_ratio = levenshtein_accuracy(extracted_text, ground_truth)\n",
    "print(f\"Levenshtein Similarity: {similarity_ratio * 100:.2f}%\")\n",
    "\n",
    "# Check word match accuracy\n",
    "precision, recall, f1 = word_match_accuracy(extracted_text, ground_truth)\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29d284ba-ed8b-4351-932a-741142a31df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Similarity: 98.50%\n",
      "Precision: 0.948, Recall: 0.952, F1-Score: 0.950\n"
     ]
    }
   ],
   "source": [
    "extracted_text = extract_text_from_pdf(pdf_path, method=\"pdfplumber\")\n",
    "\n",
    "# Check Levenshtein similarity\n",
    "similarity_ratio = levenshtein_accuracy(extracted_text, ground_truth)\n",
    "print(f\"Levenshtein Similarity: {similarity_ratio * 100:.2f}%\")\n",
    "\n",
    "# Check word match accuracy\n",
    "precision, recall, f1 = word_match_accuracy(extracted_text, ground_truth)\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbe26e31-6b40-40e6-9b7e-4a786fd1663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Similarity: 97.85%\n",
      "Precision: 0.880, Recall: 0.903, F1-Score: 0.892\n"
     ]
    }
   ],
   "source": [
    "extracted_text = extract_text_from_pdf(pdf_path, method=\"pypdf2\")\n",
    "\n",
    "# Check Levenshtein similarity\n",
    "similarity_ratio = levenshtein_accuracy(extracted_text, ground_truth)\n",
    "print(f\"Levenshtein Similarity: {similarity_ratio * 100:.2f}%\")\n",
    "\n",
    "# Check word match accuracy\n",
    "precision, recall, f1 = word_match_accuracy(extracted_text, ground_truth)\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e967e07e-dd1c-478d-8751-a7b9bdf9632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "def clean_txt(raw_text):\n",
    "    cleaned_text = re.sub(r\"(page \\d+ of \\d+)\", \"\", raw_text, flags=re.IGNORECASE)  # Remove page numbers\n",
    "    cleaned_text = re.sub(r\"(\\n\\s*\\n)|(\\r\\n|\\r|\\n)\", \"\\n\", cleaned_text)  # Remove extra newlines\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)  # Normalize whitespace\n",
    "    cleaned_text = re.sub(r\"[^\\x00-\\x7F₹$\\s]+\", \" \", cleaned_text) # Remove non-ASCII characters\n",
    "    normalized_text = cleaned_text.strip()\n",
    "\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f9bdec5-a38f-46cf-ad42-bc1dd94de92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (8.3.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.2.0,>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.0/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.6/12.8 MB 3.6 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.0/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 4.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 5.0 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f82fbb5-9062-4d0f-b6b7-3efe8bac99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Loading the spaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def key_value_extraction(text, keys):\n",
    "    \"\"\"\n",
    "    Extract key values from a cleaned contract text based on the provided keys.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): Cleaned and preprocessed contract text.\n",
    "    - keys (list, optional): A list of keys (strings) to extract. If None, all keys will be extracted.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the extracted key-value pairs.\n",
    "    \"\"\"\n",
    "    # Process the text with spaCy for Named Entity Recognition (NER)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract company names using NER (ORG - Organizations)\n",
    "    companies = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "\n",
    "    # Extract dates using regex pattern for dates\n",
    "    date_pattern = r\"\\d{1,2}[a-z]{2}\\s[A-Za-z]+\\s\\d{4}\"\n",
    "    dates = re.findall(date_pattern, text)\n",
    "\n",
    "    # Extract financial values (e.g., investment amounts, transaction values) using regex\n",
    "    amount_pattern = r\"₹[\\d,]+(?:\\s*crore|\\s*\\(.*\\))\"\n",
    "    amounts = re.findall(amount_pattern, text)\n",
    "\n",
    "    # Define helper function to check for exclusive rights\n",
    "    def check_exclusive_rights(text):\n",
    "        \"\"\"\n",
    "        Function to check if the contract mentions exclusive rights.\n",
    "        Returns True if exclusive rights are granted, otherwise False.\n",
    "        \"\"\"\n",
    "        # Keywords or phrases that indicate exclusive rights\n",
    "        exclusive_rights_keywords = [\"exclusive rights\", \"granted exclusive rights\", \"exclusive distribution rights\"]\n",
    "        \n",
    "        # Search for any of the keywords in the text\n",
    "        for keyword in exclusive_rights_keywords:\n",
    "            if re.search(rf\"{keyword}\", text, re.IGNORECASE):\n",
    "                return True\n",
    "        \n",
    "        # If no keywords are found, return False\n",
    "        return False\n",
    "\n",
    "    # Check if the contract mentions exclusive rights\n",
    "    exclusive_rights = check_exclusive_rights(text)\n",
    "\n",
    "    # Prepare the default extracted information, note that the below config depends upon predefined\n",
    "    # keywords and it can be done better for more domains\n",
    "    extracted_info = {\n",
    "        keys[0]: companies[0] if len(companies) > 0 else None,  # First company (Party 1)\n",
    "        keys[1]: companies[1] if len(companies) > 1 else None,  # Second company (Party 2)\n",
    "        keys[2]: dates[0] if dates else None,  # First date found\n",
    "        keys[3]: amounts[0] if amounts else None,  # First investment amount found\n",
    "        keys[4]: amounts[2] if len(amounts) > 1 else None,  # Second transaction value found\n",
    "        keys[5]: \"Yes\" if exclusive_rights else \"No\"  # Boolean value indicating exclusive rights\n",
    "    }\n",
    "\n",
    "    # Return the filtered dictionary of extracted information\n",
    "    return extracted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "348bd2c2-81d9-48e0-a001-e6da07653de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(key_values, output_file):\n",
    "    \"\"\"\n",
    "    Saves the extracted key-value pairs to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        extracted_values (dict): Dictionary of extracted key-value pairs.\n",
    "        output_file (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Key\", \"Value\"])\n",
    "        for key, value in key_values.items():\n",
    "            writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32ae420d-567b-48a8-8b1b-7adbc2fb6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(key_values, output_file):\n",
    "    \"\"\"\n",
    "    Saves the extracted key-value pairs to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        extracted_values (dict): Dictionary of extracted key-value pairs.\n",
    "        output_file (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Key\", \"Value\"])\n",
    "        for key, value in key_values.items():\n",
    "            writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32353361-82e6-4eaf-ba7e-5ef43d3c65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-specific patterns\n",
    "domain_rules = {\n",
    "    \"Contracts\": {\n",
    "        \"Name of the 1st Party\": r\"(?:First Party|Party 1|Party One):?\\s*([^\\n,]+)\",\n",
    "        \"Name of the 2nd Party\": r\"(?:Second Party|Party 2|Party Two):?\\s*([^\\n,]+)\",\n",
    "        \"Contract Start Date\": r\"(?:Effective Date|Start Date|Commencement):?\\s*(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\",\n",
    "        \"Contract End Date\": r\"(?:End Date|Termination Date):?\\s*(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\",\n",
    "        \"Scope of Work\": r\"(?:Scope of Work|Services):?\\s*([^\\n]+)\",\n",
    "        \"Penalty Amount\": r\"(?:Penalty|Fine):?\\s*\\$?([\\d,]+)\"\n",
    "    },\n",
    "    \"Finance\": {\n",
    "        \"Transaction Amount\": r\"(?:Amount|Transaction):?\\s*\\$?([\\d,]+)\",\n",
    "        \"Date of Transaction\": r\"(?:Transaction Date|Date):?\\s*(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\",\n",
    "        \"Account Number\": r\"(?:Account Number|Acc No):?\\s*([^\\n,]+)\",\n",
    "        \"Bank Name\": r\"(?:Bank|Financial Institution):?\\s*([^\\n,]+)\"\n",
    "    },\n",
    "    \"Legal\": {\n",
    "        \"Case Number\": r\"(?:Case Number|Case ID):?\\s*([^\\n,]+)\",\n",
    "        \"Filing Date\": r\"(?:Filing Date|Date Filed):?\\s*(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\",\n",
    "        \"Petitioner\": r\"(?:Petitioner|Claimant):?\\s*([^\\n,]+)\",\n",
    "        \"Respondent\": r\"(?:Respondent|Defendant):?\\s*([^\\n,]+)\",\n",
    "        \"Court Name\": r\"(?:Court|Jurisdiction):?\\s*([^\\n,]+)\"\n",
    "    },\n",
    "    \"HR\": {\n",
    "        \"Employee Name\": r\"(?:Employee Name|Name):?\\s*([^\\n,]+)\",\n",
    "        \"Employee ID\": r\"(?:Employee ID|ID):?\\s*([^\\n,]+)\",\n",
    "        \"Joining Date\": r\"(?:Joining Date|Start Date):?\\s*(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\",\n",
    "        \"Department\": r\"(?:Department|Team):?\\s*([^\\n,]+)\",\n",
    "        \"Salary\": r\"(?:Salary|Compensation):?\\s*\\$?([\\d,]+)\"\n",
    "    },\n",
    "    \"Invoices\": {\n",
    "        \"Invoice Number\": r\"(?:Invoice Number|Invoice ID):?\\s*([^\\n,]+)\",\n",
    "        \"Invoice Date\": r\"(?:Invoice Date|Date):?\\s*(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\",\n",
    "        \"Client Name\": r\"(?:Client Name|Customer Name):?\\s*([^\\n,]+)\",\n",
    "        \"Total Amount\": r\"(?:Total Amount|Total):?\\s*\\$?([\\d,]+)\",\n",
    "        \"Due Date\": r\"(?:Due Date|Payment Due):?\\s*(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "482aa98f-cc50-47cc-af28-6fc60ec70ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "# Step 1: Load the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Step 2: Custom Summarization Function based on Keys and Values\n",
    "def custom_summarization(text, keys, values):\n",
    "    \"\"\"\n",
    "    Summarize text by focusing on specific keys and values.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The full text to summarize.\n",
    "    - keys (list): List of keys (such as 'Investment Amount', 'Party 1', etc.)\n",
    "    - values (list): List of corresponding values (such as 'Reliance', '₹11,500 crore', etc.)\n",
    "    \n",
    "    Returns:\n",
    "    - summary (str): A summary that focuses on the provided keys and values.\n",
    "    \"\"\"\n",
    "    \n",
    "    relevant_text = \"\"\n",
    "    \n",
    "    # Try to match key-value pairs and extract related text\n",
    "    for key, value in zip(keys, values):\n",
    "        # Find sentences containing the key and its value (this is a simple heuristic)\n",
    "        pattern = rf\"([^.]*{re.escape(value)}[^.]*\\.)\"\n",
    "        matches = re.findall(pattern, text)\n",
    "        \n",
    "        # Combine the relevant sentences into the relevant_text\n",
    "        relevant_text += \" \".join(matches)\n",
    "    \n",
    "    # If no relevant text is found, use the whole text for summarization\n",
    "    if not relevant_text:\n",
    "        relevant_text = text\n",
    "    \n",
    "    # Summarize the extracted relevant text\n",
    "    summary = summarizer(relevant_text, max_length=150, min_length=50, do_sample=False)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eeba0ac-3f67-472d-a23d-e6d96e9245ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "cleaned_text = clean_txt(extracted_text)\n",
    "text = cleaned_text\n",
    "\n",
    "keys = ['Party 1', 'Party 2', 'Date of Announcement', 'Investment', 'Transaction Value', 'Exclusive Rights']\n",
    "\n",
    "key_values = key_value_extraction(text, keys)\n",
    "\n",
    "values = [i for i in key_values.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09429f27-d024-4ef9-bf8d-ce6d7708402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------------+\n",
      "|         Key          |          Value           |\n",
      "+----------------------+--------------------------+\n",
      "|       Party 1        |         Reliance         |\n",
      "|       Party 2        | the Joint Venture Disney |\n",
      "| Date of Announcement |    28th February 2024    |\n",
      "|      Investment      |      ₹11,500 crore       |\n",
      "|  Transaction Value   |      ₹70,352 crore       |\n",
      "|   Exclusive Rights   |           Yes            |\n",
      "+----------------------+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "print_pretty_table(key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cb52cf3-a4d7-494f-bf82-54b168a6a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(key_values, output_file):\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8', errors='ignore') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Key\", \"Value\"])\n",
    "        for key, value in key_values.items():\n",
    "            writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e0ad1a2-536e-4027-aab6-ae90ca2b8cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reliance Industries Limited, Viacom 18 Media Private Limited and The Walt Disney Company (NYSE:DIS) ( Disney ) today announced the signing of binding definitive agreements to form a joint venture ( JV) Reliance is India s largest private sector company, with a consolidated revenue of Rs 9,74,864 crore (US$118.5 billion) Reliances to invest ₹11,500 crore in the Joint Venture Disney to provide Content License to the Joint venture Mumbai / Burbank, Calif.\n"
     ]
    }
   ],
   "source": [
    "media_text = cleaned_text\n",
    "\n",
    "custom_summary = custom_summarization(media_text, keys, values)\n",
    "\n",
    "print(custom_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb9ada82-05b5-4fcf-8332-da6fd6f1521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to log user corrections\n",
    "def log_correction(original_summary, user_correction, feedback, keys, values):\n",
    "    \"\"\"\n",
    "    Log corrections made by the user into a CSV file.\n",
    "\n",
    "    Args:\n",
    "    - original_summary (str): The summary generated by the model.\n",
    "    - user_correction (str): The corrected summary provided by the user.\n",
    "    - feedback (str): Additional user feedback or comments.\n",
    "    - keys (list): List of keys used in the extraction process.\n",
    "    - values (list): List of values used in the extraction process.\n",
    "    \"\"\"\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"original_summary\": original_summary,\n",
    "        \"user_correction\": user_correction,\n",
    "        \"feedback\": feedback,\n",
    "        \"keys\": \", \".join(keys),\n",
    "        \"values\": \", \".join(map(str, values))\n",
    "    }\n",
    "    \n",
    "    # Append to a CSV file\n",
    "    with open(\"corrections_log.csv\", mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=log_entry.keys())\n",
    "        \n",
    "        # Write the header if the file is new\n",
    "        if file.tell() == 0:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        writer.writerow(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aa5e275-ca5e-49eb-9d26-eb8921caa6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Party 1',\n",
       " 'Party 2',\n",
       " 'Date of Announcement',\n",
       " 'Investment',\n",
       " 'Transaction Value',\n",
       " 'Exclusive Rights']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dff1cd7-8d63-4685-ad9d-14dad29963cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Test1\n",
      " Test2\n",
      " 10/20/2024\n"
     ]
    }
   ],
   "source": [
    "original_summary = custom_summary\n",
    "user_correction = input()\n",
    "feedback = input()\n",
    "\n",
    "# keys are predefined\n",
    "keys = keys\n",
    "\n",
    "values = input()\n",
    "\n",
    "log_correction(original_summary, user_correction, feedback, keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ada3735a-9f05-4ef0-8bd8-1f8b3778760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load corrections from the CSV file\n",
    "def load_corrections(file_path=\"corrections_log.csv\"):\n",
    "    \"\"\"\n",
    "    Load corrections logged by users into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the corrections log CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: User corrections and feedback.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b40c538e-9cf2-4586-83c9-93646750fb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    timestamp  \\\n",
      "0  2025-01-08T22:30:11.114439   \n",
      "1  2025-01-08T22:33:52.313421   \n",
      "\n",
      "                                    original_summary user_correction feedback  \\\n",
      "0  Reliance Industries Limited, Viacom 18 Media P...            Test    Test2   \n",
      "1  Reliance Industries Limited, Viacom 18 Media P...           Test1    Test2   \n",
      "\n",
      "                                                keys  \\\n",
      "0  Party 1, Party 2, Date of Announcement, Invest...   \n",
      "1  Party 1, Party 2, Date of Announcement, Invest...   \n",
      "\n",
      "                         values  \n",
      "0  1, 0, /, 2, 0, /, 2, 0, 2, 4  \n",
      "1  1, 0, /, 2, 0, /, 2, 0, 2, 4  \n"
     ]
    }
   ],
   "source": [
    "corrections_df = load_corrections()\n",
    "print(corrections_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23c4256d-ba66-418b-b7b1-a1ac710a2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f27e6413-3bcb-4139-852c-97ead0e6dabc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load pre-trained model and tokenizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BartTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m BartForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Prepare data for fine-tuning\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(df):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4264\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4254\u001b[0m         load_contexts\u001b[38;5;241m.\u001b[39mappend(tp_device)\n\u001b[0;32m   4256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[0;32m   4257\u001b[0m         (\n\u001b[0;32m   4258\u001b[0m             model,\n\u001b[0;32m   4259\u001b[0m             missing_keys,\n\u001b[0;32m   4260\u001b[0m             unexpected_keys,\n\u001b[0;32m   4261\u001b[0m             mismatched_keys,\n\u001b[0;32m   4262\u001b[0m             offload_index,\n\u001b[0;32m   4263\u001b[0m             error_msgs,\n\u001b[1;32m-> 4264\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[0;32m   4265\u001b[0m             model,\n\u001b[0;32m   4266\u001b[0m             state_dict,\n\u001b[0;32m   4267\u001b[0m             loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[0;32m   4268\u001b[0m             resolved_archive_file,\n\u001b[0;32m   4269\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m   4270\u001b[0m             ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[0;32m   4271\u001b[0m             sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[0;32m   4272\u001b[0m             _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[0;32m   4273\u001b[0m             low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[0;32m   4274\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m   4275\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m   4276\u001b[0m             offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[0;32m   4277\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[0;32m   4278\u001b[0m             hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[0;32m   4279\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[0;32m   4280\u001b[0m             gguf_path\u001b[38;5;241m=\u001b[39mgguf_path,\n\u001b[0;32m   4281\u001b[0m             weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[0;32m   4282\u001b[0m         )\n\u001b[0;32m   4284\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   4285\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4589\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[0;32m   4587\u001b[0m             model\u001b[38;5;241m.\u001b[39mapply(model\u001b[38;5;241m.\u001b[39m_initialize_weights)\n\u001b[0;32m   4588\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4589\u001b[0m         model\u001b[38;5;241m.\u001b[39mapply(model\u001b[38;5;241m.\u001b[39m_initialize_weights)\n\u001b[0;32m   4591\u001b[0m \u001b[38;5;66;03m# Set some modules to fp32 if any\u001b[39;00m\n\u001b[0;32m   4592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_in_fp32_modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1029\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \n\u001b[0;32m    995\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \n\u001b[0;32m   1027\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m-> 1029\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m   1030\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1029\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \n\u001b[0;32m    995\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \n\u001b[0;32m   1027\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m-> 1029\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m   1030\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1029\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \n\u001b[0;32m    995\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \n\u001b[0;32m   1027\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m-> 1029\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m   1030\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1030\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m   1029\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[1;32m-> 1030\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:1912\u001b[0m, in \u001b[0;36mPreTrainedModel._initialize_weights\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m   1910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hf_initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_weights(module)\n\u001b[0;32m   1913\u001b[0m module\u001b[38;5;241m.\u001b[39m_is_hf_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:761\u001b[0m, in \u001b[0;36mBartPreTrainedModel._init_weights\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m    759\u001b[0m         module\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mEmbedding):\n\u001b[1;32m--> 761\u001b[0m     module\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnormal_(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, std\u001b[38;5;241m=\u001b[39mstd)\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    763\u001b[0m         module\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata[module\u001b[38;5;241m.\u001b[39mpadding_idx]\u001b[38;5;241m.\u001b[39mzero_()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Prepare data for fine-tuning\n",
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "    Prepare input-output pairs for fine-tuning the summarization model.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): DataFrame containing original and corrected summaries.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Tokenized input and labels for fine-tuning.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(list(df[\"original_summary\"]), truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    labels = tokenizer(list(df[\"user_correction\"]), truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Load corrections and prepare data\n",
    "corrections_df = load_corrections()\n",
    "data = prepare_data(corrections_df)\n",
    "\n",
    "# Fine-tuning arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_bart\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd1d4e-612f-4fbb-9030-7741801cf9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
